
<!-- @import "[TOC]" {cmd="toc" depthFrom=1 depthTo=6 orderedList=false} -->

<!-- code_chunk_output -->

* [1 前言](#1-前言)
* [2 为何需要CMWQ?](#2-为何需要cmwq)
* [3 CMWQ如何解决问题的呢?](#3-cmwq如何解决问题的呢)
	* [3.1 设计原则](#31-设计原则)
	* [3.2 CMWQ的整体架构](#32-cmwq的整体架构)
	* [3.3 如何解决线程数目过多的问题?](#33-如何解决线程数目过多的问题)
	* [3.4 如何解决并发问题?](#34-如何解决并发问题)
* [4 接口API](#4-接口api)

<!-- /code_chunk_output -->

# 1 前言

一种新的机制出现的原因往往是为了解决实际的问题, 虽然linux kernel中已经提供了workqueue的机制, 那么为何还要引入cmwq呢?也就是说: 旧的workqueue机制存在什么样的问题?在新的cmwq又是如何解决这些问题的呢?它接口是如何呈现的呢(驱动工程师最关心这个了)?如何兼容旧的驱动呢?本文希望可以解开这些谜题. 

本文的代码来自linux kernel 4.0. 

# 2 为何需要CMWQ?

内核中很多场景需要异步执行环境(在驱动中尤其常见), 这时候, 我们需要定义一个work(执行哪一个函数)并挂入workqueue. 处理该work的线程叫做worker, 不断的处理队列中的work, 当处理完毕后则休眠, 队列中有work的时候就醒来处理, 如此周而复始. 一切看起来比较完美, 问题出在哪里呢?

(1)内核线程数量太多. 如果没有足够的内核知识, 程序员有可能会错误的使用workqueue机制, 从而导致这个机制被玩坏. 例如明明可以使用default workqueue, 偏偏自己创建属于自己的workqueue, 这样一来, 对于那些比较大型的系统(CPU个数比较多), 很可能内核启动结束后就耗尽了PID space(default最大值是65535), 这种情况下, 你让user space的程序情何以堪?虽然default最大值是可以修改的, 从而扩大PID space来解决这个问题, 不过系统太多的task会对整体performance造成负面影响. 

(2)尽管消耗了很多资源, 但是并发性如何呢?我们先看single threaded的workqueue, 这种情况完全没有并发的概念, 任何的work都是排队执行, 如果正在执行的work很慢, 例如4～5秒的时间, 那么队列中的其他work除了等待别无选择. multi threaded(更准确的是per-CPU threaded)情况当然会好一些(毕竟多消耗了资源), 但是对并发仍然处理的不是很好. 对于multi threaded workqueue, 虽然创建了thread pool, 但是thread pool的数目是固定的: 每个oneline的cpu上运行一个, 而且是严格的绑定关系. 也就是说本来线程池是一个很好的概念, 但是传统workqueue上的线程池(或者叫做worker pool)却分割了每个线程, 线程之间不能互通有无. 例如cpu0上的worker thread由于处理work而进入阻塞状态, 那么该worker thread处理的work queue中的其他work都阻塞住, 不能转移到其他cpu上的worker thread去, 更有甚者, cpu0上随后挂入的work也接受同样的命运(在某个cpu上schedule的work一定会运行在那个cpu上), 不能去其他空闲的worker thread上执行. 由于不能提供很好的并发性, 有些内核模块(fscache)甚至自己创建了thread pool(slow work也曾经短暂的出现在kernel中). 

(3)dead lock问题. 我们举一个简单的例子: 我们知道, 系统有default workqueue, 如果没有特别需求, 驱动工程师都喜欢用这个workqueue. 我们的驱动模块在处理release(userspace close该设备)函数的时候, 由于使用了workqueue, 那么一般会flush整个workqueue, 以便确保本driver的所有事宜都已经处理完毕(在close的时候很有可能有pending的work, 因此要flush), 大概的代码如下: 

```c
获取锁A

flush workqueue

释放锁A
```

flush work是一个长期过程, 因此很有可能被调度出去, 这样调用close的进程被阻塞, 等到keventd\_wq这个内核线程组完成flush操作后就会wakeup该进程. 但是这个default workqueue使用很广, 其他的模块也可能会schedule work到该workqueue中, 并且如果这些模块的work也需要获取锁A, 那么就会deadlock(keventd\_wq阻塞, 再也无法唤醒等待flush的进程). 解决这个问题的方法是创建多个workqueue, 但是这样又回到了内核线程数量大多的问题上来. 

我们再看一个例子: 假设某个驱动模块比较复杂, 使用了两个work struct, 分别是A和B, 如果work A依赖 work B的执行结果, 那么, 如果这两个work都schedule到一个worker thread的时候就出现问题, 由于worker thread不能并发的执行work A和work B, 因此该驱动模块会死锁. Multi threaded workqueue能减轻这个问题, 但是无法解决该问题, 毕竟work A和work B还是有机会调度到一个cpu上执行. 造成这些问题的根本原因是众多的work竞争一个执行上下文导致的. 

(4)二元化的线程池机制. 基本上workqueue也是thread pool的一种, 但是创建的线程数目是二元化的设定: 要么是1, 要么是number of CPU, 但是, 有些场景中, 创建number of CPU太多, 而创建一个线程又太少, 这时候, 勉强使用了single threaded workqueue, 但是不得不接受串行处理work, 使用multi threaded workqueue吧, 占用资源太多. 二元化的线程池机制让用户无所适从. 

# 3 CMWQ如何解决问题的呢?

## 3.1 设计原则

在进行CMWQ的时候遵循下面两个原则: 

(1)和旧的workqueue接口兼容. 

(2)明确的划分了workqueue的前端接口和后端实现机制. 

## 3.2 CMWQ的整体架构

![config](./images/14.gif)

对于workqueue的用户而言, 前端的操作包括二种, 一个是创建workqueue. 可以选择创建自己的workqueue, 当然也可以不创建而是使用系统缺省的workqueue. 另外一个操作就是将指定的work添加到workqueue. 在旧的workqueue机制中, workqueue和worker thread是密切联系的概念, 对于single workqueue, 创建一个系统范围的worker thread, 对于multi workqueue, 创建per-CPU的worker thread, 一切都是固定死的. 针对这样的设计, 我们可以进一步思考其合理性. workqueue用户的需求就是一个异步执行的环境, 把创建workqueue和创建worker thread绑定起来大大限定了资源的使用, 其实具体后台是如何处理work, 是否否启动了多个thread, 如何管理多个线程之间的协调, workqueue的用户并不关心. 

基于这样的思考, 在CMWQ中, 将这种固定的关系被打破, 提出了worker pool这样的概念(其实就是一种thread pool的概念), 也就是说, 系统中存在若干worker pool, 不和特定的workqueue关联, 而是所有的workqueue共享. 用户可以创建workqueue(不创建worker pool)并通过flag来约束挂入该workqueue上work的处理方式. workqueue会根据其flag将work交付给系统中某个worker pool处理. 例如如果该workqueue是bounded类型并且设定了high priority, 那么挂入该workqueue的work将由per cpu的highpri worker\-pool来处理. 

让所有的workqueue共享系统中的worker pool, 即减少了资源的浪费(没有创建那么多的kernel thread), 又保证了灵活的并发性(worker pool会根据情况灵活的创建thread来处理work). 

## 3.3 如何解决线程数目过多的问题?

在CMWQ中, 用户可以根据自己的需求创建workqueue, 但是已经和后端的线程池是否创建worker线程无关了, 是否创建新的work线程是由worker线程池来管理. 系统中的线程池包括两种: 

(1)和特定CPU绑定的线程池. 这种线程池有两种, 一种叫做normal thread pool, 另外一种叫做high priority thread pool, 分别用来管理普通的worker thread和高优先级的worker thread, 而这两种thread分别用来处理普通的和高优先级的work. 这种类型的线程池数目是固定的, 和系统中cpu的数目相关, 如果系统有n个cpu, 如果都是online的, 那么会创建2n个线程池. 

(2)unbound 线程池, 可以运行在任意的cpu上. 这种thread pool是动态创建的, 是和thread pool的属性相关, 包括该thread pool创建worker thread的优先级(nice value), 可以运行的cpu链表等. 如果系统中已经有了相同属性的thread pool, 那么不需要创建新的线程池, 否则需要创建. 

OK, 上面讲了线程池的创建, 了解到创建workqueue和创建worker thread这两个事件已经解除关联, 用户创建workqueue仅仅是选择一个或者多个线程池而已, 对于bound thread pool, 每个cpu有两个thread pool, 关系是固定的, 对于unbound thread pool, 有可能根据属性动态创建thread pool. 那么worker thread pool如何创建worker thread呢?是否会数目过多呢?

缺省情况下, 创建thread pool的时候会创建一个worker thread来处理work, 随着work的提交以及work的执行情况, thread pool会动态创建worker thread. 具体创建worker thread的策略为何?本质上这是一个需要在并发性和系统资源消耗上进行平衡的问题, CMWQ使用了一个非常简单的策略: 当thread pool中处于运行状态的worker thread等于0, 并且有需要处理的work的时候, thread pool就会创建新的worker线程. 当worker线程处于idle的时候, 不会立刻销毁它, 而是保持一段时间, 如果这时候有创建新的worker的需求的时候, 那么直接wakeup idle的worker即可. 一段时间过去仍然没有事情处理, 那么该worker thread会被销毁. 

## 3.4 如何解决并发问题?

我们用某个cpu上的bound workqueue来描述该问题. 假设有A B C D四个work在该cpu上运行, 缺省的情况下, thread pool会创建一个worker来处理这四个work. 在旧的workqueue中, A B C D四个work毫无疑问是串行在cpu上执行, 假设B work阻塞了, 那么C D都是无法执行下去, 一直要等到B解除阻塞并执行完毕. 

对于CMWQ, 当B work阻塞了, thread pool可以感知到这一事件, 这时候它会创建一个新的worker thread来处理C D这两个work, 从而解决了并发的问题. 由于解决了并发问题, 实际上也解决了由于竞争一个execution context而引入的各种问题(例如dead lock). 

# 4 接口API

1、初始化work的接口保持不变, 可以静态或者动态创建work. 

2、调度work执行也保持和旧的workqueue一致. 

3、创建workqueue. 和旧的create\_workqueue接口不同, CMWQ采用了alloc\_workqueue这样的接口符号, 相关的接口定义如下: 

```c
#define alloc_workqueue(fmt, flags, max_active, args...)        \ 
    __alloc_workqueue_key((fmt), (flags), (max_active),  NULL, NULL, ##args)

#define alloc_ordered_workqueue(fmt, flags, args...)            \ 
    alloc_workqueue(fmt, WQ_UNBOUND | __WQ_ORDERED | (flags), 1, ##args)

#define create_freezable_workqueue(name)                \ 
    alloc_workqueue("%s", WQ_FREEZABLE | WQ_UNBOUND | WQ_MEM_RECLAIM, 1, (name))

#define create_workqueue(name)                        \ 
    alloc_workqueue("%s", WQ_MEM_RECLAIM, 1, (name))

#define create_singlethread_workqueue(name)                \ 
    alloc_ordered_workqueue("%s", WQ_MEM_RECLAIM, name)
```

在描述这些workqueue的接口之前, 我们需要准备一些workqueue flag的知识. 

标有WQ_UNBOUND这个flag的workqueue说明其work的处理不需要绑定在特定的CPU上执行, workqueue需要关联一个系统中的unbound worker thread pool. 如果系统中能找到匹配的线程池(根据workqueue的属性(attribute)), 那么就选择一个, 如果找不到适合的线程池, workqueue就会创建一个worker thread pool来处理work. 

WQ\_FREEZABLE是一个和电源管理相关的内容. 在系统Hibernation或者suspend的时候, 有一个步骤就是冻结用户空间的进程以及部分(标注freezable的)内核线程(包括workqueue的worker thread). 标记WQ_FREEZABLE的workqueue需要参与到进程冻结的过程中, worker thread被冻结的时候, 会处理完当前所有的work, 一旦冻结完成, 那么就不会启动新的work的执行, 直到进程被解冻. 

和WQ\_MEM\_RECLAIM这个flag相关的概念是rescuer thread. 前面我们描述解决并发问题的时候说到: 对于A B C D四个work, 当正在处理的B work被阻塞后, worker pool会创建一个新的worker thread来处理其他的work, 但是, 在memory资源比较紧张的时候, 创建worker thread未必能够成功, 这时候, 如果B work是依赖C或者D work的执行结果的时候, 系统进入dead lock. 这种状态是由于不能创建新的worker thread导致的, 如何解决呢?对于每一个标记WQ\_MEM\_RECLAIM flag的work queue, 系统都会创建一个rescuer thread, 当发生这种情况的时候, C或者D work会被rescuer thread接手处理, 从而解除了dead lock. 

WQ_HIGHPRI说明挂入该workqueue的work是属于高优先级的work, 需要高优先级(比较低的nice value)的worker thread来处理. 

WQ\_CPU\_INTENSIVE这个flag说明挂入该workqueue的work是属于特别消耗cpu的那一类. 为何要提供这样的flag呢?我们还是用老例子来说明. 对于A B C D四个work, B是cpu intersive的, 当thread正在处理B work的时候, 该worker thread一直执行B work, 因为它是cpu intensive的, 特别吃cpu, 这时候, thread pool是不会创建新的worker的, 因为当前还有一个worker是running状态, 正在处理B work. 这时候C Dwork实际上是得不到执行, 影响了并发. 

了解了上面的内容, 那么基本上alloc\_workqueue中flag参数就明白了, 下面我们转向max\_active这个参数. 系统不能允许创建太多的thread来处理挂入某个workqueue的work, 最多能创建的线程数目是定义在max\_active参数中. 

除了alloc\_workqueue接口API之外, 还可以通过alloc\_ordered\_workqueue这个接口API来创建一个严格串行执行work的一个workqueue, 并且该workqueue是unbound类型的. create\_\*的接口都是为了兼容过去接口而设立的, 大家可以自行理解, 这里就不多说了. 