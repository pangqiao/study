
<!-- @import "[TOC]" {cmd="toc" depthFrom=1 depthTo=6 orderedList=false} -->

<!-- code_chunk_output -->

* [1 前言](#1-前言)
* [2 为何有softirq和tasklet](#2-为何有softirq和tasklet)
	* [2.1 为何有top half和bottom half](#21-为何有top-half和bottom-half)
	* [2.2 为何有softirq和tasklet](#22-为何有softirq和tasklet)

<!-- /code_chunk_output -->

# 1 前言

对于**中断处理**而言, linux将其分成了**两个部分**, 一个叫做**中断handler(top half**), 是**全程关闭中断**的, 另外一部分是**deferable task(bottom half**), 属于**不那么紧急需要处理**的事情. 在执行bottom half的时候, 是**开中断**的. 有**多种bottom half的机制**, 例如: **softirq**、**tasklet**、**workqueue**或是**直接创建一个kernel thread**来执行bottom half(这在旧的kernel驱动中常见, 现在, 一个理智的driver厂商是不会这么做的). 本文主要讨论softirq机制. 由于**tasklet是基于softirq**的, 因此本文也会提及tasklet, 但主要是从需求层面考虑, 不会涉及其具体的代码实现. 

在普通的驱动中**一般是不会用到softirq**, 但是由于驱动经常使用的tasklet是基于softirq的, 因此, 了解softirq机制有助于撰写更优雅的driver. **softirq不能动态分配**, 都是**静态定义(！！！**)的. **内核**已经定义了**若干种softirq number**, 例如**网络数据的收发**、**block设备的数据访问**(数据量大, 通信带宽高), **timer的deferable task**(时间方面要求高). 本文的第二章讨论了**softirq**和**tasklet**这两种机制**有何不同**, 分别适用于**什么样的场景**. 第三章描述了一些context的概念, 这是要理解后续内容的基础. 第四章是进入**softirq的实现**, 对比hard irq来解析soft irq的注册、触发, 调度的过程. 

注: 本文中的linux kernel的版本是3.14

# 2 为何有softirq和tasklet

## 2.1 为何有top half和bottom half

中断处理模块是任何OS中最重要的一个模块, 对系统的性能会有直接的影响. 想像一下: 如果在通过U盘进行大量数据拷贝的时候, 你按下一个key, 需要半秒的时间才显示出来, 这个场景是否让你崩溃?因此, 对于那些复杂的、需要大量数据处理的硬件中断, 我们不能让handler中处理完一切再恢复现场(handler是全程关闭中断的), 而是仅仅在**handler**中**处理一部分**, 具体包括: 

(1)有**实时性**要求的

(2)和**硬件相关**的. 例如**ack中断**, **read HW FIFO to ram**等

(3)如果是**共享中断**, 那么获取**硬件中断状态**以便**判断是否是本中断发生**

除此之外, 其他的内容都是放到bottom half中处理. 在把中断处理过程划分成top half和bottom half之后, 关中断的top half被瘦身, 可以非常快速的执行完毕, 大大减少了系统关中断的时间, 提高了系统的性能. 

我们可以基于下面的系统进一步的进行讨论: 

![config](./images/10.gif)

当**网卡控制器**的FIFO收到的来自**以太网的数据**的时候(例如半满的时候, 可以软件设定), 可以**将该事件**通过**irq signal**送达**Interrupt Controller**. Interrupt Controller可以把中断**分发给系统中的Processor A or B**. 

**NIC的中断处理过程**大概包括: **mask and ack interrupt controller**-------->**ack NIC**-------->**copy FIFO to ram**------>**handle Data in the ram**----------->**unmask interrupt controller**

我们先假设**Processor A处理**了这个**网卡中断事件**, 于是NIC的中断handler在Processor A上欢快的执行, 这时候, **Processor A的本地中断是disable**的. NIC的中断handler在执行的过程中, **网络数据仍然源源不断的到来**, 但是, 如果**NIC的中断handler不操作NIC的寄存器**来**ack这个中断**的话, NIC是**不会触发下一次中断**的. 还好, 我们的NIC interrupt handler总是在**最开始就会ack**, 因此, 这不会导致性能问题. ack之后, NIC已经具体再次trigger中断的能力. 当Processor A上的handler 在处理接收来自网络的数据的时候, **NIC的FIFO很可能又收到新的数据**, 并**trigger了中断**, 这时候, **Interrupt controller还没有umask**, 因此, 即便还有Processor B(也就是说有**处理器资源**), **中断控制器**也**无法把这个中断送达处理器系统**. 因此, 只能眼睁睁的看着NIC FIFO填满数据, 数据溢出, 或者向对端发出拥塞信号, 无论如何, 整体的系统性能是受到严重的影响. 

注意: 对于新的interrupt controller, 可能没有mask和umask操作, 但是原理是一样的, 只不过NIC的handler执行完毕要发生EOI而已. 

要解决上面的问题, 最重要的是尽快的**执行完中断handler, 打开中断, unmask IRQ(或者发送EOI**), 方法就是把**耗时的handle Data in the ram**这个步骤**踢出handler**, 让其在bottom half中执行. 

## 2.2 为何有softirq和tasklet

OK, linux kernel已经把中断处理分成了top half和bottom half, 看起来已经不错了, 那**为何**还要提供**softirq**、**tasklet**和**workqueue**这些bottom half机制, linux kernel本来就够复杂了, bottom half还来添乱. 实际上, 在早期的linux kernel还真是只有一个bottom half机制, 简称BH, 简单好用, 但是性能不佳. 后来, linux kernel的开发者开发了task queue机制, 试图来替代BH, 当然, 最后task queue也消失在内核代码中了. 现在的linux kernel提供了**三种bottom half的机制**, 来应对**不同的需求**. 

**workqueue**和**softirq**、**tasklet**有**本质的区别**: **workqueue**运行在**process context**, 而**softirq**和**tasklet**运行在**interrupt context**. 因此, 出现workqueue是不奇怪的, 在有**sleep需求**的场景中, defering task必须延迟到kernel thread中执行, 也就是说**必须使用workqueue机制**. softirq和tasklet是怎么回事呢?从本质上将, **bottom half机制**的设计有**两方面的需求**, 一个是**性能**, 一个是**易用性**. 设计一个通用的bottom half机制来满足这两个需求非常的困难, 因此, 内核提供了softirq和tasklet两种机制. **softirq更倾向于性能**, 而**tasklet更倾向于易用性**. 

我们还是进入实际的例子吧, 还是使用上一节的系统图. 在**引入softirq之后**, 网络数据的处理如下: 

**关中断**: **mask and ack interrupt controller**-------->**ack NIC**-------->**copy FIFO to ram**------>**raise softirq**------>**unmask interrupt controller**

**开中断**: 在**softirq**上下文中进行**handle Data in the ram**的动作

同样的, 我们先假设**Processor A**处理了这个网卡中断事件, 很快的完成了**基本的HW操作**后, **raise softirq(！！！**). 在**返回中断现场前**, 会**检查softirq的触发情况(！！！**), 因此, 后续**网络数据处理的softirq**在**Processor A**上执行. 在**softirq handler的执行过程中(！！！**), NIC硬件**再次触发中断(网卡新的中断！！！**), **Interrupt controller将该中断分发给processor B**, 执行动作**和Processor A是类似**的, 因此, 最后, 网络数据处理的softirq在**processor B**上执行. 

为了**性能**, **同一类型**的**softirq**有可能在**不同的CPU**上**并发执行**, 这给使用者带来了极大的痛苦, 因为**驱动工程师**在撰写softirq的回调函数的时候要**考虑重入**, **考虑并发**, 要引入**同步机制**. 但是, 为了性能, 我们必须如此. 

当**网络数据处理**的**softirq**同时在**Processor A和B**上运行的时候, **网卡中断又来**了(可能是10G的网卡吧). 这时候, **中断控制器分发**给**processor A**, 这时候, processor A上的handler仍然会**raise softirq**, 但是**并不会调度该softirq**. 也就是说, **softirq**在**一个CPU**上是**串行执行**的. 这种情况下, 系统**性能瓶颈**是**CPU资源**, 需要增加更多的CPU来解决该问题. 

如果是tasklet的情况会如何呢?为何tasklet性能不如softirq呢?如果**一个tasklet**在**processor A**上**被调度执行**, 那么它**永远也不会同时在processor B上执行(！！！**), 也就是说, **tasklet是串行执行**的(注: **不同的tasklet**还是会**并发**的), 不需要考虑重入的问题. 我们还是用网卡这个例子吧(注意: 这个例子仅仅是用来对比, **实际上**, **网络数据是使用softirq机制**的), 同样是上面的系统结构图. 假设使用tasklet, 网络数据的处理如下: 

关中断: mask and ack interrupt controller-------->ack NIC-------->copy FIFO to ram------>schedule tasklet------>unmask interrupt controller

开中断: 在softirq上下文中(一般使用**TASKLET\_SOFTIRQ这个softirq**)进行handle Data in the ram的动作

同样的, 我们先假设Processor A处理了这个网卡中断事件, 很快的完成了基本的HW操作后, **schedule tasklet**(同时也就**raise TASKLET\_SOFTIRQ softirq！！！**). 在**返回中断现场前**, 会**检查softirq的触发情况**, 因此, 在**TASKLET\_SOFTIRQ softirq的handler(！！！**)中, 获取**tasklet相关信息**并**在processor A上执行该tasklet的handler**. 在执行过程中, **NIC硬件再次触发中断**, **Interrupt controller**将**该中断分发给processor B**, 执行动作和Processor A是类似的, 虽然TASKLET\_SOFTIRQ softirq在processor B上可以执行, 但是, 在**检查tasklet的状态的时候**, 如果发现**该tasklet**在**其他processor**上**已经正在运行**, 那么**该tasklet不会被处理**, 一直等到**在processor A上的tasklet处理完**, 在**processor B上的这个tasklet才能被执行(！！！在B上执行！！！**). 这样的串行化操作虽然对驱动工程师是一个福利, 但是对性能而言是极大的损伤. 