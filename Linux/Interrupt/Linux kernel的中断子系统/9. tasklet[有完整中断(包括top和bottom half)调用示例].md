
<!-- @import "[TOC]" {cmd="toc" depthFrom=1 depthTo=6 orderedList=false} -->

<!-- code_chunk_output -->

* [1 前言](#1-前言)
* [2 为什么需要tasklet？](#2-为什么需要tasklet)
	* [2.1 基本的思考](#21-基本的思考)
	* [2.2 对linux中的bottom half机制的思考](#22-对linux中的bottom-half机制的思考)
* [3 tasklet的基本原理](#3-tasklet的基本原理)
	* [3.1 如何抽象一个tasklet](#31-如何抽象一个tasklet)
	* [3.2 系统如何管理tasklet？](#32-系统如何管理tasklet)
	* [3.3 如何定义一个tasklet？](#33-如何定义一个tasklet)
	* [3.4 如何调度一个tasklet](#34-如何调度一个tasklet)
	* [3.5 在什么时机会执行tasklet？](#35-在什么时机会执行tasklet)

<!-- /code_chunk_output -->

# 1 前言

对于中断处理而言, linux将其分成了两个部分, 一个叫做中断handler(top half), 属于**不那么紧急**需要处理的事情被推迟执行, 我们称之**deferable task**, 或者叫做**bottom half**, . 具体如何推迟执行分成下面几种情况: 

1、推迟到**top half执行完**毕

2、推迟到**某个指定的时间片(例如40ms)之后**执行

3、推迟到**某个内核线程被调度**的时候执行

对于**第一种**情况, 内核中的机制包括**softirq机制**和**tasklet机制**. **第二种**情况是属于**softirq机制**的一种应用场景(**timer类型的softirq**), 在本站的**时间子系统**的系列文档中会描述. **第三种**情况主要包括**threaded irq handler**以及**通用的workqueue机制**, 当然也包括**自己创建**该驱动专属kernel thread(**不推荐**使用). 本文主要描述tasklet这种机制, 第二章描述一些背景知识和和tasklet的思考, 第三章结合代码描述tasklet的原理. 

注: 本文中的linux kernel的版本是4.0

# 2 为什么需要tasklet？

## 2.1 基本的思考

我们的驱动程序或者内核模块真的需要tasklet吗？每个人都有自己的看法. 我们先抛开linux kernel中的机制, 首先进行一番逻辑思考. 

将**中断处理**分成**top half**(**cpu和外设之间的交互！！！**, **获取状态**, **ack状态**, **收发数据**等)和**bottom half**(后段的**数据处理**)已经深入人心, 对于任何的OS都一样, 将**不那么紧急**的事情推迟到**bottom half**中执行是OK的, 具体**如何推迟执行**分成**两种类型**: 有**具体时间要求**的(对应linux kernel中的**低精度timer和高精度timer**)和**没有具体时间要求**的. 对于**没有具体时间**要求的又可以分成两种: 

(1)**越快越好**型, 这种实际上是有**性能要求**的, 除了中断top half可以抢占其执行, 其他的进程上下文(无论该进程的优先级多么的高)是不会影响其执行的, 一言以蔽之, 在不影响中断延迟的情况下, OS会尽快处理. 

(2)**随遇而安**型. 这种属于那种没有性能需求的, 其调度执行依赖系统的调度器. 

本质上讲, 越快越好型的bottom half不应该太多, 而且tasklet的callback函数**不能执行时间过长**, 否则会产生进程调度延迟过大的现象, 甚至是非常长而且不确定的延迟, 对real time的系统会产生很坏的影响. 

## 2.2 对linux中的bottom half机制的思考

在linux kernel中, “**越快越好型**”有**两种**, **softirq**和**tasklet**, “**随遇而安**型”也有**两种**, **workqueue**和**threaded irq handler**. “**越快越好**型”能否**只留下一个softirq**呢？对于崇尚简单就是美的程序员当然希望如此. 为了回答这个问题, 我们先看看**tasklet**对于softirq而言有哪些**好处**: 

(1)**tasklet**可以**动态分配**, 也可以**静态分配**, 数量不限. 

(2)**同一种tasklet**在**多个cpu**上也**不会并行执行**, 这使得程序员在撰写tasklet function的时候比较方便, 减少了对**并发**的考虑(当然损失了性能). 

对于**第一种好处**, 其实也就是为乱用tasklet打开了方便之门, 很多撰写驱动的软件工程师不会仔细考量其driver**是否有性能需求**就直接使用了**tasklet机制**. 对于**第二种好处**, 本身**考虑并发**就是**软件工程师的职责**. 因此, 看起来tasklet并**没有引入特别的好处**, 而且**和softirq一样**, 都**不能sleep**, 限制了handler撰写的方便性, 看起来其实并没有存在的必要. 在4.0 kernel的代码中, grep一下tasklet的使用, 实际上是一个很长的列表, 只要对这些使用进行**简单的归类**就**可以删除对tasklet的使用**. 对于那些有**性能需求**的, 可以考虑**并入softirq**, 其他的可以考虑**使用workqueue**来取代. Steven Rostedt试图进行这方面的尝试( http://lwn.net/Articles/239484/ ), 不过这个patch始终未能进入main line. 

# 3 tasklet的基本原理

## 3.1 如何抽象一个tasklet

内核中用下面的数据结构来表示tasklet: 

```c
[include/linux/interrupt.h]
struct tasklet_struct 
{ 
    struct tasklet_struct *next; 
    unsigned long state; 
    atomic_t count; 
    void (*func)(unsigned long); 
    unsigned long data; 
};
```

**每个cpu**都会**维护一个链表(！！！**), 将**本cpu**需要处理的**tasklet管理**起来, **next**这个成员指向了该**链表中的下一个tasklet**. **func**和**data**成员描述了**该tasklet的callback函数**, func是**调用函数**, data是**传递给func的参数**. state成员表示**该tasklet的状态**, **TASKLET\_STATE\_SCHED**表示该tasklet已经**被调度到某个CPU上执行**, TASKLET\_STATE\_RUN表示该tasklet**正在某个cpu上执行**. **count成员**是和**enable或者disable该tasklet**的状态相关, 如果**count等于0**那么该tasklet是处于**enable**的, 如果大于0, 表示该tasklet是disable的. 在**softirq文档**中, 我们知道**local\_bh\_disable/enable函数**就是用来**disable/enable bottom half(！！！所有softirq和tasklet！！！**)的, 这里就**包括softirq和tasklet**. 但是, 有的时候内核同步的场景不需disable所有的softirq和tasklet, 而仅仅是**disable该tasklet**, 这时候, tasklet\_disable和tasklet\_enable就派上用场了. 

```c
static inline void tasklet_disable(struct tasklet_struct *t) 
{ 
    tasklet_disable_nosync(t);－－－－－－－给tasklet的count加一 
    tasklet_unlock_wait(t);－－－－－如果该tasklet处于running状态, 那么需要等到该tasklet执行完毕 
    smp_mb(); 
}

static inline void tasklet_enable(struct tasklet_struct *t) 
{ 
    smp_mb__before_atomic(); 
    atomic_dec(&t->count);－－－－－－－给tasklet的count减一 
}
```

tasklet\_disable和tasklet\_enable支持嵌套, 但是需要成对使用. 

## 3.2 系统如何管理tasklet？

系统中的**每个cpu**都会维护**一个tasklet的链表(！！！**), 定义如下: 

```c
static DEFINE_PER_CPU(struct tasklet_head, tasklet_vec); 
static DEFINE_PER_CPU(struct tasklet_head, tasklet_hi_vec);
```

linux kernel中, 和**tasklet**相关的softirq有两项, **HI\_SOFTIRQ**用于**高优先级的tasklet**, **TASKLET\_SOFTIRQ**用于**普通的tasklet**. 对于**softirq**而言, **优先级**就是出现在**softirq pending register(\_\_softirq\_pending)中的先后顺序(！！！**), 位于**bit 0拥有最高的优先级**, 也就是说, 如果有**多个不同类型**的**softirq同时触发**, 那么**执行的先后顺序**依赖在**softirq pending register的位置**, kernel总是从右向左依次判断是否置位, 如果置位则执行. **HI\_SOFTIRQ占据了bit 0**, 其优先级甚至**高过timer**, 需要慎用(实际上, 我grep了内核代码, 似乎没有发现对HI\_SOFTIRQ的使用). 当然**HI\_SOFTIRQ和TASKLET\_SOFTIRQ的机理**是一样的, 因此本文只讨论TASKLET\_SOFTIRQ, 大家可以举一反三. 

## 3.3 如何定义一个tasklet？

你可以用下面的宏定义来**静态定义tasklet**: 

```c
#define DECLARE_TASKLET(name, func, data) \ 
struct tasklet_struct name = { NULL, 0, ATOMIC_INIT(0), func, data }

#define DECLARE_TASKLET_DISABLED(name, func, data) \ 
struct tasklet_struct name = { NULL, 0, ATOMIC_INIT(1), func, data }
```

这两个宏都可以静态定义一个struct tasklet\_struct的变量, 只不过初始化后的tasklet一个是处于eable状态, 一个处于disable状态的. 当然, 也可以**动态分配tasklet**, 然后调用**tasklet\_init**来**初始化该tasklet**. 

## 3.4 如何调度一个tasklet

为了**调度一个tasklet执行**, 我们可以使用**tasklet\_schedule**这个接口: 

```c
static inline void tasklet_schedule(struct tasklet_struct *t) 
{ 
    if (!test_and_set_bit(TASKLET_STATE_SCHED, &t->state)) 
        __tasklet_schedule(t); 
}
```

程序在**多个上下文**中可以**多次调度同一个tasklet执行**(也可能**来自多个cpu core**), 不过实际上**该tasklet只会一次挂入首次调度到的那个cpu的tasklet链表(！！！**), 也就是说, 即便是**多次调用tasklet\_schedule**, 实际上**tasklet只会挂入一个指定CPU的tasklet队列**中(而且**只会挂入一次**), 也就是说只会调度一次执行. 这是通过**TASKLET\_STATE\_SCHED这个flag**来完成的, 我们可以用下面的图片来描述: 

![config](./images/11.gif)

我们假设**HW block A**的**驱动**使用的**tasklet机制**并且在**中断handler(top half！！！**)中将**静态定义的tasklet**(这个tasklet是**各个cpu共享**的, 不是per cpu的)**调度执行**(也就是**调用tasklet\_schedule！！！**函数). 当**HW block A检测到硬件**的动作(例如**接收FIFO中数据达到半满**)就会**触发IRQ line上的电平或者边缘信号**, **GIC**检测到**该信号**会将**该中断**分发给**某个CPU执行其top half handler(！！！**), 我们假设这次是**cpu0**, 因此**该driver的tasklet**被挂入**CPU0对应的tasklet链表！！！**(tasklet\_vec)并**将state的状态**设定为**TASKLET\_STATE\_SCHED**. **HW block A的驱动**中的**tasklet虽已调度**, 但是**没有执行(！！！**), 如果这时候, 硬件**又一次触发中断**并**在cpu1上执行top handler！！！**, 虽然**tasklet\_schedule函数被再次调用**, 但是由于**TASKLET\_STATE\_SCHED已经设定**, 因此**不会(！！！**)将**HW block A的驱动**中的这个**tasklet挂入cpu1的tasklet链表**中. 

下面我们再仔细研究一下底层的\_\_tasklet\_schedule函数: 

```c
void __tasklet_schedule(struct tasklet_struct *t) 
{ 
    unsigned long flags;

    local_irq_save(flags);－－－－(1) 
    t->next = NULL;－－－－(2) 
    *__this_cpu_read(tasklet_vec.tail) = t; 
    __this_cpu_write(tasklet_vec.tail, &(t->next)); 
    raise_softirq_irqoff(TASKLET_SOFTIRQ);－－－－(3) 
    local_irq_restore(flags); 
}
```

(1)下面的**链表操作是per\-cpu**的, 因此这里**禁止本地中断**就可以**拦截所有的并发**. 

(2)这里的三行代码就是**将一个tasklet挂入链表的尾部**

(3)**raise TASKLET\_SOFTIRQ类型**的**softirq**. 

## 3.5 在什么时机会执行tasklet？

上面描述了tasklet的调度, 当然**调度tasklet不等于执行tasklet(！！！**), 系统会在适合的时间点执行tasklet callback function. 由于**tasklet是基于softirq**的, 因此, 我们首先总结一下**softirq的执行场景**: 

(1)在**中断返回用户空间(进程上下文**)的时候, 如果有**pending的softirq**, 那么将执行该softirq的处理函数. 这里限定了**中断返回用户空间**也就是意味着**限制了下面两个场景的softirq被触发执行**: 

(a)中断返回hard interrupt context, 也就是**中断嵌套**的场景

(b)中断返回software interrupt context, 也就是**中断抢占软中断上下文**的场景

(2)上面的描述缺少了一种场景: **中断返回内核态的进程上下文**的场景, 这里我们需要详细说明. **进程上下文**中调用**local\_bh\_enable**的时候, 如果有**pending的softirq**, 那么将**执行该softirq的处理函数**. 由于**内核同步**的要求, **进程上下文**中有可能会调用**local\_bh\_enable/disable来保护临界区**. 在**临界区代码执行过程**中, **中断随时**会到来, **抢占该进程(内核态)的执行**(注意: 这里**只是disable了bottom half, 没有禁止中断！！！**). 在这种情况下, 中断返回的时候是否会执行softirq handler呢？当然不会, 我们disable了bottom half的执行, 也就是意味着不能执行softirq handler, 但是**本质上bottom half应该比进程上下文有更高的优先级(！！！**), 一旦条件允许, 要**立刻抢占进程上下文(！！！**)的执行, 因此, 当立刻离开临界区, 调用**local\_bh\_enable**的时候, 会**检查softirq pending**, 如果**bottom half处于enable的状态**, pending的s**oftirq handler会被执行**. 

(3)系统太繁忙了, 不过的**产生中断**, **raise softirq**, 由于**bottom half的优先级高**, 从而导致进程无法调度执行. 这种情况下, **softirq会推迟到softirqd这个内核线程**中去执行. 

对于**TASKLET\_SOFTIRQ类型的softirq**, 其**handler是tasklet\_action**, 我们来看看各个tasklet是如何执行的: 

```c
static void tasklet_action(struct softirq_action *a) 
{ 
    struct tasklet_struct *list;

    local_irq_disable();－－－－－－－－－－(1) 
    list = __this_cpu_read(tasklet_vec.head); 
    __this_cpu_write(tasklet_vec.head, NULL); 
    __this_cpu_write(tasklet_vec.tail, this_cpu_ptr(&tasklet_vec.head)); 
    local_irq_enable();

    while (list) {－－－－－－－－－遍历tasklet链表 
        struct tasklet_struct *t = list;

        list = list->next;

        if (tasklet_trylock(t)) {－－－－－－－－(2) 
            if (!atomic_read(&t->count)) {－－－－－－(3) 
                if (!test_and_clear_bit(TASKLET_STATE_SCHED, &t->state)) 
                    BUG(); 
                t->func(t->data); 
                tasklet_unlock(t); 
                continue;－－－－－处理下一个tasklet 
            } 
            tasklet_unlock(t);－－－－清除TASKLET_STATE_RUN标记 
        }

        local_irq_disable();－－－－－－－(4) 
        t->next = NULL; 
        *__this_cpu_read(tasklet_vec.tail) = t; 
        __this_cpu_write(tasklet_vec.tail, &(t->next)); 
        __raise_softirq_irqoff(TASKLET_SOFTIRQ); －－－－－－再次触发softirq, 等待下一个执行时机 
        local_irq_enable(); 
    } 
}
```

(1)从本cpu的tasklet链表中取出全部的tasklet, 保存在list这个临时变量中, 同时重新初始化本cpu的tasklet链表, 使该链表为空. 由于bottom half是开中断执行的, 因此在操作tasklet链表的时候需要使用关中断保护

(2)tasklet\_trylock主要是用来设定该tasklet的state为TASKLET\_STATE\_RUN, 同时判断该tasklet是否已经处于执行状态, 这个状态很重要, 它决定了后续的代码逻辑. 

```c
static inline int tasklet_trylock(struct tasklet_struct *t) 
{ 
    return !test_and_set_bit(TASKLET_STATE_RUN, &(t)->state); 
}
```

你也许会奇怪: 为何这里从tasklet的链表中摘下一个本cpu要处理的tasklet list, 而这个list中的tasklet已经处于running状态了, 会有这种情况吗？会的, 我们再次回到上面的那个软硬件结构图. 同样的, HW block A的驱动使用的tasklet机制并且在中断handler(top half)中将静态定义的tasklet 调度执行. HW block A的硬件中断首先送达cpu0处理, 因此该driver的tasklet被挂入CPU0对应的tasklet链表并在适当的时间点上开始执行该tasklet. 这时候, cpu0的硬件中断又来了, 该driver的tasklet callback function被抢占, 虽然tasklet仍然处于running状态. 与此同时, HW block A硬件又一次触发中断并在cpu1上执行, 这时候, 该driver的tasklet处于running状态, 并且TASKLET\_STATE\_SCHED已经被清除, 因此, 调用tasklet\_schedule函数将会使得该driver的tasklet挂入cpu1的tasklet链表中. 由于cpu0在处理其他硬件中断, 因此, cpu1的tasklet后发先至, 进入tasklet_action函数调用, 这时候, 当从cpu1的tasklet摘取所有需要处理的tasklet链表中, HW block A对应的tasklet实际上已经是在cpu0上处于执行状态了. 

我们在设计tasklet的时候就规定, 同一种类型的tasklet只能在一个cpu上执行, 因此tasklet_trylock就是起这个作用的. 

(3)检查该tasklet是否处于enable状态, 如果是, 说明该tasklet可以真正进入执行状态了. 主要的动作就是清除TASKLET\_STATE\_SCHED状态, 执行tasklet callback function. 

(4)如果该tasklet已经在别的cpu上执行了, 那么我们将其挂入该cpu的tasklet链表的尾部, 这样, 在下一个tasklet执行时机到来的时候, kernel会再次尝试执行该tasklet, 在这个时间点, 也许其他cpu上的该tasklet已经执行完毕了. 通过这样代码逻辑, 保证了特定的tasklet只会在一个cpu上执行, 不会在多个cpu上并发. 