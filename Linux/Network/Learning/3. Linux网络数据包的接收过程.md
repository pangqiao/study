- 1 网卡到内存
- 2 内核的网络模块
- 3 协议栈
    - 3.1 IP层
    - 3.2 UDP层
- 4 socket
- 5 总结语
- 6 参考

https://segmentfault.com/a/1190000008836467

介绍在Linux系统中, 数据包是如何一步一步从网卡传到进程手中的. 

强烈建议阅读后面参考里的两篇文章, 里面介绍的更详细. 

本文只讨论以太网的物理网卡, 不涉及虚拟设备, 并且以一个UDP包的接收过程作为示例.

>本示例里列出的函数调用关系来自于kernel 3.13.0

# 1 网卡到内存

网卡需要有**驱动**才能工作, 驱动是加载到**内核中的模块**, 负责衔接**网卡**和**内核**的**网络模块**, 驱动在**加载**的时候将自己**注册进网络模块**, 当相应的**网卡**收到**数据包**时, **网络模块**会调用相应的驱动程序处理数据. 

下图展示了数据包(packet)如何进入内存, 并被内核的网络模块开始处理: 

```
                   +-----+
                   |     |                            Memroy
+--------+   1     |     |  2  DMA     +--------+--------+--------+--------+
| Packet |-------->| NIC |------------>| Packet | Packet | Packet | ...... |
+--------+         |     |             +--------+--------+--------+--------+
                   |     |<--------+
                   +-----+         |
                      |            +---------------+
                      |                            |
                    3 | Raise IRQ                  | Disable IRQ
                      |                          5 |
                      |                            |
                      ↓                            |
                   +-----+                   +------------+
                   |     |  Run IRQ handler  |            |
                   | CPU |------------------>| NIC Driver |
                   |     |       4           |            |
                   +-----+                   +------------+
                                                   |
                                                6  | Raise soft IRQ
                                                   |
                                                   ↓
```

- 1:  **数据包**从**外面的网络**进入**物理网卡**. 如果**目的地址不是该网卡(！！！**), 且该**网卡没有开启混杂模式(！！！**), 该包会**被网卡丢弃**. 

- 2:  网卡(驱动)将数据包通过**DMA的方式(！！！**)写入到**指定的内存地址**, 该地址由**网卡驱动分配并初始化(！！！**). 

    注: **老的网卡**可能**不支持DMA**, 不过新的网卡一般都支持. 

- 3:  网卡通过**硬件中断(IRQ**)通知**CPU**, 告诉它有数据来了

- 4:  CPU根据**中断表**, 调用已经注册的**中断函数**, 这个中断函数会调到**驱动程序(NIC Driver**)中相应的函数

- 5:  **驱动**先**禁用网卡的中断(不是CPU中断！！！**), 表示驱动程序已经知道内存中有数据了, 告诉网卡下次再收到数据包**直接写内存(！！！**)就可以了, **不要再通知CPU(驱动程序自己的行为, 驱动收到网络数据！！！**)了, 这样可以提高效率, **避免CPU不停的被中断**. 

- 6:  启动**软中断**. 这步结束后, **硬件中断**处理函数就**结束返回**了. 由于**硬中断处理程序**执行的过程中**不能被中断**, 所以如果它执行时间过长, 会导致CPU没法响应其它硬件的中断, 于是内核引入软中断, 这样可以将硬中断处理函数中耗时的部分移到软中断处理函数里面来慢慢处理. 

# 2 内核的网络模块

软中断会触发**内核网络模块(！！！**)中的**软中断处理函数(！！！**), 后续流程如下

```
                                                     +-----+
                                             17      |     |
                                        +----------->| NIC |
                                        |            |     |
                                        |Enable IRQ  +-----+
                                        |
                                        |
                                  +------------+                                      Memroy
                                  |            |        Read           +--------+--------+--------+--------+
                 +--------------->| NIC Driver |<--------------------- | Packet | Packet | Packet | ...... |
                 |                |            |          9            +--------+--------+--------+--------+
                 |                +------------+
                 |                      |    |        skb
            Poll | 8      Raise softIRQ | 6  +-----------------+
                 |                      |             10       |
                 |                      ↓                      ↓
         +---------------+  Call  +-----------+        +------------------+        +--------------------+  12  +---------------------+
         | net_rx_action |<-------| ksoftirqd |        | napi_gro_receive |------->| enqueue_to_backlog |----->| CPU input_pkt_queue |
         +---------------+   7    +-----------+        +------------------+   11   +--------------------+      +---------------------+
                                                               |                                                      | 13
                                                            14 |        + - - - - - - - - - - - - - - - - - - - - - - +
                                                               ↓        ↓
                                                    +--------------------------+    15      +------------------------+
                                                    | __netif_receive_skb_core |----------->| packet taps(AF_PACKET) |
                                                    +--------------------------+            +------------------------+
                                                               |
                                                               | 16
                                                               ↓
                                                      +-----------------+
                                                      | protocol layers |
                                                      +-----------------+
```

- 7:  内核中的**ksoftirqd进程**专门负责**软中断**的处理, 当它收到软中断后, 就会调用相应软中断所对应的处理函数, 对于上面第6步中是网卡驱动模块抛出的软中断, ksoftirqd会调用网络模块的**net\_rx\_action**函数

- 8:  net\_rx\_action调用**网卡驱动**里的**poll函数**来一个一个的**处理数据包**

- 9:  在pool函数中, 驱动会**一个接一个(！！！**)的**读取网卡写到内存中的数据包(！！！**), 内存中**数据包的格式只有驱动(！！！**)知道

- 10:  **驱动程序**将**内存中的数据包**转换成**内核网络模块**能识别的**skb格式**, 然后调用napi\_gro\_receive函数

- 11:  napi\_gro\_receive会处理GRO相关的内容, 也就是**将可以合并的数据包进行合并**, 这样就**只需要调用一次协议栈**. 然后判断是否开启了RPS, 如果开启了, 将会调用enqueue\_to\_backlog

- 12:  在enqueue\_to\_backlog函数中, 会将**数据包**放入CPU的**softnet\_data结构体**的**input\_pkt\_queue**中, 然后返回, 如果input\_pkt\_queue满了的话, 该数据包将会被丢弃, queue的大小可以通过net.core.netdev\_max\_backlog来配置

- 13:  CPU会接着在自己的**软中断上下文**中处理自己input\_pkt\_queue里的网络数据(调用\_\_netif\_receive\_skb\_core)

- 14:  如果没开启RPS, napi\_gro\_receive会直接调用\_\_netif\_receive\_skb\_core

- 15:  看是不是有AF\_PACKET类型的**socket(也就是我们常说的原始套接字**), 如果有的话, 拷贝一份数据给它. **tcpdump抓包就是抓的这里的包！！！**. 

- 16:  **调用协议栈相应的函数**, 将**数据包**交给**协议栈(！！！**)处理. 

- 17:  待内存中的所有数据包被处理完成后(即**poll函数执行完成**), 启用网卡的硬中断, 这样下次网卡再收到数据的时候就会通知CPU

>enqueue\_to\_backlog函数也会被netif\_rx函数调用, 而netif\_rx正是lo设备发送数据包时调用的函数

# 3 协议栈

## 3.1 IP层

由于是UDP包, 所以第一步会进入IP层, 然后一级一级的函数往下调: 

```
          |
          |
          ↓         promiscuous mode &&
      +--------+    PACKET_OTHERHOST (set by driver)   +-----------------+
      | ip_rcv |-------------------------------------->| drop this packet|
      +--------+                                       +-----------------+
          |
          |
          ↓
+---------------------+
| NF_INET_PRE_ROUTING |
+---------------------+
          |
          |
          ↓
      +---------+
      |         | enabled ip forword  +------------+        +----------------+
      | routing |-------------------->| ip_forward |------->| NF_INET_FORWARD |
      |         |                     +------------+        +----------------+
      +---------+                                                   |
          |                                                         |
          | destination IP is local                                 ↓
          ↓                                                 +---------------+
 +------------------+                                       | dst_output_sk |
 | ip_local_deliver |                                       +---------------+
 +------------------+
          |
          |
          ↓
 +------------------+
 | NF_INET_LOCAL_IN |
 +------------------+
          |
          |
          ↓
    +-----------+
    | UDP layer |
    +-----------+
```

- ip\_rcv:  **ip\_rcv**函数是**IP模块的入口函数**, 在该函数里面, 第一件事就是将**垃圾数据包**(**目的mac地址不是当前网卡！！！**, 但由于**网卡设置了混杂模式而被接收进来！！！**)直接丢掉, 然后调用注册在NF\_INET\_PRE\_ROUTING上的函数

- NF\_INET\_PRE\_ROUTING:  netfilter放在协议栈中的钩子, 可以通过iptables来注入一些数据包处理函数, 用来修改或者丢弃数据包, 如果数据包没被丢弃, 将继续往下走

- routing:  进行路由, 如果是**目的IP不是本地IP**, 且**没有开启ip forward功能**, 那么数据包将被丢弃, 如果开启了ip forward功能, 那将进入ip\_forward函数

- ip\_forward:  ip\_forward会先调用netfilter注册的NF\_INET\_FORWARD相关函数, 如果数据包没有被丢弃, 那么将继续往后调用dst\_output\_sk函数

- dst\_output\_sk:  该函数会调用IP层的相应函数将该数据包发送出去, 同下一篇要介绍的数据包发送流程的后半部分一样. 

- ip\_local\_deliver: 如果上面routing的时候发现目的IP是本地IP, 那么将会调用该函数, 在该函数中, 会先调用NF\_INET\_LOCAL\_IN相关的钩子程序, 如果通过, 数据包将会向下发送到UDP层

## 3.2 UDP层

```
          |
          |
          ↓
      +---------+            +-----------------------+
      | udp_rcv |----------->| __udp4_lib_lookup_skb |
      +---------+            +-----------------------+
          |
          |
          ↓
 +--------------------+      +-----------+
 | sock_queue_rcv_skb |----->| sk_filter |
 +--------------------+      +-----------+
          |
          |
          ↓
 +------------------+
 | __skb_queue_tail |
 +------------------+
          |
          |
          ↓
  +---------------+
  | sk_data_ready |
  +---------------+
```

- udp\_rcv: udp\_rcv函数是UDP模块的入口函数, 它里面会调用其它的函数, 主要是做一些必要的检查, 其中一个重要的调用是\_\_udp4\_lib\_lookup\_skb, 该函数会根据**目的IP和端口**找对应的**socket**, 如果没有找到相应的socket, 那么该数据包将会被丢弃, 否则继续

- sock\_queue\_rcv\_skb:  主要干了两件事, 一是检查这个**socket**的**receive buffer**是不是满了, 如果满了的话, 丢弃该数据包, 然后就是调用sk\_filter看这个包是否是满足条件的包, 如果当前socket上设置了filter, 且该包不满足条件的话, 这个数据包也将被丢弃(在Linux里面, 每个socket上都可以像tcpdump里面一样定义filter, 不满足条件的数据包将会被丢弃)

- \_\_skb\_queue\_tail: 将**数据包**放入**socket接收队列的末尾**

- sk\_data\_ready: 通知socket数据包已经准备好

>调用完sk\_data\_ready之后, 一个数据包处理完成, 等待应用层程序来读取, 上面所有函数的执行过程都在**软中断的上下文(协议栈操作在软中断上下文！！！**)中. 

# 4 socket

**应用层**一般有**两种方式接收数据**, 一种是recvfrom函数阻塞在那里等着数据来, 这种情况下当socket收到通知后, recvfrom就会被唤醒, 然后读取接收队列的数据; 另一种是通过**epoll或者select监听相应的socket**, 当收到通知后, 再**调用recvfrom函数**去读取接收队列的数据. 两种情况都能正常的接收到相应的数据包. 

# 5 总结语

了解数据包的接收流程有助于帮助我们搞清楚我们可以在哪些地方监控和修改数据包, 哪些情况下数据包可能被丢弃, 为我们处理网络问题提供了一些参考, 同时了解netfilter中相应钩子的位置, 对于了解iptables的用法有一定的帮助, 同时也会帮助我们后续更好的理解Linux下的网络虚拟设备. 

总结: 

1. 数据从物理网线/无线方式到达网卡后, 网卡驱动会通过DMA操作(网卡一般自带了DMA控制器, 没有使用主板上通用的)将网卡的数据拷贝到物理内存(网卡驱动初始化分配的, 所以这是一段内核态内存), 然后网卡触发硬件中断, 驱动先禁用网卡的中断(不是CPU中断), 网卡下次再收到数据包直接写内存, 这样避免CPU不停的被中断. 最后数据处理主要在下半部, 软中断中.

2. 驱动程序一个一个处理内存中数据包, 将内存中的数据包(之前只有驱动能识别)转换成内核网络模块能识别的skb格式, 然后整合等操作, 将其转发给协议栈(协议栈动作也是在软中断上下文), 等到内存中数据处理完后开启网卡中断

3. IP层将目的mac非当前网卡的数据包丢弃(由于网卡的混杂模式接收的), 非本地IP且没有IP forward则丢弃; UDP层根据目的IP和端口找对应的socket, socket的receive buffer没满并且没被过滤则将数据放到队列, 然后通过socket; 应用层会将数据拷贝到**自己的用户空间(用户态内存空间！！！**)

在接下来的几篇文章中, 将会介绍Linux下的网络虚拟设备和iptables. 

# 6 参考

- [Monitoring and Tuning the Linux Networking Stack: Receiving Data](https://blog.packagecloud.io/eng/2016/06/22/monitoring-tuning-linux-networking-stack-receiving-data/)
- [Illustrated Guide to Monitoring and Tuning the Linux Networking Stack: Receiving Data](https://blog.packagecloud.io/eng/2016/10/11/monitoring-tuning-linux-networking-stack-receiving-data-illustrated/)
- [NAPI](https://wiki.linuxfoundation.org/networking/napi)