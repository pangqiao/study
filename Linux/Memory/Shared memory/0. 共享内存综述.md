
早期的共享内存，着重于强调把同一片内存，map 到多个进程的虚拟地址空间（在**相应进程**找到一个 **VMA 区域**），以便于 CPU 可以在各个进程访问到这片内存。

![2024-02-01-12-58-54.png](./images/2024-02-01-12-58-54.png)

**现阶段**广泛应用于多媒体、Graphics领域的**共享内存方式**，某种意义上**不再强调**映射到**进程虚拟地址空间**的概念（那无非是**为了让 CPU 访问**），而更强调以某种 “**句柄**” 的形式，让大家知道某一片视频、图形图像数据的存在并可以借助此 “句柄” 来跨进程引用这片内存，让视频 encoder、decoder、GPU 等可以跨进程访问内存。所以不同进程用的加速硬件其实是不同的，他们更在乎的是可以通过一个 handle 拿到这片内存，而不再特别在乎 CPU 访问它的虚拟地址（当然仍然可以映射到进程的虚拟地址空间供 CPU 访问）。

![2024-02-01-13-10-15.png](./images/2024-02-01-13-10-15.png)

只要内存的拷贝(memcpy)仍然是一个占据内存带宽、CPU利用率的消耗大户存在，共享内存作为Linux进程间通信、计算机系统里各个不同硬件组件通信的最高效方法，都将持续繁荣。关于内存拷贝会大多程度地占据CPU利用率，这个可以最简单地尝试拷贝 1080P，帧率每秒60的电影画面，我保证你的系统的CPU，蛋会疼地不行。

综述Linux里面各种共享内存方式, 共享内存的方式有很多种，目前主流的方式仍然有：

共享内存的方式

1.基于传统 SYS V 的共享内存；

2.基于 POSIX mmap 文件映射实现共享内存；

3.通过 memfd_create() 和 fd 跨进程共享实现共享内存；

4.多媒体、图形领域广泛使用的基于 dma-buf 的共享内存。

# SYS V共享内存

历史悠久、年代久远、API怪异，对应内核代码 `linux/ipc/shm.c`，当你编译内核的时候**不选择** `CONFIG_SYSVIPC`，则不再具备此能力。

你在 Linux 敲 ipcs 命令看到的 share memory 就是这种共享内存：

![2024-02-01-13-21-53.png](./images/2024-02-01-13-21-53.png)

下面写一个最简单的程序来看共享内存的**写端** `sw.c`：

```cpp
#include <sys/shm.h>
#include <unistd.h>
#include <string.h>

int main(int argc, char **argv)
{
    key_t key = ftok("/dev/shm/myshm2", 0);
    int shm_id = shmget(key, 0x400000, IPC_CREAT | 0666);
    char *p = (char *)shmat(shm_id, NULL, 0);

    memset(p, 'A', 0x400000);
    shmdt(p);

    return 0;
}
```

以及共享内存的**读端** `sr.c`:

```cpp
#include <sys/shm.h>
#include <unistd.h>
#include <stdio.h>

int main(int argc, char **argv)
{
    key_t key = ftok("/dev/shm/myshm2", 0);
    int shm_id = shmget(key, 0x400000, 0666);
    char *p = (char *)shmat(shm_id, NULL, 0);

    printf("%c %c %c %c\n", p[0], p[1], p[2], p[3]);
    shmdt(p);

    return 0;
}
```

编译和准备运行:

```
gcc sw.c -o sw
gcc sr.c -o sr
touch /dev/shm/myshm2
```

在此之前我们看一下系统的 free:

![2024-02-01-13-33-58.png](./images/2024-02-01-13-33-58.png)

下面运行sw和sr：

```
# ./sw
# ./sr
A A A A
```

我们再看下 free:

![2024-02-01-13-35-17.png](./images/2024-02-01-13-35-17.png)

可以看到 **used** 显著增大了(1376548 -> 1380552), **shared** 显著地增大了(41808 -> 45904)，而 cached这一列也显著地增大(14721288 -> 14725436)。

我们都知道 cached 这一列统计的是 `file-backed` 的文件的 **page cache** 的大小。理论上，**共享内存**属于**匿名页**，但是由于这里面有个非常特殊的 `tmpfs`(`/run/shm` 指向 `/dev/shm`, `/dev/shm` 则 mount 为 **tmpfs**)：

![2024-02-01-13-47-23.png](./images/2024-02-01-13-47-23.png)

所以可以看出 tmpfs 的东西其实真的是有点含混：我们可以理解它为 **file-backed** 的**匿名页**(`anonymous page`). 前面我们反复强调, **匿名页**是**没有文件背景**的，这样当进行**内存交换**的时候，是与 swap 分区交换。**磁盘文件系统**里面的东西在内存的副本是 file-backed 的页面，所以不存在与 swap 分区交换的问题。但是 tmpfs 里面的东西，真的是在统计意义上统计到 **page cache** 了，但是它**并没有真实的磁盘背景**，这又和你访问磁盘文件系统里面的文件产生的 page cache 有本质的区别。所以，它是真地有那么一点 misc 的感觉，凡事都没有绝对，唯有变化本身是不变的。

也可以通过 ipcs 找到新创建的 SYS V 共享内存：

![2024-02-01-13-51-28.png](./images/2024-02-01-13-51-28.png)

`4194304 bytes` = `0x400000 bytes`

# POSIX 共享内存

我对POSIX shm_open()、mmap () API系列的共享内存的喜爱，远远超过SYS V 100倍。原谅我就是一个懒惰的人，我就是讨厌ftok、shmget、shmat、shmdt这样的API。

上面的程序如果用POSIX的写法，可以简化成**写端** psw.c：

```cpp
#include <sys/mman.h>
#include <sys/stat.h>   /* For mode constants */
#include <fcntl.h>      /* For O_* constants */
#include <stdlib.h>
#include <string.h>

#define SIZE 0x400000
int main(int argc, char **argv)
{
    int fd = shm_open("posixsm", O_CREAT | O_RDWR, 0666);
    ftruncate(fd, SIZE);

    char *p = mmap(NULL, SIZE, PROT_READ | PROT_WRITE, MAP_SHARED, fd, 0);
    memset(p, 'A', SIZE);
    munmap(p, SIZE);

    return 0;
}
```

**读端** psr.c

```cpp
#include <sys/mman.h>
#include <sys/stat.h>   /* For mode constants */
#include <fcntl.h>      /* For O_* constants */
#include <stdlib.h>
#include <stdio.h>

#define SIZE 0x400000
int main(int argc, char **argv)
{
        int fd = shm_open("posixsm", O_CREAT | O_RDWR, 0666);
        ftruncate(fd, SIZE);

        char *p = mmap(NULL, SIZE, PROT_READ, MAP_SHARED, fd, 0);

        printf("%c %c %c %c\n", p[0], p[1], p[2], p[3]);

        munmap(0, SIZE);

        return 0;
}
```

编译和执行:

```
# gcc psr.c -o psr -lrt
# gcc psw.c -o psw -lrt
# ./psw
# ./psr
A A A A
```

这样我们会在 `/dev/shm/`、`/run/shm` 下面看到一个文件：

![2024-02-01-14-13-35.png](./images/2024-02-01-14-13-35.png)

当然，如果你不喜欢 `shm_open()` 这个 API，你也可以用常规的 open 来打开文件，然后进行 mmap。关键的是 mmap.

POSIX 的共享内存，仍然符合我们前面说的 tmpfs 的特点，在运行了 sw, sr 后，再运行 psw 和 psr，我们发现 free 命令再次戏剧性变化：

![2024-02-01-14-16-43.png](./images/2024-02-01-14-16-43.png)

# memfd_create

如果说 POSIX 的 mmap 让我找到回家的感觉，那么 `memfd_create()` 则是万般惊艳.

在所有的所有开始之前，我们要先提一下跨进程分享fd（文件描述符，对应我们很多时候说的 “**句柄**”）这个重要的概念。

众所周知，Linux 的 fd 属于一个**进程级别**的东西。进入每个进程的 `/proc/pid/fd` 可以看到它的fd的列表：



# reference

来自 宋宝华: https://cloud.tencent.com/developer/article/1551288