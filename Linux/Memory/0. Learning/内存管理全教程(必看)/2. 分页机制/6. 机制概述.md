
<!-- @import "[TOC]" {cmd="toc" depthFrom=1 depthTo=6 orderedList=false} -->

<!-- code_chunk_output -->

- [分页机制](#分页机制)
  - [为什么使用多级页表来完成映射](#为什么使用多级页表来完成映射)
  - [32位系统中2级页表](#32位系统中2级页表)
  - [64位系统中的分页](#64位系统中的分页)
  - [Linux中的分页](#linux中的分页)
- [页表](#页表)
- [Linux分页机制的演变](#linux分页机制的演变)
  - [Linux的页表实现](#linux的页表实现)
  - [Linux最初的二级页表](#linux最初的二级页表)
  - [Linux的三级页表](#linux的三级页表)
  - [Linux的四级页表](#linux的四级页表)
- [链接](#链接)

<!-- /code_chunk_output -->

# 分页机制

在**虚拟内存**中, **页表**是个**映射表**的概念,即从进程能理解的**线性地址(进程使用线性地址, 不管物理地址！！！**)(linear address)映射到存储器上的**物理地址**(phisical address).

也就是说, **进程**使用的是**线性地址**, 进程使用的**空间大小**取决于**线性地址**, 不关心物理地址, 页表的目的是为了将**线性地址**映射到**物理地址**, 所以**页表大小取决于线性地址空间**, 而且映射之前先是在线性空间分配空间, 然后建立页表来映射相应大小的物理地址, 即使用多少线性空间映射多少, 不是一次性映射所有！！！

很显然, 这个**页表**是需要**常驻内存**的东西,以应对频繁的查询映射需要(实际上, 现代支持VM的处理器都有**TLB的硬件级页表缓存部件**, 本文不讨论).

## 为什么使用多级页表来完成映射

**但是为什么要使用多级页表来完成映射呢**?

用来将虚拟地址映射到物理地址的数据结构称为**页表**,实现两个地址空间的关联**最容易的方式是使用数组**,对虚拟地址空间中的**每一页**,都分配一个数组项. 该数组指向与之关联的**页帧(虚拟空间分配多大, 对应就要有多大的页帧**), 但这会引发一个问题, 例如, IA-32体系结构使用4KB大小的页, 在虚拟地址空间为4GB的前提下, 则需要包含100万项的页表. 这个问题在64位体系结构下, 情况会更加糟糕. 而每个进程都需要自身的页表, 这回导致系统中**大量的内存**都用来**保存页表**.

设想一个典型的32位的X86系统, 它的**虚拟内存用户空间**(user space)大小为3G, 并且典型的一个**页表项**(page table entry,pte)大小为4bytes, 每一个页(page)大小为4k bytes. 那么这3G空间一共有(3G/4k=)786432个页面, 每个页面需要一个pte来保存映射信息, 这样一共需要786432个pte(**page table entry**)!

如何存储这些信息呢？一个直观的做法是用数组来存储, 这样每个页能存储(4k/4byte=)1K个, 这样一共需要(786432/1k=)768个连续的物理页面(phsical page). 而且, 这只是一个进程, 如果要存放所有N个进程, 这个数目还要乘上N! 这是个巨大的数目, 哪怕内存能提供这样数量的空间, 要找到连续768个连续的物理页面在系统运行一段时间后碎片化的情况下, 也是不现实的.

为减少页表的大小并容许忽略不需要的区域, 计算机体系结构的涉及会将虚拟地址分成多个部分. 同时虚拟地址空间的大部分们区域都没有使用, 因而页没有关联到页帧, 那么就可以使用功能相同但内存用量少的多的模型: **多级页表**

但是新的问题来了, 到底**采用几级页表**合适呢?

## 32位系统中2级页表

从80386开始, intel处理器的分页单元是4KB的页, 32位的地址空间被分为3部分

| 单元 | 描述 |
|:---|:----|
| 页目录表Directory | 最高10位 |
| 页中间表Table 	| 中间10位 |
| 页内偏移  		| 最低12位 |

即页表被划分为页目录表Directory和页中间表Tabl两个部分

此种情况下, 线性地址的转换分为两步完成.

- 第一步, 基于两级转换表(页目录表和页中间表), 最终查找到地址所在的**页帧**
- 第二步, 基于**偏移**, 在所在的页帧中查找到**对应偏移的物理地址**

使用这种二级页表可以有效的减少每个进程页表所需的RAM的数量.如果使用简单的一级页表, 那将需要高达2\^20个页表,假设每项4Byte(32位),则共需要占用2\^20 \* 4Byte = 4MB的RAM来表示每个进程的页表.当然我们并**不需要映射所有的线性地址空间(！！！**)(32位机器上线性地址空间为4GB),内核通常只为进程**实际使用的那些虚拟内存区请求页表**来减少内存使用量.

## 64位系统中的分页

正常来说, 对于32位的系统两级页表已经足够了, 但是对于64位系统的计算机, 这远远不够.

首先假设一个大小为4KB的标准页.因为1KB覆盖2\^10个地址的范围,**4KB**覆盖**2\^12**个地址, 所以**offset**字段需要**12位**.

这样**线性地址**空间就剩下64-12=**52位**分配给**页中间表Table**和**页目录表Directory**. 如果我们现在决定**仅仅使用64位中的48位来寻址**(**线性地址还是64位, 但是有效寻址位是48位！！！**这个限制其实已经足够了,2^48=256TB,即可达到256TB的寻址空间).剩下的48-12=**36位**被分配给**Table**和**Directory**字段. 即使我们现在决定位两个字段各预留18位,那么每个进程的页目录和页表都包含2\^18个项, 即超过256000个项.

基于这个原因, 所有64位处理器的硬件分页系统都使用了额外的分页级别. 使用的**级别**取决于**处理器的类型**

| 平台名称 | 页大小 | 寻址所使用的位数 | 分页级别数 | 线性地址分级 |
|:-----:|:-----:|:-----:|:-----:|:-------:|
| alpha  | 8KB | 43 | 3 | 10 + 10 + 10 + 13  |
| ia64   | 4KB | 39 | 3 |  9 +  9 +  9 + 12  |
| ppc64  | 4KB | 41 | 3 | 10 + 10 +  9 + 12  |
| sh64   | 4KB | 41 | 3 | 10 + 10 +  9 + 12  |
| x86\_64 | 4KB | 48 | 4 | 9 + 9 + 9 + 9 + 12 |

## Linux中的分页

层次话的页表用于支持对大地址空间快速,高效的管理.因此linux内核对页表进行了分级.

前面我们提到过, 对于**32位系统**中,**两级页表**已经足够了.但是**64位**需要更多数量的**分页级别**.

为了同时支持适用于32位和64位的系统,Linux采用了通用的分页模型.在**Linux-2.6.10**版本中, Linux采用了**三级分页模型**.而从2.6.11开始普遍采用了**四级分页**模型.

目前的内核的内存管理总是**固定使用四级页表**, 而不管底层处理器是否如此.

| 单元 | 描述 |
|:---:|:----:|
| 页全局目录 | Page GlobalDirectory  |
| 页上级目录	| Page Upper Directory  |
| 页中间目录	| Page Middle Directory |
| 页表	        | Page Table 			|
| 页内偏移      | Page Offset		    |

**Linux**不同于其他的操作系统,它把计算机分成**独立层(体系结构无关**)/**依赖层(体系结构相关**)两个层次.对于页面的映射和管理也是如此.

**Linux页表管理**分为两个部分,第一个部分**依赖于体系结构**,第二个部分是**体系结构无关的**.

所有**数据结构**几乎都定义在**特定体系结构的文件**中.这些数据结构的定义可以在**头文件arch/对应体系/include/asm/page.h**和**arch/对应体系/include/asm/pgtable.h**中找到. 但是对于**AMD64**和**IA\-32**已经统一为**一个体系结构**.但是在处理页表方面仍然有很多的区别,因为相关的定义分为**两个不同的文件**arch/x86/include/asm/page\_32.h和arch/x86/include/asm/page\_64.h, 类似的也有pgtable\_xx.h.

# 页表

Linux内核通过**四级页表**将**虚拟内存空间**分为**5个部分**(4个页表项用于选择页, 1个索引用来表示页内的偏移).各个体系结构不仅**地址长度不同**,而且**地址字拆分的方式也不一定相同**. 因此内核使用了**宏**用于将**地址分解**为各个分量.

![2020-12-28-15-14-29.png](./images/2020-12-28-15-14-29.png)

页表其他内容请参照两篇博客

[深入理解计算机系统-之-内存寻址(五)--页式存储管理](http://blog.csdn.net/gatieme/article/details/50651561), 详细讲解了传统的页式存储管理机制

[深入理解计算机系统-之-内存寻址(六)--linux中的分页机制](http://blog.csdn.net/gatieme/article/details/50756050), 详细的讲解了Linux内核分页机制的实现机制

# Linux分页机制的演变

## Linux的页表实现

由于程序存在**局部化特征**,这意味着在**特定的时间内**只有**部分内存会被频繁访问**, 具体点, **进程空间**中的text段(即程序代码),堆, 共享库, 栈都是**固定在进程空间的某个特定部分**, 这样导致**进程空间**其实是**非常稀疏**的,于是, 从硬件层面开始, 页表的实现就是采用分级页表的方式, Linux内核当然也这么做. 所谓分级简单说就是, 把整个进程空间分成区块, 区块下面可以再细分, 这样在内存中只要**常驻某个区块**的**页表**即可, 这样可以大量节省内存.

## Linux最初的二级页表

Linux最初是在一台i386机器上开发的, 这种机器是典型的32位X86架构, 支持两级页表

一个32位虚拟地址如上图划分. 当在进行地址转换时,

- 结合在**CR3寄存器**中存放的页目录(page directory, PGD)的这一页的物理地址, 再加上从虚拟地址中抽出高10位叫做页目录表项(内核也称这为pgd)的部分作为偏移, 即定位到可以描述该地址的pgd;

- 从该pgd中可以获取可以描述该地址的**页表的物理地址**, 再加上从虚拟地址中抽取中间10位作为偏移, 即定位到可以描述该地址的pte;

- 在这个pte中即可获取该地址**对应的页的物理地址**,加上从虚拟地址中抽取的最后12位, 即形成该页的页内偏移, 即可最终完成从虚拟地址到物理地址的转换.

从上述过程中, 可以看出, 对虚拟地址的分级解析过程, 实际上就是不断**深入页表层次**, 逐渐定位到**最终地址**的过程, 所以这一过程被叫做**page talbe walk**.

至于这种做法为什么能**节省内存**, 举个更简单的例子更容易明白. 比如要记录16个球场的使用情况, 每张纸能记录4个场地的情况. 采用4+4+4+4, 共4张纸即可记录, 但问题是球场使用得很少, 有时候一整张纸记录的4个球场都没人使用. 于是, 采用4 x 4方案, 即把16个球场分为4组, 同样每张纸刚好能记录4组情况. 这样, 使用一张纸A来记录4个分组球场情况, 当某个球场在使用时, 只要额外使用多一张纸B来记录该球场, 同时, 在A上记录"某球场由纸B在记录"即可. 这样在大部分球场使用很少的情况下, 只要很少的纸即困记录, 当有球场被使用, 有需要再用额外的纸来记录, 当不用就擦除. 这里一个很重要的前提就是:**局部性**.

## Linux的三级页表

当x86引入**物理地址扩展**(Pisycal Addrress Extension, PAE)后, 可以支持大于4G的**物理内存**(**36位,64GB！！！**), 但**虚拟地址**依然是**32位**, 原先的**页表项不适用**, 它实际**4bytes**被扩充到**8bytes**, 这意味着, **每一页**现在能存放的**pte数目**从1024变成512了(4k/8). 相应地, 页表层级发生了变化, **Linux**新增加了一个层级, 叫做**页中间目录**(page middle directory, PMD), 变成:

| 字段 | 描述 | 位数 |
|:---:|:---:|:---:|:---:|
| cr3 | 指向一个PDPT | cr3寄存器存储                |
| PGD | 指向PDPT中**4个项**中的一个 | 位31\~30    |
| PMD | 指向页目录中**512项**中的一个 | 位29\~21  |
| PTE | 指向页表中**512项**中的一个 | 位20\~12      |
| page offset  | 4KB页中的偏移 | 位11\~0          |

2\+9\+9\+12=32位, 线性地址还是32位

实际的page table walk依然类似, 只不过多了一级.

现在就同时存在2级页表和3级页表, 在代码管理上肯定不方便. 巧妙的是, Linux采取了一种抽象方法: **所有架构全部使用3级页表**: 即PGD -> PMD -> PTE. 那只使用2级页表(如**非PAE的X86**)怎么办？

办法是针对使用2级页表的架构, 把PMD抽象掉, 即虚设一个PMD表项. 这样在page table walk过程中, PGD本直接指向PTE的, 现在不了, 指向一个**虚拟的PMD**, 然后再由PMD指向PTE. 这种抽象保持了代码结构的统一.

## Linux的四级页表

硬件在发展, 3级页表很快又捉襟见肘了, 原因是**64位CPU**出现了, 比如X86\_64,  它的硬件是实实在在支持4级页表的. 它支持**48位的虚拟地址空间**(不过Linux内核最开始只使用47位). 如下:

| 字段 | 描述 | 位数 |
|:---:|:---:|:---:|:---:|
| **PML4**| 指向一个PDPT | 位47\~39 |
| PGD | 指向PDPT中4个项中的一个 | 位38\~30 |
| PMD | 指向页目录中512项中的一个 | 位29\~21 |
| PTE | 指向页表中512项中的一个 | 位20\~12 |
| page offset  | 4KB页中的偏移 | 位11\~0

9\+9\+9\+9\+12=48位, 共48位线性地址

Linux内核针为使用**原来的3级列表**(`PGD->PMD->PTE`), 做了折衷. 即**采用一个唯一的, 共享的顶级层次, 叫PML4**. 这个**PML4没有编码在地址**中, 这样就能套用原来的3级列表方案了. 不过代价就是, 由于只有唯一的PML4, 寻址空间被局限在(239=)512G, 而本来PML4段有9位, 可以支持512个PML4表项的. 现在为了使用3级列表方案, 只能限制使用一个, 512G的空间很快就又不够用了, 解决方案呼之欲出.

在2004年10月, 当时的 `X86_64` 架构代码的维护者Andi Kleen提交了一个叫做4level page tables for Linux的PATCH系列, 为Linux内核带来了4级页表的支持. 在他的解决方案中, 不出意料地, 按照X86\_64规范, 新增了一个PML4的层级,在这种解决方案中, X86\_64拥一个有512条目的PML4,512条目的PGD,512条目的PMD,512条目的PTE. 对于仍使用3级目录的架构来说, 它们依然拥有一个虚拟的PML4,相关的代码会在编译时被优化掉. 这样, 就把Linux内核的3级列表扩充为4级列表. 这系列PATCH工作得不错, 不久被纳入Andrew Morton的-mm树接受测试.

不出意外的话, 它将在v2.6.11版本中释出. 但是, 另一个知名开发者Nick Piggin提出了一些看法, 他认为Andi的Patch很不错, 不过他认为最好还是把PGD作为第一级目录, 把新增加的层次放在中间, 并给出了他自己的**Patch:alternate 4-level page tables patches**. Andi更想保持自己的PATCH,他认为Nick不过是玩了改名的游戏, 而且他的PATCH经过测试很稳定, 快被合并到主线了, 不宜再折腾.

不过Linus却表达了对Nick Piggin的支持, 理由是Nick的做法conceptually least intrusive. 毕竟作为Linux的扛把子, 稳定对于Linus来说意义重大.

最终, 不意外地, 最后Nick Piggin的PATCH在v2.6.11版本中被合并入主线. 在这种方案中, 4级页表分别是: PGD -> PUD -> PMD -> PTE.

# 链接

[我对linux内核四级分页理解](http://bbs.csdn.net/topics/390831818)

[Linux内核4级页表的演进](http://blog.csdn.net/hmsiwtv/article/details/39956981)

[Linux内存 之 页表](http://biancheng.dnbcw.info/linux/335152.html)

[内存管理(四) 页表数据结构 ](http://blog.chinaunix.net/uid-21718047-id-3140041.html)

[Linux内存管理之我见(二)-页表、页式内存管理机制](http://www.360doc.com/content/11/0804/10/7204565_137844381.shtml)