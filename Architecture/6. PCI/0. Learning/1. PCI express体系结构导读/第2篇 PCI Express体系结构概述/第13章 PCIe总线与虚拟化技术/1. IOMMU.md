
在**多进程环境**下, **处理器**使用 **MMU** 机制,  使得**每一个进程**都有**独立的虚拟地址空间**, 从而各个进程运行在独立的地址空间中, 互不干扰. MMU 具有两大功能:

* 一是进行**地址转换**, 将分属**不同进程的虚拟地址**转换为**物理地址**;

* 二是**对物理地址的访问**进行**权限检查**, 判断虚实地址转换的合理性.

在多数操作系统中, **每一个进程**都具有**独立的页表**存放**虚拟地址**到**物理地址**的**映射关系和属性**. 但是如果进程每次访问物理内存时, 都需要访问页表时, 将严重影响进程的执行效率. 为此处理器设置了 **TLB**(`Translation Lookaside Buffer`) 作为**页表的 Cache**. 如果进程的虚拟地址在 TLB 中命中时,  则从 TLB 中直接获得物理地址,  而不需要使用页表进行虚实地址转换, 从而极大提高了访问存储器的效率.

从地址转换的角度来看, IOMMU 与 MMU 较为类似. 只是 **IOMMU** 完成的是**外部设备地址**到**存储器地址**的转换. 我们可以将一个 **PCI 设备**模拟成为处理器系统的一个特殊进程, 当这个进程**访问存储器**时使用特殊的 MMU, 即**IOMMU**, 进行**虚实地址转换**, 然后再**访问存储器**. 在这个 I0MMU 中, 同样存在 **IO 页表**存放虚实地址转换关系和访问权限, 而且处理器为了加速这种虚实地址的转换, 还设置了 **IOTLB** 作为 **IO 页表**的 Cache. 单纯从这个角度来看, 许多 **HOST 主桥**和 **RC** 也具备同样的功能, 如 PowerPC 处理器的 Inbound 窗口和 Out-bound 窗口, 也可以完成这种特殊的地址转换. 但是这些窗口仅能完成 **PCI 总线域**到**一个存储器域**的地址转换, 无法实现 PCI 总线域到多个存储器域的转换.

目前设置 IOMMU 的主要作用是**支持虚拟化技术**, 当然使用 IOMMU 也可以实现其他功能, 如使 "仅支持 32 位地址的 PCI 设备" 访问 4GB 以上的存储器空间. IA 处理器和 AMD 处理器分别使用 "`VT-d`" 和 "`I0MMU`", 实现外部设备的地址转换. 这两种技术都可以将 PCI 总线域地址空间转换为不同的存储器域地址空间, 便于虚拟化技术的设计与实现.

# IOMMU 的工作原理

根据虚拟化的理论, 假设在一个处理器系统中存在两个 Domain, 其中一个为 Domain1, 而另一个为 Domain2. 这**两个 Domain** 分别对应**不同的虚拟机**, 并使用独立的**虚拟机物理地址空间**, 分别为 **GPA1**(Guest Physical Address) 和 **GPA2** 空间, 其中在 Domain1 上运行的所有进程使用 GPA1 空间, 而在 Domain2 上运行的所有进程使用 GPA2 空间.

GPA1 和 GPA2 采用独立的编码格式, 其地址都可以从各自 GPA 空间的 `0x0000-0000` 地址开始, 只是 GPA1 和 GPA2 空间在 System Memory 中占用的**实际物理地址** HPA(Host Physi-cal Address) 并不相同, HPA 也被称为 **MPA**(Machine Physical Address), 是处理器系统中真实的物理地址. 而 **PCI 设备**依然使用 **PCI 总线域地址空间**, PCI 总线地址需要通过 `DMA-Remapping` 逻辑转换为 HPA 地址后, 才能访问存储器. DMA-Remapping 逻辑的组成结构如图 13-1 所示.

![2022-05-04-22-33-21.png](./images/2022-05-04-22-33-21.png)

在以上处理器模型中，假设存在**两个外部设备** DeviceA 和 DeviceB。这**两个外部设备**分属于**不同的 Domain**，其中 **Device A** 属于 **Domain1**，而 Device B 属于 Domain 2。在同一段时间内，**Device A** 只能访问 **Domain1** 的 **GPA1** 空间，也**只能**被 **Domain1** 操作; 而 Device B 只能访问 GPA2 空间，也只能被 Domain2 操作。Device A 和 Device B 通过 DMA-Remmaping 机制最终访问不同 Domain 的存储器。

使用这种方法可以保证 Device A/B 访问的空间彼此独立，而且只能被指定的 Domain 访问，从而满足了虚拟化技术要求的空间隔离。这一模型远非完美，如果每个 Domain 都可以自由访问所有外部设备当然更加合理，但是单纯使用 VT-d 机制还不能实现这种访问机制。

在这种模型之下，**Device A/B** 进行 **DMA** 操作时使用的**物理地址**仍然属于 **PCI 总线域的物理地址**，Device A/B 仍然使用地址路由或者 ID 路由进行存储器读写 TLP 的传递。值得注意的是虽然在 **x86** 处理器系统中，这个 **PCI 总线地址**与 **GPA 地址**一一对应且**相等**，但是这两个地址所代表的含义仍然完全不同。

GPA 地址为存储器域的地址，隶属于不同的 Domain，而 **PCI 设备**使用的地址依然是 **PCI 总线域的物理地址**，只是在虚拟化环境下，PCI 设备与 Domain 间有明确的对应关系。当这个 PCI 设备进行 DMA 读写时，**TLP** 首先到达**地址转换部件** TA (`Translation Agent`)，并通过 **ATPT** (Address Translation and Protection Table, 相当于 I/O 页表, 每个 Domain 都有独立的 I/O 页表) 后将 **PCI 总线域的物理地址**转换为与 GPA 地址对应的 **HPA** 地址，然后对**主存储器**进行读写操作。其转换关系如图 13-2 所示.

![2024-08-19-11-49-12.png](./images/2024-08-19-11-49-12.png)

在上图所示的处理器系统中，存在**两个虚拟机**，其使用的地址空间分别为GPA Domain1和 GPA Domain2。假设每个 GPA Domain 使用1GB 大小的物理地址空间，而且 Domain 间使用的地址空间独立，其地址范围都为0x0000-0000~0x4000-0000。其中Domain1使用的GPA地址空间对应的HPA地址范围为0x0000-0000~0x3FFF-FFFF:Domain2使用的GPA地址空间对应的HPA地址范围为0x4000-0000~0x7FFF-FFFF。在一个处理器系统中，不同的虚拟机使用的物理空间是隔离的。

在这个处理器系统中存在两个PCIe设备，分别为EP1和EP2，其中EP1隶属于Domain1，而EP2 隶属于 Domain2，即EP1和EP2 进行 DMA 操作时只能访问 Domainl 和 Do-main2 对应的HPA空间，但是EP1和EP2作为一个PCIe 设备，并不知道处理器系统进行的这种绑定操作，EPI和EP2依然使用PCI总线域的地址进行正常的数据传送。因为处理器系统的这种绑定操作由TA和ATPT决定，而对PCle设备透明。在EP1和EP2进行 DMA 操作时，当TLP到达TA和ATPT，经过地址转换后，才能访问实际的存储器空间。

下面以EP1和EP2进行DMA写操作为例，说明在这种虚拟化环境下，不同种类地址的
转换关系，其步骤如下所示。

(1) Domain1和Domain2填写EP1和EP2的DMA写地址和长度寄存器启动DMA操作。

其中EP1最终将数据写人到GPA1的0x1000-0000~0x1000-007F这段数据区域，而EP2最终将数据写人到GPA2的0x1000-0000~0x1000-007F这段数据区域。然而EP1和EP2仅能识别PCI总线域的地址。Domain1和Domain2填人EP1和EP2的DMA写地址为0x10000000，而长度为0x80，这些地址都是PCI总线地址。

在x86处理器系统中，这个地址与GPA1和GPA2存储器域的地址恰好相等，但是这个地址仍然是PCI总线域的地址，只是由于IOMMU的存在，相同的PCI总线地址，可能被映射到相同的GPA地址空间，然而这些GPA地址空间对应的HPA地址空间不同。这个PCI总线地址仍然在RC中被转换为存储器域地址，并由TA转换为合适的HPA地址。

(2) EP1和EP2的存储器写TLP到达RC。

来自EP1和EP2存储器写TLP经过地址路由最终到达RC，并由RC将TLP的地址字段转发到 TA 和 ATPT，进行地址翻译。

EP1和EP2使用的/0页表已经事先被VMM设置完毕，TA将使用Domain1或者Domain2的 O页表，进行地址翻译。EP1隶属于Domain1，其地址0x1000-0000(PCI总线地址)被翻译为0x1000-0000(HPA):而EP2隶属于Domain2，其地址0x1000-0000(PCI总线地址)被翻译为0x5000-0000(HPA)。值得注意的是在TA中设置了IOTLB，以加速 I/O页表的翻译效率，因此 TA并不会每次都从存储器中査找 0页表。

(3)来自EP1和EP2存储器写TLP的数据将被分别写人到0x1000-0000~0x1000-007F和0x5000-0000~0x5000-007F这两段数据区域。

(4)Domain1和Domain2都使用0x1000-0000~0x1000-007F这段GPA地址访问来自EPI和EP2的数据，这个GPA地址将转换为HPA地址，然后发向存储器控制器。在IA处理器系统中，使用EPT和VPID技术进行CPA地址到HPA地址的转换。IA 处理器和 AMD处理器使用不同的技术，实现TA和ATPT。其中IA 处理器使用 VT-d技术，而AMD使用IOMMU。从工作原理上看，这两种技术类似，但是在实现细节上，两者有较大区别。

# IA 处理器的 VT-d

IA(IntelArchitecture)处理器使用VT-d技术将PCI总线域的物理地址转换为HPA地址。这个映射过程也被称为DMA Remapping。IA处理器系统使用DMARemapping机制可以辅助虚拟化技术对外部设备进行管理。

在IA处理器系统中，所有的外部设备都是PCI设备。每一个设备都唯一对应一个BusNumber、Device Number 和Function Number，为此 IA 处理器设置了一个专门的结构，即 Root Entry Table(如果处理器系统有多个PCI总线树`<Segment>`，则需要设备多个RotEntryTable)，管理每一棵PCI总线树。在这种结构下，每一个PCI设备根据其 Bus、Device和 Function 号唯一确定一个 Context Entry。VT-d将这个结构称为“Device to Domain Map-ping”结构，如图 13-3 所示。

![2024-08-19-14-50-52.png](./images/2024-08-19-14-50-52.png)

VT-d一共设置了两种结构描述PCI总线树结构，分别为RootEntry和Context Entry。其
中RootEntry描述PCI总线，一棵PCI总线树最多有256条PCI总线，其中每一条PCI 总线对应一个Root Enlny;每条PCI总线中最多有32个设备，而每个设备最多有8个Function,其中每一个 Function 对应一个 Context Entry，因此每个 Context Entry 表中共有 256 表项。

在一个处理器系统中，一个指定的PCIFunction唯一对应一个Context Entry，这个 Con-text Entry 指向这个 PCI Function使用的地址转换结构(Address Translation Structures)。当一个PCIFunction 隶属于不同的Domain时，将使用不同的地址转换结构，但是在一-个时间段里，PCFunction只能使用一个地址转换结构，即Context Entry只能指向一个 Domain 的地址转换结构。这个地址转换结构的主要功能是完成PCI总线域到HPA存储器域的地址转换。

如图13-1所示，当一个设备进行DMA操作时，Domain使用PCI总线域的地址填写这个设备和与DMA转送相关的寄存器。当这个设备启动DMA操作时，将使用PCI总线地址，之后通过 DMA Remapping机制将PCI总线地址转换为HPA存储器域地址，然后将数据传送到实际的物理地址空间中。而Domain通过处理器的MMU机制将GPA转换为HPA，访问物理地址空间。

从图13-3中可以发现，每一个Function在每一个Domain中都可能有一个地址转换结构，以完成GPA到HPA的转换，因此在每一个Domain中最多有256个地址转换结构。这些结构无疑将占用部分内存，但是并不会产生较大的浪费。因为在实际设计中，同一个Domain下的所有PCI设备使用的总线地址到HPA地址的转换结构可以相同。因此在实现中每个 Domain 仅使用一个地址转换结构即可。

IA 处理器使能 VT-d机制后，PCI设备进行DMA操作需要根据Bus、Device 和 Function号确定 Context Entry，之后使用图13-4所示的方法完成PCI总线地址到 HPA 地址的转换。

![2024-08-19-14-52-15.png](./images/2024-08-19-14-52-15.png)

在上图中，4KBPage Table中的每个Entry的大小为8B，因此在计算偏移时，需要左移3位，PCI总线地址通过3级目录，最终找到与HPA所对应的4KB大小的页面，从而完成PCI总线地址到HPA的转换。值得注意的是，IA处理器还支持2MB(SP=1)、1GB(SP=2)、512GB(SP=3)和1TB(SP=4)大小的Super Page，而本节仅使用了4KB大小的页面。

为了加快 PCI总线地址到HPA地址的转换速度，IA处理器分别为RootEntry和ContextEntry 设置了 Context Cache 以加快 Context Entry 的获取速度，同时还设置了IOTLB 加速 PCI总线地址到HPA地址的转换速度。

IOTLB相当于0页表的Cache，当一个PCI设备进行 DMA操作时，首先在IOTLB中查找PCI总线地址与HPA地址的映射关系，如果在IOTLB命中时，PCI设备直接获得HPA地址进行DMA操作;如果没有在IOTLB命中，则需要使用图13-4中所示的算法进行PCI总线地址到HPA地址的转换。

Intel并没有公开“没有在IOTB命中”的实现细节，当出现这种情况时，IA处理器可能使用内部的 Microcode 完成图13-4所示的算法。使用VT-d除了可以有效地支持虚拟化技术之外，还可以支持一些只能访问32 位地址空间的 PCI设备访问4GB之上的物理地址空间。

# AMD 处理器的 IOMMU

AMD 处理器的IOMMU 技术与Intel 的 VT-d技术类似，其完成的主要功能也类似。AMD 率先提出了IOMMU的概念，并发布了IOMMU的技术手册，但是Intel首先将这一技术在芯片中实现。由于AMD和Intel使用的x86体系结构略有不同，因此AMD的IOMMU 技术在细节上与Intel 的 VT-d并不完全一致。

AMD处理器使用HT(Hyper Transport)总线连接0Hub，其中每一个 /O Hub 都含有一个IOMMU，其结构如图13-5所示。

![2024-08-19-14-53-50.png](./images/2024-08-19-14-53-50.png)

其中每一个IOMMU都使用一个DeviceTable。AMD处理器使用Device Table 存放图13-3中的结构，Device Table 最多由2“个Entry 组成，其中每个 Entry的大小为256b，因此Device Table最大将占用2MB内存空间，与Intel使用的Root/ContextEntry结构相比AMD 使用的这种方法容易造成内存的浪费。

在 I/0 Hub中的设备使用16位的DeviceID在 Device Table 查找该设备所对应的 Entry,并使用这个Entry，根据I/0Page Table结构最终找到IOPTE表，并完成CPA到 HPA 的转换。在AMD处理器中，GPA到HPA的转换与图13-4中所示的方法有类似之处，但实现细节不同。IOMMU使用一个新型的页表结构完成GPA到HPA的转换，这个页表结构基于AMD64使用的虚拟地址到物理地址的页表结构，但是做出了一些改动。

> 其中 PCI设备使用 Bus Number、Device Number和 Funetion Number组成 16 位的 Device lD,而 HT 设备使用 HT Bus Number 和 Unit ID 组成16 位的 Device ID。

AMD64进行虚拟地址到物理地址的转换时使用4级页表结构，如图13-6所示。

![2024-08-19-14-54-59.png](./images/2024-08-19-14-54-59.png)

与Intel 处理器的结构类似，一个进程首先从CR3寄存器中获得页表的基地址指针寄存器“Page Map Level-4 Base Address”，之后通过4级索引最终获得4KB 大小的物理页面，完成虚拟地址到物理地址的转换。AMD处理器也支持大页面方式，如果使用三级索引，可以获得 2MB大小的物理页面;使用二级索引，可以获得1GB大小的页面。

IOMMU使用的 I/O 页表结构基于以上结构，但是做出了一定的改动。在I0MMU中，4级0页表指针可以直接指向2级V0页表指针，从而越过第3级O页表，使用这种方法可以节省Page Table 的空间。如图13-7 所示。

![2024-08-19-14-56-35.png](./images/2024-08-19-14-56-35.png)

当设备进行 DMA 操作时，首先需要从相应的Device Table 的Entry 中获得“Level4 Page Table Address”指针，并定位设备使用的O页表，最后使用多级页表结构，最终完成PCI总线地址到HPA地址的转换。Page Table的Entry由64位组成，其主要字段如下所示。

* 第 51 ~ 12位为Next Table Address/Page Address字段，该字段存放下一级页表或者物理页面的地址，该地址为系统物理地址，属于HPA空间。

* 第11~9位为Next Level字段，表示下一级页表的级数，其中在Device Table中存放的级数一般为4，Level-N级页表中存放的Next Level字段为N-1~1。

如图13-7所示，在第4级页表的Entry 中的Next Level 字段为2，表示第4级页表直接指向第2级页表，而忽略第3级页表。当该字段为0b000或者0b111时，表示下一级指针指向物理页面而不是页表。NextLevel字段为0b000时，表示所指向的物理页面的大小是固定的，AMD64支持4KB、2MB、1GB、512GB和1TB(SP=4)大小固定页面;如果Next Level 字段为0b111时，表示所指向的物理页面大小是浮动的。如果Level2 Page Table 的En-try中的 Next Level字段为 0b111，表示该Enty 指向的物理页面大小浮动，其中物理页面大小和 GPA 的第 29~21 位相关，如表 13-1 所示。

![2024-08-19-14-58-01.png](./images/2024-08-19-14-58-01.png)

AMD64处理器使用这种I0页表方式，可以方便地支持4KB、8KB、…、4GB大小的浮动物理页面。除了O页表外，IOMMU也设置了IOTLB 以加快GPA到HPA地址的转换这部分内容与IA处理器的实现方式类似，本章不对此继续进行描述。对IOMMU 感兴趣的读者可以参考AMD /0 Virtualization Technology Specification。