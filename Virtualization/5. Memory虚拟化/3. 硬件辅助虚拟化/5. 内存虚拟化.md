
<!-- @import "[TOC]" {cmd="toc" depthFrom=1 depthTo=6 orderedList=false} -->

<!-- code_chunk_output -->

- [1. 概述](#1-概述)
- [2. 影子页表的缺点](#2-影子页表的缺点)
- [3. VT-x 支持](#3-vt-x-支持)
  - [3.1. EPT](#31-ept)
  - [3.2. VPID](#32-vpid)
- [4. Intel EPT](#4-intel-ept)
  - [4.1. EPT 原理](#41-ept-原理)
    - [4.1.1. GVA->HPA 地址转换过程](#411-gva-hpa-地址转换过程)
  - [4.2. EPT 的硬件支持](#42-ept-的硬件支持)
    - [4.2.1. VM-Execution 控制域的 Enable EPT 字段](#421-vm-execution-控制域的-enable-ept-字段)
    - [4.2.2. VM-Execution 控制域的 Extended page table pointer 字段](#422-vm-execution-控制域的-extended-page-table-pointer-字段)
    - [4.2.3. TLB、INVEPT 指令和 VPID](#423-tlb-invept-指令和-vpid)
    - [4.2.4. VM-Exit 信息域的 EPT Violation 相关字段](#424-vm-exit-信息域的-ept-violation-相关字段)
  - [4.3. EPT 的软件使用](#43-ept-的软件使用)
- [5. VPID](#5-vpid)
  - [5.1. 传统 TLB](#51-传统-tlb)
  - [5.2. TLB 项的 VPID 标识不同虚拟处理器的不同 TLB](#52-tlb-项的-vpid-标识不同虚拟处理器的不同-tlb)
  - [5.3. VPID 的软件使用](#53-vpid-的软件使用)
- [6. Linux 系统检查](#6-linux-系统检查)
  - [6.1. /proc/cpuinfo](#61-proccpuinfo)
  - [6.2. sysfs 文件系统](#62-sysfs-文件系统)
- [7. AMD NPT](#7-amd-npt)

<!-- /code_chunk_output -->

# 1. 概述

内存虚拟化的主要任务是实现地址空间的虚拟化.

由于引入了**客户机物理地址空间**, **内存虚拟化**通过**两次地址转换**来支持地址空间的虚拟机, 即 **客户机虚拟地址 GVA** → **客户机物理地址 GPA** → **宿主机物理地址 HPA**的转换.

其中,

- **GVA→GPA**的转换是由**客户机软件决定**, 通常是**客户机 OS**通过**VMCS 中客户机状态域 CR3**指向的**页表**来指定;

- **GPA→HPA**的转换是由**VMM 决定**, VMM 在将**物理内存分配给客户机时**就确定了**GPA→HPA**的转换, VMM 通常会用**内部数据结构**来记录这个关系. VMM 为**每个虚拟机**动态地维护了一张**客户机物理地址**与**宿主机物理地址**映射表.

# 2. 影子页表的缺点

传统的 IA32 架构只支持一次地址转换, 即通过**CR3 指定的页表**实现"**虚拟地址**"→"**物理地址**"的转换.

这和**内存虚拟化**所要求的**两次地址转换**产生**矛盾**.

可通过将**两次转换合并成一次转换**解决, 即 VMM 根据**GVA → GPA → HPA 的映射关系**, 计算出**GVA→HPA 的映射关系**, 并将其写入"**影子页表**".

影子页表的作用:

![](./images/2019-07-03-09-27-31.png)

这样的软件方法尽管可以, 但缺点也很明显.

- 首先是**实现非常复杂**, 例如要考虑 **各种页表同步情况！！！** 等, 这样导致开发、调试和维护比较困难. 可参考 Xen/KVM 中影子页表的实现.
- 另外, **开销也很大**, 因为要为 **每个客户机进程对应的页表！！！** 都要维护一个"**影子页表**".

具体可以看 `第 4 章 基于软件的完全虚拟化`

# 3. VT-x 支持

## 3.1. EPT

为解决这个问题, VT-x 提供了**Extend Page Table(EPT**)技术, 直接在**硬件支持 GVA→GPA→HPA 的两次地址转换**, 降低内存虚拟化的难度, 提高内存虚拟化性能.

## 3.2. VPID

此外, 为进一步**提高 TLB 的使用效率**, VT-x 还引入了**Virtual Processor ID(VPID**)功能, 进一步增强内存虚拟化性能.

# 4. Intel EPT

## 4.1. EPT 原理

在原有**客户机页表！！！** 对 **客户机虚拟地址！！！** 到 **客户机物理地址！！！**映射 (**CR3 页表地址映射！！！GVA→GPA！！！**) 基础上, EPT 还引入了 **EPT 页表(GPA→HPA！！！**) 来实现 **客户机物理地址**到**宿主机物理地址**的另一次映射.

这样, **GVA→GPA→HPA 两次地址转换！！！** 都由 **CPU 硬件自动完成！！！**.

图 5-13 描述了 EPT 基本原理.

![](./images/2019-07-03-09-36-51.png)

**客户机运行**时, **客户机页表基地址**被载入**物理 CR3**, 而**EPT 页表基地址**从**VMCS**中获取.

### 4.1.1. GVA->HPA 地址转换过程

1. 处于`non-root`**模式**的**CPU**加载**guest 进程的 gCR3**;
2. gCR3 是**GPA**,cpu 需要通过**查询 EPT 页表**来实现`GPA->HPA`;
3. 如果没有, CPU 触发**EPT Violation**, 由**VMM 截获处理**;
4. 假设**客户机**有**m 级页表**, **宿主机 EPT**有**n 级**, 在 TLB 均 miss 的最坏情况下, 会产生**m*n 次内存访问**, 完成**一次客户机的地址翻译**;

注: 这里没有包括 gCR3 的访问, 带上 gCR3 就是`<m+1> * n`

![2020-03-30-11-40-39.png](./images/2020-03-30-11-40-39.png)

这里假设**客户机页表！！！**和**EPT 页表！！！**都是**4 级页表**, CPU 完成一次地址转换的基本过程如下.

CPU 首先查找**Guest CR3**指向的**L4 页表**. 由于**Guest CR3**给的是**GPA(！！！客户机物理地址, 不是客户机逻辑地址, 也不是宿主机物理地址！！！**), 因此 CPU 需要通过**EPT 页表！！！**来实现**Guest CR3 GPA→HPA 的转换！！！**: CPU 首先会查看**硬件的 EPT TLB(因为 CR3 是 GPA！！！所以查找的是 EPT 的 TLB！！！**), 如果**没有对应的转换**, CPU 会**进一步查找 EPT 页表**, 如果还没有, CPU 则**抛出 EPT Violation 异常**由**VMM**来处理.

获取**L4 页表物理地址(！！！HPA！！！**)后, CPU 根据**GVA！！！**和**L4 页表项内容**, 获取**L3 页表项的 GPA！！！**. 如果 L4 页表中 GVA 对应的表项显示为"缺页", 那么 CPU 产生**Page Fault**, 直接交给**Guest Kernel！！！**处理. 注意, 这里**不会产生 VM-Exi！！！**t. 获得**L3 页表项的 GPA**后, CPU 同样要通过**查询 EPT 页表！！！来实现 L3 GPA→HPA 的转换**, 过程和上面一样.

同样, CPU 依次查找 L2、L1 页表, 最后获得**GVA**对应的**GPA**, 然后通过**查询 EPT 页表获得 HPA**.

上图可以看出, CPU 需要**5 次！！！查询 EPT 页表(CR3\<L4 页表地址 GPA>、L3 页表地址、L2 页表地址、L1 页表地址和最终 GPA 查询 EPT 页表！！！**), **每次查询！！！**都需要**4 次内存访问(！！！每级页表都是一次内存访问！！！**), 最坏共**20 次内存访问**. EPT 硬件通过增大 EPT TLB 来尽量减少内存访问.

一个**GPA**到**HPA**的转换过程如下图.

![config](./images/24.png)

![2020-03-17-15-03-04.png](./images/2020-03-17-15-03-04.png)

在**客户机物理地址 GPA**到**宿主机物理地址 HPA**转换的过程中, 由于缺页、写权限不足等原因也会导致客户机退出, 产生 **EPT 异常**.

注意: 上面的**EPT TLB**是用来缓存**客户机的 GPA**转换成**宿主机的 HPA**; 而在这之前, **客户机的 GVA 转成 GPA 是可以使用原始 TLB???**

## 4.2. EPT 的硬件支持

### 4.2.1. VM-Execution 控制域的 Enable EPT 字段

为了支持 EPT, VT-x 规范在**VMCS**的"**VM-Execution 控制域**"中提供了**Enable EPT 字段**. 如果在 VM-Entry 的时候该位被置, CPU 会使用 EPT 功能进行两次转换.

### 4.2.2. VM-Execution 控制域的 Extended page table pointer 字段

**EPT 页表的基地址**是由**VMCS"VM-Execution 控制域**"的**Extended page table pointer 字段**来执行的, 它包含了**EPT 页表的宿主机物理地址 HPA！！！**.

**EPT 是一个多级页表**, **每级页表**的 **表项格式是相同！！！** 的, 如表 5-8

![config](./images/21.png)

EPT 页表转换过程和**CR3 页表转换是类似**的. 图 5-14 展现了**CPU 使用 EPT 页表进行地址转换**的过程.

![config](./images/22.png)

EPT 通过**EPT 页表**的**SP 字段！！！**支持大小为**2MB 或 1GB 的超级页**. 图 5-15 给出**2MB 超级页**的地址转换过程. 和上图不同在于, 当 CPU 发现**SP 字段为 1**时, 就停止继续向下遍历页表, 而是直接转换了.

![config](./images/23.png)

### 4.2.3. TLB、INVEPT 指令和 VPID

EPT 同样会使用**TLB 缓冲**来加速页表的查找过程. 因此, VT-x 还提供了一条新指令**INVEPT**, 可使 EPT 的 TLB 项失效. 这样, 当 EPT 页表有更新时, CPU 可以执行 INVEPT 使旧的 TLB 失效, 使 CPU 使用新的 EPT 表项.

VPID 下面讲到

### 4.2.4. VM-Exit 信息域的 EPT Violation 相关字段

和 CR3 页表会导致 Page Fault 一样, 使用 EPT 后, 如果 CPU 在**遍历 EPT 页表**进行**GPA→HPA 转换**时, 也会发生异常.

⓵ GPA 的地址位数大于 GAW.

⓶ 客户机试图读一个不可读的页(R=0).

⓷ 客户机试图写一个不可写的页(W=0)

⓸ 客户机试图执行一个不可执行的页(X=0)

发生异常, CPU 会产生**VM-Exit！！！**, 退出原因是**EPT Violation！！！**. VMCS 的"**VM-Exit 信息域**"还包括如下信息.

- VM-Exit physical-address information: 引起 EPT Violalation 的**GPA**
- VM-Exit linear-address information: 引起 EPT Violation 的**GVA**
- Qualification: 引起**EPT Violation 的原因**, 如由于读或写引起等.

## 4.3. EPT 的软件使用

使用 EPT, VMM 需要做如下事情.

- 首先需要在**VMCS 中将 EPT 功能打开**, 这个只需要写 VMCS 相应字段即可.

- 其次需要设置好 EPT 的页表. **EPT 页表**反应了**GPA→HPA 的映射关系**. 由于是**VMM 负责给虚拟机分配物理内存！！！**, 因此, VMM 拥有足够信息建立 EPT 页表. 此外, 如果 VMM 给虚拟机分配的物理内存足够连续的话, VMM 可以在**EPT 页表**中**尽量使用超级页**, 这样有利于**提高 TLB 的性能**.

- 当 CPU 开始使用 EPT 时, VMM 还需要处理**EPT Violation**.

通常, EPT Violation 的来源如下几种.

⓵ 客户机访问**MMIO 地址！！！**. 这时, VMM 需要将请求**转给 I/O 虚拟化模块！！！**.

⓶ **EPT 页表的动态创建**. 有些 VMM 采用懒惰方法, 一开始**EPT 页表为空**, 第一次使用发生 EPT Violation 时再建立映射.

由此, EPT 相对"影子页表", 大大简化了.

- 而且, 由于**客户机内存的 Page Fault 不用发生 VM-Exit**, 也大大**减少 VM-Exit 个数**, 提高了性能.
- 此外, **EPT 只需要维护一张 EPT 页表**, 不像"影子页表"那样需要为**每个客户机进程的页表维护一张影子页表**, 也**减少了内存的开销**.

# 5. VPID

## 5.1. 传统 TLB

**TLB**是**页表项**的缓存. TLB 需要和页表一起工作才有效. 因此, 当**页表发生切换**时, **TLB 原有内容也就失效**了, CPU 需要使用**INVLPG 指令**使其**所有项失效**, 这样才不会影响之后页表的工作. 例如, 进程切换时, 通过切换 CR3 切换进程地址空间, 使前一个进程的 TLB 项全部失效.

类似, **每次 VM-Entry**和**VM-Exit**时, CPU 会**强制 TLB 内容全部失效**, 以避免 VMM 以及不同虚拟机虚拟处理器之间 TLB 项的混用, 因为**硬件无法区分！！！**一个**TLB**项属于**VMM**还是**特定虚拟机的虚拟处理器**.

## 5.2. TLB 项的 VPID 标识不同虚拟处理器的不同 TLB

**VPID**是一种硬件级的对 TLB 资源管理的优化. 通过在**硬件上**为**每个 TLB 项增加一个标志**, 来**标识不同的虚拟处理器地址空间**, 从而区分开**VMM**以及**不同虚拟机的不同虚拟处理器**的 TLB. 即, **硬件**具备了区分**不同的 TLB 项**属于**不同虚拟处理器地址空间**(对应不同的虚拟处理器)的能力.

这样,

- 硬件可以**避免**在**每次 VM-Entry**和**VM-Exit**时, 使**全部 TLB 失效！！！**, 提高 VM 切换效率.
- 并且, 由于这些继续存在的 TLB, 硬件也避免了 VM 切换后的一些**不必要页表遍历**, 减少内存访问, 提高了 VMM 以及虚拟机运行速度.

VT-x 通过在**VMCS**中增加**两个域**来支持 VPID.

- 一个是 VMCS 中的**Enable VPID 域**, 当该域被置上, VT-x 硬件会开启 VPID 功能.
- 第二个是 VMCS 中的**VPID 域**, 标识该 VMCS 对应的 TLB. VMM 本身也需要一个 VPID, VT-x 规定虚拟处理器标志**0**被指定为用于**VMM 自身**.

注意: VPID 是和 VCPU 对应的, 不是和虚拟机本身

![](./images/2019-05-14-09-42-20.png)

## 5.3. VPID 的软件使用

因此, **软件**上使用 VPID 很简单, 两件事.

- 首先为 VMCS**分配一个 VPID**, 只要是非 0 的且和其它 VMCS 的 VPID 不同即可;
- 其次是 VMCS 中**Enable VPID 置位**.

# 6. Linux 系统检查

## 6.1. /proc/cpuinfo

对于内存虚拟化**EPT**以及**vpid**的支持查询

```
[root@kvm-host ~]# grep -E "ept|vpid" /proc/cpuinfo
```

## 6.2. sysfs 文件系统

在**宿主机**中, 可以根据**sysfs 文件系统**中**kvm\_intel 模块**的**当前参数值**来确定 KVM 是否打开**EPT 和 VPID**特性.

在默认情况下, 如果硬件支持了 EPT、VPID, 则**kvm\_intel 模块加载**时**默认开启 EPT 和 VPID**特性, 这样 KVM 会默认使用它们.

```
[root@kvm-host ~]# cat /sys/module/kvm_intel/parameters/ept
Y
[root@kvm-host ~]# cat /sys/module/kvm_intel/parameters/vpid
Y
```

在加载 kvm\_intel 模块时, 可以通过**设置 ept**和**vpid 参数**的值来**打开或关闭 EPT 和 VPID**.

当然, 如果 kvm\_intel 模块已经处于**加载状态**, 则需要**先卸载**这个模块, 在**重新加载**之时加入所需的参数设置. 当然, 一般不要手动关闭 EPT 和 VPID 功能, 否则会导致客户机中内存访问的性能下降.

```
[root@kvm-host ~]# modprobe kvm_intel ept=0,vpid=0
[root@kvm-host ~]# rmmod kvm_intel
[root@kvm-host ~]# modprobe kvm_intel ept=1,vpid=1
```

# 7. AMD NPT

AMD NPT(Nested Page Table, 嵌套页表)是 AMD 提供的内存虚拟化支持技术.

如图 2-9 所示, 传统的分页技术下的地址转换是直接将线性地址空间上的线性地址转换为物理地址空间上的物理地址. CR3 寄存器中存储着页表的物理基地址.

图 2-9 传统分页技术下的地址转换:

![2019-12-14-18-03-08.png](./images/2019-12-14-18-03-08.png)

嵌套分页技术的原理如图 2-10 所示.

1)客户机和宿主机都有自己的 CR3 寄存器, 分别记为 gCR3(guest CR3)和 nCR3(nested CR3). 真正的 CR3 由 VMM 所控制和使用.

2)gPT(guest Page Table, 客户机页表)负责将客户机线性地址转换为客户机物理地址. 客户机页表存在于客户机物理内存中, 并由 gCR3 索引.

3)nPT(nested Page Table, 嵌套页表)负责将客户机物理地址转换为系统物理地址. 嵌套页表存在于系统物理内存中, 并由 nCR3 索引.

4)最常用到的客户机线性地址到系统物理地址的映射关系在 TLB 中缓存.

5)gCR3 和客户机页表中存放的都是客户机物理地址, 所以, 在访问客户机页表前需要将客户机物理地址转换为系统物理地址.

图 2-10 嵌套分页技术下的地址转换:

![2019-12-14-18-03-43.png](./images/2019-12-14-18-03-43.png)