
<!-- @import "[TOC]" {cmd="toc" depthFrom=1 depthTo=6 orderedList=false} -->

<!-- code_chunk_output -->

- [1. KVM模块初始化流程](#1-kvm模块初始化流程)
- [2. 虚拟机的创建](#2-虚拟机的创建)
  - [2.1. 虚拟机创建过程kvm_dev_ioctl_create_vm](#21-虚拟机创建过程kvm_dev_ioctl_create_vm)
    - [2.1.1. kvm_create_vm流程](#211-kvm_create_vm流程)
    - [2.1.2. 返回kvm_vm的fd文件描述符](#212-返回kvm_vm的fd文件描述符)
- [3. vCPU的创建](#3-vcpu的创建)
  - [3.1. vCPU的创建过程kvm_vm_ioctl_create_vcpu](#31-vcpu的创建过程kvm_vm_ioctl_create_vcpu)
    - [3.1.1. kvm_arch_vcpu_create创建kvm_vcpu](#311-kvm_arch_vcpu_create创建kvm_vcpu)
    - [3.1.2. kvm_arch_vcpu_setup初始化kvm_vcpu](#312-kvm_arch_vcpu_setup初始化kvm_vcpu)
    - [3.1.3. vCPU检测](#313-vcpu检测)
    - [3.1.4. 创建vcpu_fd](#314-创建vcpu_fd)
    - [3.1.5. 释放内核锁和返回vcpu_fd](#315-释放内核锁和返回vcpu_fd)
- [4. vCPU的运行](#4-vcpu的运行)

<!-- /code_chunk_output -->

# 1. KVM模块初始化流程

KVM模块分为三个主要模块：kvm.ko、kvm\-intel.ko和kvm\-amd.ko，这三个模块在初始化阶段的流程如图5\-4所示。
￼
图5\-4 KVM模块初始化阶段:

![2019-07-05-21-29-03.png](./images/2019-07-05-21-29-03.png)

**KVM模块**可以**编译进内核**中，也可以作为**内核模块**在Linux系统启动完成**之后加载**。加载时，KVM 根据主机所用的体系架构是 Intel的 VMX技术还是AMD的SVM技术，会采用略微不同的加载流程。

Linux的**子模块入口**通常通过**module\_init**宏进行定义，由**内核进行调用**。KVM的初始化流程如图5\-5所示。
￼
图5\-5 KVM的初始化流程:

![2019-07-05-22-18-28.png](./images/2019-07-05-22-18-28.png)

KVM的初始化步骤分为以下三步。

1）在**平台相关的KVM模块**中通过**module\_init宏**正式进入KVM的初始化阶段，并且执行相关的**硬件初始化准备**。(arch/x86/kvm/vmx.c)

2）进入**kvm\_main.c**中的**kvm\_init**函数进行正式的初始化工作，期间进行了一系列子操作。(virt/kvm/kvm\_main.c)

注: vmx.c中定义了一个结构体`vmx_x86_ops`, 这是函数指针集合(各种操作的集合). 初始化很多动作以及iotcl调用也都会用到这些函数. 作为参数传给`kvm_init()`, 再传给`kvm_arch_init()`, 在传递到全局变量`kvm_x86_ops`. 可以见4的说明

* 通过**kvm\_arch\_init**函数初始化KVM内部的一些数据结构：**注册全局变量kvm\_x86\_ops**、**初始化MMU**等数据结构、**初始化Timer定时器**架构。(arch/x86/kvm/x86.c)
* 分配KVM内部操作所需要的**内存空间**。
* 调用kvm\_x86\_ops的**hardware\_setup**函数进行具体的硬件体系结构的初始化工作。
* 注册**sysfs**和**devfs**等API接口信息。
* 最后初始化**debugfs**的调试信息。

3）进行后续的硬件初始化准备操作。

# 2. 虚拟机的创建

基于KVM的虚拟机创建分为**虚拟机创建**和**虚拟CPU创建**两个步骤。

在下文的描述中，**虚拟机**对应的**文件描述符**为**vm\_fd**，**虚拟CPU**对应的**文件描述符**为**vcpu\_fd**。

## 2.1. 虚拟机创建过程kvm_dev_ioctl_create_vm

**打开/dev/kvm** 文件并且获得**文件描述符 fd** 后，通过 ioctl 指令写入KVM\_CREATE\_VM，即可创建一个 VM 虚拟。

KVM 的**该部分代码**实现在**kvm\_dev** 的 **file\_operation 结构体**中，对应的代码在 **kvm\_main.c** 中调用 **kvm\_dev\_ioctl\_create\_vm**函数实现，其代码如下：

代码5\-7 KVM\_CREATE\_VM实现代码

```
// virt/kvm/kvm_main.c
static long kvm_dev_ioctl(struct file *filp,
			  unsigned int ioctl, unsigned long arg)
{
(1880)        case KVM_CREATE_VM:￼
(1881)             r = -EINVAL;￼
(1882)             if (arg)￼
(1883)                  goto out;￼
(1884)             r = kvm_dev_ioctl_create_vm();￼
(1885)             break;
```

**kvm\_dev\_ioctl\_create\_vm**函数(virt/kvm/kvm\_main.c)通过调用**kvm\_create\_vm**函数对**KVM结构体**进行创建。

```c
// virt/kvm/kvm_main.c
static int kvm_dev_ioctl_create_vm(unsigned long type)
{
	int r;
	struct kvm *kvm;

	kvm = kvm_create_vm(type);
	if (IS_ERR(kvm))
		return PTR_ERR(kvm);
#ifdef KVM_COALESCED_MMIO_PAGE_OFFSET
	r = kvm_coalesced_mmio_init(kvm);
	if (r < 0) {
		kvm_put_kvm(kvm);
		return r;
	}
#endif
	r = anon_inode_getfd("kvm-vm", &kvm_vm_fops, kvm, O_RDWR);
	if (r < 0)
		kvm_put_kvm(kvm);

	return r;
}
```

**KVM 结构体**如前文所述，保存了**虚拟机运行的上下文**及**其他相关状态**，在**使用之前**，需要进行**一定的初始化**工作。

kvm\_create\_vm的相关流程

### 2.1.1. kvm_create_vm流程

```c
// virt/kvm/kvm_main.c
static struct kvm *kvm_create_vm(unsigned long type)
{
	int r, i;
	struct kvm *kvm = kvm_arch_alloc_vm();

	r = kvm_arch_init_vm(kvm, type);

	r = hardware_enable_all();

	kvm->memslots = kzalloc(sizeof(struct kvm_memslots), GFP_KERNEL);
    
	kvm_init_memslots_id(kvm);
	if (init_srcu_struct(&kvm->srcu))
		goto out_err_nosrcu;
	for (i = 0; i < KVM_NR_BUSES; i++) {
		kvm->buses[i] = kzalloc(sizeof(struct kvm_io_bus),
					GFP_KERNEL);
		if (!kvm->buses[i])
			goto out_err;
	}

	spin_lock_init(&kvm->mmu_lock);
	kvm->mm = current->mm;
	atomic_inc(&kvm->mm->mm_count);

	kvm_eventfd_init(kvm);

	r = kvm_init_mmu_notifier(kvm);

	raw_spin_lock(&kvm_lock);
	list_add(&kvm->vm_list, &vm_list);
	raw_spin_unlock(&kvm_lock);

	return kvm;
}
```

先**分配一个虚拟机struct kvm数据结构**

在 x86体系架构中，**KVM结构体**的**初始化**任务在**kvm\_arch\_init\_vm**函数(arch/x86/kvm/x86.c)中进行，进行了**分配内存**、**初始化设备列表**、设置**中断管理**和**初始化 tsc 的spin\_lock的功能**。

在**完成之后**，将执行**硬件初始化**工作，该部分硬件初始化工作通过调用**on\_each\_cpu宏**，将在**每个物理CPU**上执行同样的操作。该操作主要是尝试将**所有的CPU**切换入**vitualize模式**，并且设置好**时钟**等信息，这个过程通过 **kvm\_arch\_hardware\_enable** 函数完成。该函数代码(arch/x86/kvm/x86.c)如下，主要执行了两个工作：

- **整理CPU的时钟信息**；
- 调用kvm\_x86\_ops的**硬件相关**的函数进行具体操作。

代码5\-8 kvm\_arch\_hardware\_enable函数代码

```
￼(5799)   int kvm_arch_hardware_enable(void *garbage)￼
(5800)   {￼
(5801)        struct kvm *kvm;￼
(5802)        struct kvm_vcpu *vcpu;￼
(5803)        int i;￼
(5804)￼
(5805)        kvm_shared_msr_cpu_online();￼
(5806)        list_for_each_entry(kvm, &vm_list, vm_list)￼
(5807)             kvm_for_each_vcpu(i, vcpu, kvm)￼
(5808)                  if (vcpu->cpu == smp_processor_id())￼
(5809)                       kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);￼
(5810)        return kvm_x86_ops->hardware_enable(garbage);￼
(5811)   }
```

接下来，将初始化 KVM 的 **memslot 结构体**、**Bus 总线结构体信息**、**scru读/写锁信息**、**eventfd事件通知信息**、**mmu内存管理结构体**信息。

### 2.1.2. 返回kvm_vm的fd文件描述符

然后，调用 **anon\_inode\_getfd** 函数。

该函数设置了对**所有 KVM 的操作**都将给予**kvm\_vm这个共享文件**进行，该共享文件的操作封装在**kvm\_vm\_fops**结构体中，对**VM的操作**实际上就是对此文件的操作。因此，对其**ioctl调用**的是**kvm\_vm\_fops中的成员函数**。

代码5\-9 调用anon\_inode\_getfd创建kvm\-vm

```
// 
￼(1840)        fd = anon_inode_getfd("kvm-vm", &kvm_vm_fops, kvm, O_RDWR);￼
(1841)        if (fd < 0)￼
(1842)             kvm_put_kvm(kvm);￼
(1843)￼
(1844)        return fd;
```

通过anon\_inode\_getfd获得的**fd文件描述符**，就是供**用户态**使用的**vm\_fd**，用户态将通过该fd进行进一步的虚拟机操作，首先要做的事情是**初始化vCPU**。

# 3. vCPU的创建

创建vCPU实际上就是**创建vCPU的描述符**，在KVM中，**vCPU对应的数据结构体**为**kvm\_vcpu**。因此，创建vCPU的描述符，简单来说就是**分配相应大小的内存**，并且进行相应的**初始化**工作。kvm\_vcpu中描述符包含的内容有很多，通常会包含各个**平台通用的内容**和**平台相关的内容**。

在物理CPU上电之后，需要进一步初始化才可以使用。在这个过程中，硬件会自动将CPU初始化成特定的状态。

**kvm\_vcpu的初始化**也是一个类似的过程，将 **kvm\_vcpu 的各个数据结构体**设置成为可用的状态，通常需要包含如下内容。

- 分配**vCPU标识**，设置cpu\_id属于哪个KVM虚拟机，并且分配对该vCPU的**唯一标识符**。
- 初始化**虚拟寄存器组**。
- 初始化 kvm\_vcpu 的**状态信息**，设置 kvm\_vcpu 在被调度前需要配置的必要标志。
- 初始化额外的信息，并且配置**apic等虚拟化组件**。

## 3.1. vCPU的创建过程kvm_vm_ioctl_create_vcpu

接下来讲述KVM中**vCPU的创建过程**。

在获得了**fd\_vm**之后，通过**ioctl**调用**KVM\_CREATE\_VCPU指令**，可以对该fd\_vm对应的虚拟机**创建vCPU**，其入口函数地址在**kvm\_vm\_ioctl**函数(virt/kvm/kvm\_main.c)中，通过switch之后，程序流程将选择进入**kvm\_vm\_ioctl\_create\_vcpu**函数中进行处理，其代码如下。

代码5-10 kvm\_vm_ioctl\_create\_vcpu代码

```c
// virt/kvm/kvm_main.c
￼(1366)   static int kvm_vm_ioctl_create_vcpu(struct kvm *kvm, u32 id)￼
(1367)   {￼
(1368)        int r;￼
(1369)        struct kvm_vcpu *vcpu, *v;￼
(1370)￼
(1371)        vcpu = kvm_arch_vcpu_create(kvm, id);￼
(1372)        if (IS_ERR(vcpu))￼
(1373)             return PTR_ERR(vcpu);￼
(1374)￼
(1375)        preempt_notifier_init(&vcpu->preempt_notifier, &kvm_preempt_ops);￼
(1376)￼
(1377)        r = kvm_arch_vcpu_setup(vcpu);￼
(1378)        if (r)￼
(1379)             return r;￼
(1380)￼
(1381)        mutex_lock(&kvm->lock);￼
(1382)        if (atomic_read(&kvm->online_vcpus) == KVM_MAX_VCPUS) {￼
(1383)             r = -EINVAL;￼
(1384)             goto vcpu_destroy;￼
(1385)        }￼
(1386)￼
(1387)        kvm_for_each_vcpu(r, v, kvm)￼
(1388)             if (v->vcpu_id == id) {￼
(1389)                  r = -EEXIST;￼
(1390)                  goto vcpu_destroy;￼
(1391)             }￼
(1392)￼
(1393)        BUG_ON(kvm->vcpus[atomic_read(&kvm->online_vcpus)]);￼
(1394)￼
(1395)        /* Now it's all set up, let userspace reach it */￼
(1396)        kvm_get_kvm(kvm);￼
(1397)        r = create_vcpu_fd(vcpu);￼
(1398)        if (r < 0) {￼
(1399)             kvm_put_kvm(kvm);￼
(1400)             goto vcpu_destroy;￼
(1401)        }￼
(1402)￼
(1403)        kvm->vcpus[atomic_read(&kvm->online_vcpus)] = vcpu;￼
(1404)        smp_wmb();￼
(1405)        atomic_inc(&kvm->online_vcpus);￼
(1406)￼
(1407)   #ifdef CONFIG_KVM_APIC_ARCHITECTURE￼
(1408)        if (kvm->bsp_vcpu_id == id)￼
(1409)             kvm->bsp_vcpu = vcpu;￼
(1410)   #endif￼
(1411)        mutex_unlock(&kvm->lock);￼
(1412)        return r;￼
(1413)￼
(1414)   vcpu_destroy:￼
(1415)        mutex_unlock(&kvm->lock);￼
(1416)        kvm_arch_vcpu_destroy(vcpu);￼
(1417)        return r;￼
(1418)   }
```

### 3.1.1. kvm_arch_vcpu_create创建kvm_vcpu

首先，在1371行，调用**kvm\_arch\_vcpu\_create**函数创建一个**kvm\_vcpu结构体！！！**。该创建内容**与架构相关**，因此，直接调用**kvm\_x86\_ops**中的create\_vcpu方法执行。

(kvm\_arch\_vcpu\_create在arch/x86/kvm/x86.c中, 调用struct kvm\_x86\_ops vmx\_x86\_ops(定义在arch/x86/kvm/vmx.c)中的create\_vcpu(即vmx\_create\_vcpu, 在arch/x86/kvm/vmx.c中))

**每个vCPU**的创建先alloc一个struct **vcpu\_vmx**里面包含了msr, vmcs, vcpu等, 这里不是主要对kvm\_vcpu做初始化

虽然代码不同，但是AMD平台和Intel平台实现的思路都是类似的：

- **先指定CPUID之后**，
- 接着**初始化MSR！！！** 和**VMCS！！！等寄存器**，(**每个VCPU都有**)
- 最后完成**I/O**和**内存部分寄存器的初始化**，为被**初次调度运行**做好准备。

### 3.1.2. kvm_arch_vcpu_setup初始化kvm_vcpu

其次，在1377行调用**kvm\_arch\_vcpu\_setup**函数对**kvm\_vcpu**中的数据结构进行**初始化**。

这里将先调用**kvm\_x86\_ops**中的**put\_vcpu函数**，实现将**vCPU的参数信息**加载入**CPU**中，并且执行**MMU初始化**和**CPU复位操作**。

### 3.1.3. vCPU检测

在 1381～1391 行中，进行了**两个检测**。

- 第一个是检测到如果**当前的 vCPU数量**已经达到了**系统设置的上限** **KVM\_MAX\_VCPU**，则将销毁刚才创建的实例。
- 第二个是如果**当前的vCPU**创建出来已经加入了**某一个已有的KVM主机**，则也将销毁该实例。

### 3.1.4. 创建vcpu_fd

然后，在 1396～1405 行，会创建**当前 vCPU** 对应的文件描述符 **vcpu\_fd**，并且将**kvm\_vcpu**添加入**KVM**的**vCPU数组**中。

这里有一个特别之处，是使用了 atom\_read 和 atom\_inc 宏，这两个宏能够保证在进行 KVM 虚拟机的 vCPU添加时按照给定的顺序，不会因为执行中途的中断、进程切换等方式导致添加到不正确的kvm\_vcpu数组中。

### 3.1.5. 释放内核锁和返回vcpu_fd

最后，释放掉所有的内核锁，完成本次vCPU的创建工作。

返回**vcpu\_fd**

# 4. vCPU的运行

在创建完**VM**和**vCPU**并且完成了**初始化工作**之后，就可以通过**调度程序调度执行**。因为在Linux中，KVM虚拟机作为一个**系统线程**运行，因此，KVM虚拟机的调度程序实际上也就是Linux的调度程序，具体的调度将在后文qemu部分进行讨论，在当前，**KVM的调用**是从ioctl的**KVM\_RUN指令**字开始的。

```c
// virt/kvm/kvm_main.c
static long kvm_vcpu_ioctl(struct file *filp,
			   unsigned int ioctl, unsigned long arg)
{
    case KVM_RUN:
		r = -EINVAL;
		if (arg)
			goto out;
		if (unlikely(vcpu->pid != current->pids[PIDTYPE_PID].pid)) {
			/* The thread running this VCPU changed. */
			struct pid *oldpid = vcpu->pid;
			struct pid *newpid = get_task_pid(current, PIDTYPE_PID);

			rcu_assign_pointer(vcpu->pid, newpid);
			if (oldpid)
				synchronize_rcu();
			put_pid(oldpid);
		}
		r = kvm_arch_vcpu_ioctl_run(vcpu, vcpu->run);
		trace_kvm_userspace_exit(vcpu->run->exit_reason, r);
		break;
}
```

**KVM\_RUN指令字**针对**fd\_vcpu描述符**操作，当**vCPU准备完成**之后，即可通过该指令让虚拟机运行起来。

**虚拟机运行**的主要任务则是进行**上下文切换**。上下文切换的内容较多，通常包括**通用寄存器**、**浮点寄存器**、**段寄存器**、**控制寄存器**、**MSR**等，在**KVM**中，还包括**APIC状态**、**TLB**等。

通常，进行**上下文切换的过程**可以归纳为如下步骤。

1）**KVM保存自己的上下文**。

2）KVM通过使用将**kvm\_vcpu结构体**中的相关上下文加载到**物理CPU**中。

3）KVM执行**kvm\_x86\_ops**中的**run\_vcpu函数**，调用具体的平台相关指令，进入**虚拟机运行环境**中。

由此可见，上下文切换次数过于频繁会带来不小的性能开销，因此，很有必要对这方面进行优化。和操作系统进行**进程切换的思路**一样，KVM使用**Lazy Save/Restore** 的方法进行优化。其基本思想是**尽量不要对寄存器进行恢复/保存操作**，直到必须要这么做的时候，才进行类似的操作。

**执行vCPU**的请求首先发送到**kvm\_vcpu\_ioctl函数**中，然后**加载vCPU参数**，调用**kvm\_arch\_vcpu\_ioctl\_run**函数进入**具体的vCPU运行**环节。(arch/x86/kvm/x86.c)

1）通过调用**sigprocmask函数**，保证在**vCPU的初始化**过程中，不会因为来自**其他线程的信号干扰而中断**。

2）将**vCPU的状态**切换为**KVM\_MP\_STATE\_UNINITIALIZED**。

3）配置**APIC**和**mmio**的**中断信息**。

4）对要进入的虚拟机进行一些**关键指令的测试**，在测试中主要针对**内存读/写**情况进行测试。

5）将**vCPU**中保存的**上下文信息（寄存器状态等**）写入指定的位置。

6）接下来才开始实质性的工作，调用\_**vcpu\_run函数**进行后续处理。

\_vcpu\_run函数的代码如下。

代码5\-11 \_vcpu\_run函数

```c
// arch/x86/kvm/x86.c
(5232)   static int __vcpu_run(struct kvm_vcpu *vcpu)￼
(5233)   {￼
(5234)        int r;￼
(5235)        struct kvm *kvm = vcpu->kvm;￼
(5236)￼
(5237)        if (unlikely(vcpu->arch.mp_state == KVM_MP_STATE_SIPI_RECEIVED)) {￼
(5238)             pr_debug("vcpu %d received sipi with vector # %x\n",￼
(5239)                   vcpu->vcpu_id, vcpu->arch.sipi_vector);￼
(5240)             kvm_lapic_reset(vcpu);￼
(5241)             r = kvm_arch_vcpu_reset(vcpu);￼
(5242)             if (r)￼
(5243)                  return r;￼
(5244)             vcpu->arch.mp_state = KVM_MP_STATE_RUNNABLE;￼
(5245)        }￼
(5246)￼
(5247)        vcpu->srcu_idx = srcu_read_lock(&kvm->srcu);￼
(5248)        vapic_enter(vcpu);￼
(5249)￼
(5250)        r = 1;￼
(5251)        while (r > 0) {￼
(5252)             if (vcpu->arch.mp_state == KVM_MP_STATE_RUNNABLE)￼
(5253)                  r = vcpu_enter_guest(vcpu);￼
(5254)             else {￼
(5255)                  srcu_read_unlock(&kvm->srcu, vcpu->srcu_idx);￼
(5256)                  kvm_vcpu_block(vcpu);￼
(5257)                  vcpu->srcu_idx = srcu_read_lock(&kvm->srcu);￼
(5258)                  if (kvm_check_request(KVM_REQ_UNHALT, vcpu))￼
(5259)                  {￼
(5260)                       switch(vcpu->arch.mp_state) {￼
(5261)                       case KVM_MP_STATE_HALTED:￼
(5262)                            vcpu->arch.mp_state =￼
(5263)                                KVM_MP_STATE_RUNNABLE;￼
(5264)                       case KVM_MP_STATE_RUNNABLE:￼
(5265)                            break;￼
(5266)                       case KVM_MP_STATE_SIPI_RECEIVED:￼
(5267)                       default:￼
(5268)                            r = -EINTR;￼
(5269)                            break;￼
(5270)                       }￼
(5271)                  }￼
(5272)             }￼
(5273)￼
(5274)             if (r <= 0)￼
(5275)                  break;￼
(5276)￼
(5277)             clear_bit(KVM_REQ_PENDING_TIMER, &vcpu->requests);￼
(5278)             if (kvm_cpu_has_pending_timer(vcpu))￼
(5279)                  kvm_inject_pending_timer_irqs(vcpu);￼
(5280)￼
(5281)             if (dm_request_for_irq_injection(vcpu)) {￼
(5282)                  r = -EINTR;￼
(5283)                  vcpu->run->exit_reason = KVM_EXIT_INTR;￼
(5284)                  ++vcpu->stat.request_irq_exits;￼
(5285)             }￼
(5286)             if (signal_pending(current)) {￼
(5287)                  r = -EINTR;￼
(5288)                  vcpu->run->exit_reason = KVM_EXIT_INTR;￼
(5289)                  ++vcpu->stat.signal_exits;￼
(5290)             }￼
(5291)             if (need_resched()) {￼
(5292)                  srcu_read_unlock(&kvm->srcu, vcpu->srcu_idx);￼
(5293)                  kvm_resched(vcpu);￼
(5294)                  vcpu->srcu_idx = srcu_read_lock(&kvm->srcu);￼
(5295)             }￼
(5296)        }￼
(5297)￼
(5298)        srcu_read_unlock(&kvm->srcu, vcpu->srcu_idx);￼
(5299)￼
(5300)        vapic_exit(vcpu);￼
(5301)￼
(5302)        return r;￼
(5303)   }
```

该函数较长，执行的内容也比较重要，重要的部分如下。

1）在5232～5240行中，将**虚拟APIC**和**vCPU**的状态**重置**。这个操作通过调用kvm\_lapic\_reset和kvm\_arch\_vcpu\_reset函数实现。

2）在5245～5270行中，正常情况下，将kvm\_vcp\-\>arch.mp\_state的取值作为**单机未运行的取值**：KVM\_MP\_STATE\_RUNNABLE；如果 vCPU 在 VM 中处于其他状态，则会出错。在运行中有一个**循环**，直到确认了**vcpu\_enter\_guest**函数执行完毕，即物理CPU进入了GUEST状态并且执行完成后，才会执行下一步操作。

3）在5273～5278行中，将检查本次执行的一些结果。如果CPU当前有挂起的定时器或者其他中断，则会保存该中断的现场。

4）在5286～5289行中，如果对当前执行的vCPU需要调度，会引用Linux的进程调度子程序进行一次任务调度。

在上面的执行流程中，最重要的执行在 **vcpu\_enter\_guest** 函数中，该函数实现了进入客户机并且执行具体指令的操作。