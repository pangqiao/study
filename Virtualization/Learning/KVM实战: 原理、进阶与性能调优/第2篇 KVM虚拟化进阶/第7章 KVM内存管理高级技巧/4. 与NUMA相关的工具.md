
<!-- @import "[TOC]" {cmd="toc" depthFrom=1 depthTo=6 orderedList=false} -->

<!-- code_chunk_output -->

- [0 NUMA和UMA](#0-numa和uma)
- [1 numastat](#1-numastat)
- [2 numad](#2-numad)
  - [2.1 numad的参数](#21-numad的参数)
  - [2.1 numad相关操作](#21-numad相关操作)
    - [2.1.1 启动和退出numad](#211-启动和退出numad)
    - [2.1.2 numad将QEMU进程绑定到numa节点](#212-numad将qemu进程绑定到numa节点)
      - [2.1.2.1 auto NUMA balancing: 没有numad](#2121-auto-numa-balancing-没有numad)
      - [2.1.2.2 打开numad](#2122-打开numad)
- [3 numactl](#3-numactl)
  - [3.1 numactl主要用法](#31-numactl主要用法)
  - [3.2 numactl示例](#32-numactl示例)
    - [3.2.1 查看numa节点信息](#321-查看numa节点信息)
    - [3.2.2 numactl启动客户机](#322-numactl启动客户机)
      - [3.2.2.1 只运行在一个节点上](#3221-只运行在一个节点上)
      - [3.2.2.2 均匀占用节点资源](#3222-均匀占用节点资源)
- [4 小结](#4-小结)

<!-- /code_chunk_output -->

# 0 NUMA和UMA

NUMA（Non\-Uniform Memory Access，非统一内存访问架构）是相对于UMA（Uniform Memory Access）而言的。早年的计算机架构都是UMA，如图7\-2所示。所有的CPU处理单元（Processor）均质地通过共享的总线访问内存，所有CPU访问所有内存单元的速度是一样的。在多处理器的情形下，多个任务会被分派在各个处理器上并发执行，则它们**竞争内存资源**的情况会非常频繁，从而引起效率的下降。

![](./images/2019-05-28-21-14-56.png)

所以，随着多处理器架构的逐渐普及以及数量的不断增长，NUMA架构兴起，如图7\-3所示。处理器与内存被划分成一个个的节点（node），处理器访问自己节点内的内存会比访问其他节点的内存快。

![](./images/2019-05-28-21-15-17.png)

Intel Xeon系列平台从**2007年**的**Nehalem**那一代开始，就支持NUMA架构了。现在主流的E5、E7系列Xeon平台，通常是2个、4个NUMA node的。

# 1 numastat

numastat用来查看**某个（些）进程**或者**整个系统**的**内存消耗**在**各个NUMA节点的分布情况**。

它的典型输出如下：

```
[root@kvm-host ~]# numastat 
                           node0           node1
numa_hit                72050204        55925951
numa_miss                      0               0
numa_foreign                   0               0
interleave_hit             38068           39139
local_node              71816493        54027058
other_node                233711         1898893
```

- **numa\_hit**表示成功地从**该节点**分配到的**内存页数**。
- **numa\_miss**表示成功地从该节点分配到的**内存页数**，但其本意是希望从别的节点分配，失败以后退而求其次从该节点分配。
- numa\_foreign与numa\_miss互为“影子”，**每个numa\_miss**都来自**另一个节点的numa\_foreign**。
- interleave\_hit，有时候内存请求是没有NUMA节点偏好的，此时会**均匀分配自各个节点（interleave**），这个数值就是这种情况下从该节点分配出去的内存页面数。
- local\_node表示分配给运行在**同一节点的进程**的**内存页数**。
- other\_node与上面相反。**local\_node**值**加上other\_node**值就是**numa\_hit值**。

以上数值**默认**都是**内存页数**，要看具体**多少MB**，可以通过加上\-**n参数**实现。

numastat还可以**只看某些进程**，甚至只要名字片段匹配。比如看QEMU进程的内存分布情况，可以通过“**numastat qemu**”即可。比如：

```
[root@kvm-host ~]# numastat qemu

Per-node process memory usage (in MBs) for PID 73658 (qemu-system-x86)
                           Node 0          Node 1           Total
                  --------------- --------------- ---------------
Huge                         0.00            0.00            0.00
Heap                         0.00           21.96           21.96
Stack                        0.00            8.37            8.37
Private                      0.03         1538.17         1538.20
----------------  --------------- --------------- ---------------
Total                        0.03         1568.49         1568.52
```

更多参数及用法，大家可以通过man numastat命令了解。

# 2 numad

numad是一个可以**自动管理NUMA亲和性**（affinity）的**工具**（同时也是一个后台进程）。

- 它**实时监控NUMA拓扑结构（topology**）和**资源使用**，
- 并**动态调整**。
- 同时它还可以在**启动一个程序前**，提供**NUMA优化建议**。

与numad功能类似，Kernel的**auto NUMA balancing**（/proc/sys/kernel/**numa\_balancing**）也是进行**动态NUMA资源的调节**。

numad启动后会**覆盖**Kernel的auto NUMA balancing功能。

numad与THP和KSM都有些纠葛，下面讲到。

## 2.1 numad的参数

numad比较复杂，它有很多参数进行**精细化控制**，下面是几个重要的。

- \-p\<pid\>，\-x\<pid\>，\-r\<pid\>，分别指定numad**针对哪些pid**以及**不针对哪些pid**进行**自动的NUMA资源优化**。

numad自己**内部**维护一个**inclusive list**和一个**exclusive list**；\-p\<pid\>、\-x\<pid\>就是分别往这两个list里面**添加进程id**；\-r\<pid\>就是从这两个**list里面移除**。

在numad**首次启动**时候，可以**重复多个**\-p或者\-x；启动后，每次调用numad只能跟一个\-p、\-x或者\-r参数。

**默认没有这些指定**的话，numad会对系统**所有进程进行NUMA资源优化**。

- \-S 0\/1，**0**表示只对**inclusive list**里面的进程进行NUMA优化；**1**表示对**除exclusive list**以外的**所有进程**进行优化。通常，\-S与\-p、\-x搭配使用。

- \-R\<cpu\_list\>，reserve，指定一些CPU是numad不能染指的，numad**不会**在自动优化NUMA资源的时候把**进程放到这些CPU上**去运行。

- \-t\<百分比\>，它指示**逻辑（logical）CPU**（比如Intel HyperThread打开时）运算能力对于它的**HW core**的比例。

这个值关系到**numad内部调配资源**时的计算，**默认是20%**。

- \-u\<百分比\>，numad最多能消耗**每个NUMA节点多少资源**（CPU和内存），默认是**85%**。

numad毕竟**不能取代内核调度器**，并不能接管系统里所有route的调度，所以，留有余地是必须的。但当你确定将一个node专属（dedicate）给一个进程时，也可以设置\-u 100%，甚至超过100%，但要小心。

- \-C 0\/1，是否将NUMA节点的inactive file cache作为free memory对待。默认为1，表示进程的inactive file cache不纳入NUMA优化的考量，即如果一个进程还有一些inactive file cache留在另一个节点上，numad也不会把它搬过来。

- \-K 0\/1，控制是否将interleaved memory合并到一个NUMA节点。默认是合并的，但要注意，合并到一个节点并不一定有最好的performance，而应该根据实际的work\-load来决定。比如，如果一个系统里主要就是一个大型的数据库应用程序（大量内存访问且地址随机），\-K 1禁止numad合并interleaved memory反而有更好的性能。

- \-m\<百分比\>，它是一个阈值，表示当内存中在本地节点的数量达到它所属进程的内存总量的多少时，numad停止对该进程的NUMA优化。

- \-i\<最小间隔：最大间隔\>，最小值可以省略。它设置numad 2次扫描系统情况的时间间隔。通常用它来终止（退出）numad，-i 0。

- \-H\<时间间隔\>，它设置（**override）透明大页**（见7.2节）的**扫描间隔时间**。

**默认地**，numad会将/sys/kernel/mm/tranparent\_hugepage/khugepaged/**scan\_sleep\_millisecs**值从默认的**10000毫秒**缩短为**1000毫秒**，因为**更激进的透明大页合并**更有利于numad将页面在**NUMA节点之间迁移**。

- \-w\<NCPUS\[:MB]\>，它就是numad**一次性地运行一下**（而**不是作为系统后台daemon**）以供咨询：“我有一个应用程序将要运行，它会需要NCPUS个CPU，M兆内存，numad，你告诉我该把它放到哪个NUMA节点上运行好啊？”**numad**此时会**返回**一个**合适的NUMA node list**，这个list可以作为后面**numactl**（下面介绍）的**参数**。

另外，**与THP的关联**，上面\-**H参数**已涉及。

**与KSM的关联**，在于/sys/kernel/mm/ksm/**merge\_nodes**最好**设置为0**，禁止KSM跨NUMA节点地同页合并。

## 2.1 numad相关操作

我们通过几个典型用例来了解上面部分重要参数的用法。

### 2.1.1 启动和退出numad

1）**启动和退出numad**；退出通过“\-**i 0**”完成。

```
[root@kvm-host ~]# numad

[root@kvm-host ~]# ps aux | grep numad
root   175821  0.2  0.0  19860  572 ?      Ssl  20:44  0:00 numad
root   175827  0.0  0.0 112652  960 pts/2  S+   20:44  0:00 grep --color=auto numad

[root@kvm-host ~]# numad -i 0      #退出numad
[root@kvm-host ~]# ps aux | grep numad
root   175836  0.0  0.0 112652 960 pts/2    S+  20:44  0:00 grep --color=auto numad
```

### 2.1.2 numad将QEMU进程绑定到numa节点

通过**numad**将QEMU进程**搬到一个NUMA节点**上。

#### 2.1.2.1 auto NUMA balancing: 没有numad

先看看**没有numad**的时候，内核的**auto NUMA balancing**会是怎样的行为。

查看当前情况，并启动一个客户机。

```
[root@kvm-host ~]# cat /proc/sys/kernel/numa_balancing
1

[root@kvm-host ~]# qemu-system-x86_64 -enable-kvm -cpu host -smp 4 -m 8G -drive file=./rhel7.img,format=raw,if=virtio -device virtio-net-pci,netdev=nic0 -netdev bridge,id=nic0,br=virbr0 -snapshot
```

启动后，查看numastat。可以看到，**客户机**所使用的内存，在**两个节点上都有分布**。

```
[root@kvm-host ~]# numastat qemu-system

Per-node process memory usage (in MBs) for PID 61898 (qemu-system-x86)
                           Node 0          Node 1           Total
                  --------------- --------------- ---------------
Huge                         0.00            0.00            0.00
Heap                        19.80            0.36           20.17
Stack                        8.23            0.11            8.34
Private                    127.71          893.88         1021.60
----------------  --------------- --------------- ---------------
Total                      155.75          894.36         1050.11
```

接下来在**客户机里进行编译内核的行为**，并时不时地在宿主机中查看其numastat的状况。

```
[root@kvm-guest linux-4.9]# make -j 4
```

随着**客户机的运行**，可以看到**两个节点**依然各自分布着一些**内存占用**。

```
[root@kvm-host ~]# numastat qemu-system

Per-node process memory usage (in MBs) for PID 61898 (qemu-system-x86)
                           Node 0          Node 1           Total
                  --------------- --------------- ---------------
Huge                         0.00            0.00            0.00
Heap                        20.69            1.71           22.40
Stack                       10.41            6.21           16.63
Private                   1192.11         2469.65         3661.76
----------------  --------------- --------------- ---------------
Total                     1223.21         2477.57         3700.79
```

#### 2.1.2.2 打开numad

接下来我们**打开numad**，并重做上面的实验。

可以看到，一开始**打开numad之前**，客户机（QEMU进程）的内存**分布于两个节点**上，随着客户机的运行，**numad**把内存都搬到**一个节点**上了。

```
[root@kvm-host ~]# qemu-system-x86_64 -enable-kvm -cpu host -smp 4 -m 8G -drive file=./rhel7.img,format=raw,if=virtio -device virtio-net-pci,netdev=nic0 -netdev bridge,id=nic0,br=virbr0 -snapshot

[root@kvm-host ~]# ps aux | grep -i qemu
root  61898  130  0.7 10604948 996800 pts/1 Sl+  15:08   0:19 qemu-system-x86_64 -enable-kvm -cpu host -smp 4 -m 8G -drive file=./rhel7.img,format=raw,if=virtio -device virtio-net-pci,netdev=nic0 -netdev bridge,id=nic0,br=virbr0 -snapshot
root  61959  0.0  0.0 112652  976 pts/8  S+  15:09  0:00 grep --color=auto -i qemu

[root@kvm-host ~]# numad -S 0 -p 64686

[root@kvm-host ~]# numastat 64686

Per-node process memory usage (in MBs) for PID 64686 (qemu-system-x86)
                           Node 0          Node 1           Total
                  --------------- --------------- ---------------
Huge                         0.00            0.00            0.00
Heap                         5.05           16.94           21.98
Stack                        0.06            2.06            2.12
Private                    137.43         1121.89         1259.32
----------------  --------------- --------------- ---------------
Total                      142.54         1140.89         1283.43
[root@kvm-host ~]# numastat 64686

Per-node process memory usage (in MBs) for PID 64686 (qemu-system-x86)
                           Node 0          Node 1           Total
                  --------------- --------------- ---------------
Huge                         0.00            0.00            0.00
Heap                         0.00           22.09           22.09
Stack                        0.00           20.58           20.58
Private                      0.00         2105.27         2105.27
----------------  --------------- --------------- ---------------
Total                        0.00         2147.93         2147.93
```

# 3 numactl

如果说**numad**是事后（**客户机起来以后**）**调节NUMA资源分配**，那么**numactl**则是**主动**地在程序起来时候就**指定好它的NUMA节点**。

numactl其实不止它名字表示的那样

- **设置NUMA相关亲和性**，
- 它还可以**设置共享内存/大页文件系统的内存策略**，
- 以及**进程的CPU和内存的亲和性**。

## 3.1 numactl主要用法

它的主要用法如下：

```
numactl  [  --all  ]  [ --interleave nodes ] [ --preferred node ] [ --membind nodes ] [ --cpunodebind nodes ] [ --physcpubind cpus ] [ --localalloc ] [--] command {arguments...}
```

它的一些主要参数如下：
- \-\-hardware，列出来目前系统中**可用的NUMA节点**，以及它们**之间的距离**（distance）。
- \-\-membind，确保**command执行**时候**内存都是从指定的节点**上分配；如果该节点没有足够内存，返回失败。
- \-\-cpunodebind，确保command**只在指定node的CPU**上面执行。
- \-\-phycpubind，确保command**只在指定的CPU**上执行。
- \-\-localalloc，指定**内存只从本地节点**上分配。
- \-\-preferred，指定一个偏好的节点，command执行时内存**优先从这个节点分配**，不够的话才从别的节点分配。

其他还有更多参数，读者可以通过“man numactl”命令了解。

## 3.2 numactl示例

我们还是通过一个简单的例子来了解一下numactl的一般用法。

### 3.2.1 查看numa节点信息

通过\-\-hardware，我们可以看到，笔者系统上有两个NUMA节点，相互间的distance为21。

```
[root@kvm-host ~]# numactl --hardware
available: 2 nodes (0-1)
node 0 cpus: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65
node 0 size: 65439 MB
node 0 free: 43948 MB
node 1 cpus: 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87
node 1 size: 65536 MB
node 1 free: 53230 MB
node distances:
node   0   1 
  0:  10  21 
  1:  21  10
```

### 3.2.2 numactl启动客户机

#### 3.2.2.1 只运行在一个节点上

我们用numactl来**控制启动一个客户机**，让它**只运行在节点1**上，然后通过numastat来确认。可以看到，这个qemu\-system\-x86\_64进程，一开始就被绑定到了节点1上。

```
[root@kvm-host ~]# numactl --membind=1 --cpunodebind=1 -- qemu-system-x86_64 -enable-kvm -cpu host -smp 4 -m 8G -drive file=./rhel7.img,format=raw,if=virtio -device virtio-net-pci,netdev=nic0 -netdev bridge,id=nic0,br=virbr0 -snapshot --daemonize
[root@kvm-host ~]# numastat qemu-system

Per-node process memory usage (in MBs) for PID 72511 (qemu-system-x86)
                           Node 0          Node 1           Total
                  --------------- --------------- ---------------
Huge                         0.00            0.00            0.00
Heap                         0.00           23.03           23.03
Stack                        0.00           10.41           10.41
Private                      0.00          979.96          979.96
----------------  --------------- --------------- ---------------
Total                        0.00         1013.41         1013.41
```

#### 3.2.2.2 均匀占用节点资源

我们如果想让客户机均匀地占用两个节点的资源，可以使用\-\-interleave参数。

```
[root@kvm-host ~]# numactl --interleave=0,1 -- qemu-system-x86_64 -enable-kvm -cpu host -smp 4 -m 8G -drive file=./rhel7.img,format=raw,if=virtio -device virtio-net-pci,netdev=nic0 -netdev bridge,id=nic0,br=virbr0 -snapshot --daemonize
[root@kvm-host ~]# numastat qemu-system

Per-node process memory usage (in MBs) for PID 73119 (qemu-system-x86)
                           Node 0          Node 1           Total
                  --------------- --------------- ---------------
Huge                         0.00            0.00            0.00
Heap                        11.35           11.55           22.89
Stack                        6.12            2.13            8.25
Private                    470.18          467.83          938.01
----------------  --------------- --------------- ---------------
Total                      487.65          481.50          969.15
```

# 4 小结 

综上，我们介绍了**numastat**、**numad**、**numactl**三个常用的NUMA控制、查看的工具。

- **numad**可以在程序（包括客户机的QEMU进程）起来以后，**事后调节**、**优化其NUMA资源**。
- **numactl**则是在**一开始**就指定好这个命令的**NUMA政策**。
- **numastat**则可以用来方便地**查看当前系统**或**某个进程的NUMA资源**使用分布情况。

在想要专属地让**某个客户机**得到**优先服务**的时候，我们可以把**KSM关闭**，通过**numactl**将客户机**QEMU进程绑定在某个node或某些CPU**上。

当我们想要**更高的客户机密度**，而**不考虑特别的服务质量**的时候，我们可以通过**numactl \-\-all**，同时**打开KSM**，**关掉numad**。
