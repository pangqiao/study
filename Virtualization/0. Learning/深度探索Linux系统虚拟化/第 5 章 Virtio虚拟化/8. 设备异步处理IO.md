
前面讨论的模拟设备中的I/O处理都是同步的，也就是说，当驱动发起I/O通知VIRTIO_PCI_QUEUE_NOTIFY后，触发VM exit，在控制权从Guest转换到kvmtool中的模拟设备后，一直要等到模拟设备处理完I/O，模拟设备才调用kvm__irq_line向Guest发送中断，CPU才会从模拟设备返回到Guest系统。可见，在模拟设备进行I/O时，Guest系统是被block住的，如图5-11所示.

![2024-02-27-16-39-15.png](./images/2024-02-27-16-39-15.png)

而在真实的块设备中，操作系统只是挂起发起I/O的任务，继续运行其他就绪任务，而不是把整个系统都block住。所以，模拟设备的I/O处理过程也完全可以抽象为另外一个线程，和VCPU这个线程并发执行。在单核系统上，I/O处理线程和VCPU线程可以分时执行，避免Guest系统长时间没有响应；在多核系统上，I/O处理线程和VCPU线程则可以利用多核并发执行。在异步模式下，VCPU这个线程只需要告知一下模拟设备开始处理I/O，然后可以迅速地再次切回到Guest。在模拟设备完成I/O处理后，再通过中断的方式告知Guest I/O处理完成了。这个过程如下图所示。

![2024-02-27-16-40-14.png](./images/2024-02-27-16-40-14.png)

起初，Virtio blk使用了一个单独的线程处理I/O，后来，kvmtool增加了一个线程池。每当I/O来了之后，会在队列中挂入线程池，然后唤醒线程处理任务。线程池的实现我们就不详细介绍了，我们仅关注Virtio blk设备处理I/O逻辑的变迁:

```cpp
commit fb0957f29981d280fe890b7aadcee4f3df95ca65
kvm tools: Use threadpool for virtio-blk
kvmtool.git/virtio-blk.c

static bool virtio_blk_pci_io_out(struct kvm *self, uint16_t port, void *data, int size, uint32_t count)
{
    ...
	case VIRTIO_PCI_QUEUE_PFN: {
    ...
		blk_device.jobs[blk_device.queue_selector] =
			thread_pool__add_jobtype(self, virtio_blk_do_io, queue);

		break;
	}
    ...
	case VIRTIO_PCI_QUEUE_NOTIFY: {
		uint16_t queue_index;
		queue_index		= ioport__read16(data);
		thread_pool__signal_work(blk_device.jobs[queue_index]);
		break;
	}
    ...
}

static void virtio_blk_do_io(struct kvm *kvm, void *param)
{
	struct virt_queue *vq = param;

	while (virt_queue__available(vq))
		virtio_blk_do_io_request(kvm, vq);

	kvm__irq_line(kvm, VIRTIO_BLK_IRQ, 1);
}
```

当Guest中的Virtio blk驱动初始化Virtqueue时，在将Virtqueue 的地址告知模拟设备，即写I/O地址VIRTIO_PCI_QUEUE_PFN时，我们看到模拟设备将创建一个job，job的callback函数就是之前同步处理部分的代码逻辑。每当Guest中的驱动通知设备处理I/O request，模拟设备会将这个job添加到线程池的队列，然后唤醒线程池中的线程处理这个job。通过这种方式，函数virtio_blk_pci_io_out不必再等待I/O 处理完成，而是马上再次进入内核空间，切入Guest。在线程池中的某个线程处理完这个job，即函数virtio_blk_do_io的最后，将调用 kvm__irq_line向Guest注入中断，告知Guest设备已经处理完了I/O。使用异步的方式，即使执行长耗时I/O，Guest也不会被block，也不会出现不反应的情况，而且对于多核系统，可以充分利用多核的并发，处理I/O的线程和VCPU(Guest)分别在不同核上同时运行。

