
本节介绍一些与性能优化相关的新指令, 如AVX和XSAVE指令的支持、AES新指令的支持. 最后介绍完全暴露宿主机CPU的特性给客户机.
9.3.1　AVX/AVX2/AVX512
AVX(Advanced Vector Extensions, 高级矢量扩展)是Intel和AMD的x86架构指令集的一个扩展, 它最早是Intel在2008年3月提出的指令集, 在2011年Intel发布Sandy Bridge处理器时开始第一次正式支持AVX, 随后AMD最新的处理器也支持AVX指令集. Sandy Bridge是Intel处理器微架构从Nehalem(2009年)后的革新, 而引入AVX指令集是Sandy Bridge一个较大的新特性, 甚至有人将AVX指令认为Sandy Bridge中最大的亮点, 其重要性堪比1999年PentiumⅢ处理中引入的SSE(流式SIMD扩展)指令集. AVX中的新特性有: 将向量化宽度从128位提升到256位, 且将XMM0～XMM15寄存器重命名为YMM0～YMM15; 引入了三操作数、四操作数的SIMD指令格式; 弱化了SIMD指令中对内存操作对齐的要求, 支持灵活的不对齐内存地址访问.
向量就是多个标量的组合, 通常意味着SIMD(单指令多数据), 就是一个指令同时对多个数据进行处理, 达到很大的吞吐量. 早期的超级计算机大多都是向量机, 而随着图形图像、视频、音频等多媒体的流行, PC处理器也开始向量化. x86上最早出现的是1996年的MMX(多媒体扩展)指令集, 之后是1999年的SSE指令集, 它们分别是64位向量和128位向量, 比超级计算机用的要短得多, 所以叫作“短向量”. Sandy Bridge的AVX指令集将向量化宽度扩展到了256位, 原有的16个128位XMM寄存器扩充为256位的YMM寄存器, 可以同时处理8个单精度浮点数和4个双精度浮点数. AVX的256位向量仅支持浮点, 到了AVX2(Haswell平台开始引入)大多数整数运算指令已经支持256位向量. 而最新的AVX512(Skylake平台开始引入)已经扩展到512位向量.
最新的AVX512已经不是一个单一的指令, 而是一个指令家族(family). 它目前包含(以后还会继续扩充)AVX-512F, AVX-512CD, AVX-512ER, AVX-512PF, AVX-512BW, AVX-512DQ, AVX-512VL. 并且, 这些指令集在不同的硬件平台上只部分出现, 且编译选项不同, 会分别编译出不同指令集的二进制文件.
CPU硬件方面, Intel的Xeon Phi x200(Knights Landing, 2016年发布)、Xeon X5-E26xx v5(2017年下半年发布), 都支持最新的AVX512. 在编译器方面, GCC 4.9版本(binutil 2.4.0)、ICC(Intel C/C++Compiler)15.2版本都已经支持AVX512. 截至笔者写作时, MS Visual Studio还没有支持.
在操作系统方面, Linux内核3.19、RHEL7.3已经支持AVX512. 不过在应用程序方面, 目前只有较少的软件提供了对AVX512的支持, 因为新的指令集的应用需要一定的时间, 就像当年SSE指令集出来后, 也是过了几年后才被多数软件支持. 但对AVX2的支持已经比较普遍, 比如QEMU自身, 在笔者的Xeon E5-2699 v4平台上编译的, 就已经支持了AVX2优化.

[root@kvm-host qemu]# cat config-host.mak | grep -i avxCONFIG_AVX2_OPT=y

从网上的这篇文章http://www.sisoftware.eu/2016/02/24/future-performance-with-avx512-in-sandra-2016-sp1/, 我们可以看到, 从SSE→AVX2→AVX512, 每一代都有至少60+%乃至最多2.35倍的多媒体处理性能提升.
9.3.2　XSAVE指令集
另外, XSAVE指令集(包括XGETBV、XSETBV、XSAVE、XSAVEC、XSAVEOPT、XSAVES、XRSTOR、XSAVES等)是在Intel Nehalem以后的处理器中陆续引入、用于保存和恢复处理器扩展状态的, 这些扩展状态包括但不限于上节提到的AVX特性相关的寄存器. 在KVM虚拟化环境中, 客户机的动态迁移需要保存处理器状态, 然后在迁移后恢复处理器的执行状态, 如果有AVX指令要执行, 在保存和恢复时也需要XSAVE(S)/XRSTOR(S)指令的支持.
下面介绍一下如何在KVM中为客户机提供AVX、XSAVE特性.
1)检查宿主机中AVX、XSAVE指令系列的支持, Intel Broadwell硬件平台支持到AVX2、XSAVE、XSAVEOPT.

[root@kvm-host ~]# cat /proc/cpuinfo | grep -E "xsave|avx" | uniqflags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch ida arat epb pln pts dtherm intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local

2)启动客户机, 将AVX、XSAVE特性提供给客户机使用(-cpu host, 完全暴露宿主机CPU特性, 详见9.3.4节). 命令行操作如下:

[root@kvm-host ~]# qemu-system-x86_64 -enable-kvm -cpu host -smp 4 -m 8G -drive file=./rhel7.img,format=raw,if=virtio -device virtio-net-pci,netdev=nic0 -netdev bridge,id=nic0,br=virbr0 -snapshot

3)在客户机中, 查看QEMU提供的CPU信息中是否支持AVX和XSAVE. 命令行如下:

[root@kvm-guest ~]# cat /proc/cpuinfo | grep -E "xsave|avx" | uniqflags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology eagerfpu pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat

由上面输出可知, 客户机已经检测到CPU有AVX和XSAVE的指令集支持了, 如果客户机中有需要使用到它们的程序, 就可正常使用, 从而提高程序执行的性能.
由于XSAVES、AVX512都是Skylake以后的CPU才支持的, 所以笔者的环境中没有看到它们.
9.3.3　AES新指令
AES(Advanced Encryption Standard, 高级加密标准)是一种用于对电子数据进行加密的标准, 它在2001年就被美国政府正式接纳和采用. 软件工业界广泛采用AES对个人数据、网络传输数据、公司内部IT基础架构等进行加密保护. AES的区块长度固定为128位, 密钥长度则可以是128位、192位或256位. 随着大家对数据加密越来越重视, 以及AES应用越来越广泛, 并且AES算法用硬件实现的成本并不高, 一些硬件厂商(如Intel、AMD等)都在自己的CPU中直接实现了针对AES算法的一系列指令, 从而提高AES加解密的性能.
AESNI(Advanced Encryption Standard new instructions, AES新指令)是Intel在2008年3月提出的在x86处理器上的指令集扩展. 它包含了7条新指令, 其中6条指令是在硬件上对AES的直接支持, 另外一条是对进位乘法的优化, 从而在执行AES算法的某些复杂的、计算密集型子步骤时, 使程序能更好地利用底层硬件, 减少计算所需的CPU周期, 提升AES加解密的性能. Intel公司从Westmere平台开始就支持AESNI.
目前有不少的软件都已经支持AESNI了, 如OpenSSL(1.0.l版本以上)、Oracle数据库(11.2.0.2版本以上)、7-Zip(9.1版本以上)、Libgcrypt(1.5.0以上)、Linux的Crypto API等. 在KVM虚拟化环境中, 如果客户机支持AESNI(如RHEL6、7的内核都支持AESNI), 而且在客户机中用到AES算法加解密, 那么将AESNI的特性提供给客户机使用, 会提高客户机的性能.
笔者在KVM的客户机中对AESNI进行了测试, 对比在使用AESNI和不使用AESNI的情况下对磁盘进行加解密的速度. 下面介绍一下AESNI的配置和测试过程及测试结果.
1)在进行测试之前, 检查硬件平台是否支持AESNI. 一般来说, 如果CPU支持AESNI, 则会默认暴露到操作系统中去. 而一些BIOS中的CPU configuration下有一个“AESNI Intel”这样的选项, 也需要查看并且确认打开AESNI的支持. 不过, 在设置BIOS时需要注意, 笔者曾经在一台Romley-EP的BIOS中设置了“Advanced”→“Processor Configuration”→“AES-NI Defeature”的选项, 需要看清楚这里是设置“Defeature”而不是“Feature”, 所以这个“AES-NI Defeature”应该设置为“disabled”(其默认值也是“disabled”), 表示打开AESNI功能. 而BIOS中没有AESNI相关的任何设置之时, 就需要到操作系统中加载“aesni_intel”等模块, 来确认硬件是否提供了AESNI的支持.
2)需要保证在内核中将AESNI相关的配置项目编译为模块或直接编译进内核. 当然, 如果不是通过内核来使用AESNI而是直接应用程序的指令使用它, 则该步对内核模块的检查来说不是必要的. RHEL 7.3的内核关于AES的配置如下:

CONFIG_CRYPTO_AES=yCONFIG_CRYPTO_AES_X86_64=yCONFIG_CRYPTO_AES_NI_INTEL=mCONFIG_CRYPTO_CAMELLIA_AESNI_AVX_X86_64=mCONFIG_CRYPTO_CAMELLIA_AESNI_AVX2_X86_64=mCONFIG_CRYPTO_DEV_PADLOCK_AES=m

3)在宿主机中, 查看/proc/cpuinfo中的AESNI相关的特性, 并查看“aseni_intel”这个模块加载情况(如果没有, 需加载). 命令行操作如下:

[root@kvm-host ~]# cat /proc/cpuinfo | grep aes | uniqflags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch ida arat epb pln pts dtherm intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local[root@kvm-host ~]# lsmod | grep aesaesni_intel            69884  0 lrw                    13286  1 aesni_intelglue_helper            13990  1 aesni_intelablk_helper            13597  1 aesni_intelcryptd                 20359  3 ghash_clmulni_intel,aesni_intel,ablk_helper

4)启动KVM客户机, 默认QEMU启动客户机时, 没有向客户机提供AESNI的特性, 可以用“-cpu host”或“-cpu qemu64, +aes”选项来暴露AESNI特性给客户机使用. 当然, 由于前面提及一些最新的CPU系列是支持AESNI的, 所以也可用“-cpu Westmere”“-cpu SandyBridge”这样的参数提供相应的CPU模型, 从而提供对AESNI特性的支持. 注意, 笔者在启动客户机时, 使用了7.4节讲到的numactl技巧, 将客户机绑定在node1上执行, 进一步提高性能. 另外, 笔者给客户机配备了16个vCPU和48G内存, 因为后面测试中要用到20G大小的ramdisk.

[root@kvm-host ~]# numactl --membind=1 --cpunodebind=1 -- qemu-system-x86_64 -enable-kvm -smp 16 -m 48G -drive file=./rhel7.img,format=raw,if=virtio -device virtio-net-pci,netdev=nic0 -netdev bridge,id=nic0,br=virbr0

5)在客户机中可以看到aes标志在/proc/cpuinfo中也是存在的, 然后像宿主机中那样确保“aesni_intel”模块被加载, 再执行使用AESNI测试程序, 即可得出使用了AESNI的测试结果. 当然, 为了衡量AESNI带来的性能提升, 还需要做对比测试, 即不添加“-cpu host”等参数启动客户机, 从而没有AESNI特性的测试结果. 注意, 如果在QEMU启动命令行启动客户机时带了AESNI参数, 一些程序使用AES算法时可能会自动加载“aesni_intel”模块, 所以, 为了确保没有AESNI的支持, 也可以用“rmod aesni_intel”命令移除该模块, 再找到aesni-intel.ko文件并将其删除, 以防被自动加载.
笔者用下面的脚本在Xeon E5-2699 v4(Broadwell)平台上测试了有无AESNI支持对客户机加解密运算性能的影响. 宿主机和客户机内核都是RHEL7.3自带的.
测试原理是: 用cryptsetup命令对ramdisk进行一层加密封装, 然后对封装后的有加密层的磁盘设备进行读写以比较性能. 加密封装我们采用aes-ecb-plain、aes-cbc-plain、aes-ctr-plain、aes-lrw-plain、aes-pcbc-plain、aes-xts-plain这6种AES算法分别测试. 由于cryptsetup会调用Linux内核的Crypto API(前面已提到它是支持AESNI的), 所以测试中会用上AESNI的指令. 测试脚本的内容如下, 笔者是RHEL7的环境, 系统默认mount了tmpfs(一种ramfs)在/run目录. 读者可能需要根据自己的Linux发行版的情况, 自行mount或者修改路径.

#!/bin/bashCRYPT_DEVICE_NAME='crypt0'SYS_RAMDISK_PATH='/run'CIPHER_SET='aes-ecb-plain aes-cbc-plain aes-ctr-plain aes-lrw-plain aes-pcbc-plain aes-xts-plain'# create a crypt device using cryptsetup# $1 -- create crypt device name# $2 -- backend real ramfs device# $3 -- encryption algorithmcreate_dm(){    echo 123456 | cryptsetup open --type=plain $2 $1 -c $3return $?}# remove the device-mapper using cryptsetupremove_dm(){    if [ -b /dev/mapper/$CRYPT_DEVICE_NAME ]; then        cryptsetup close $CRYPT_DEVICE_NAME &> /dev/null    fi}remove_dm# create raw backend ramdiskecho -n "Create raw ramdisk with 20GB size, and its raw writing speed is: "dd if=/dev/zero of=$SYS_RAMDISK_PATH/ramdisk bs=1G count=20if [ $? -ne 0 ]; then    echo "Create raw ramdisk fail, exit"exit 1fiecho "Now start testing..."for cipher in $CIPHER_SETdocreate_dm crypt0 $SYS_RAMDISK_PATH/ramdisk $cipherif [ ! -b /dev/mapper/$CRYPT_DEVICE_NAME ]; then    echo "creating dm with $cipher encryption failed, exit"    rm -fr $SYS_RAMDISK_PATH/ramdisk    exit 1firm -fr $cipher*.txt# do read and write action for several times, so that you can calculate their average value.for i in $(seq 10); do    # the following line is for read action        echo "Start reading test with $cipher, round $i"        dd if=/dev/mapper/$CRYPT_DEVICE_NAME of=/dev/null bs=1G count=20 >> $cipher-read.txt 2>&1    # the following line is for write action        echo "Start write test with $cipher, round $i"        dd of=/dev/mapper/$CRYPT_DEVICE_NAME if=/dev/zero bs=1G count=20 >> "$cipher"-write.txt 2>&1doneremove_dmdonerm -fr $SYS_RAMDISK_PATH/ramdisk

该脚本执行过程中, 会分别以aes-ecb-plain、aes-cbc-plain、aes-ctr-plain、aes-lrw-plain、aes-pcbc-plain、aes-xts-plain加密算法加密ramdisk(路径为/run/ramdisk), 虚拟的加密层硬盘为/dev/mapper/crypt0. 然后对这个虚拟磁盘分别进行10次读写操作, 读写的速度保存在诸如aes-cbc-plain-read.txt、aes-cbc-plain-write.txt等文件中. 最后, 笔者对这10次速度求平均值, 得到图9-5、图9-6. 可以看到, 读速率方面, 有AESNI比没有AESNI快40%～91%, 写速率要快16%～50%.
另外, 在执行脚本中“cryptsetup open”命令时, 有可能会出现如下的错误:

[root@kvm-guest ~]# echo 123456 | cryptsetup create crypt0 /dev/ram0 -c aes-ctr-plaindevice-mapper: reload ioctl on  failed: No such file or directory


图9-5　AESNI在各AES加密算法下读性能优势


图9-6　AESNI在各AES加密算法下写性能优势
这有可能是内核配置的问题, 缺少一些加密相关的模块. 在本示例中, 需要确保内核配置中有如下的配置项:

CONFIG_CRYPTO_CBC=mCONFIG_CRYPTO_CTR=mCONFIG_CRYPTO_CTS=mCONFIG_CRYPTO_ECB=mCONFIG_CRYPTO_LRW=mCONFIG_CRYPTO_PCBC=mCONFIG_CRYPTO_XTS=mCONFIG_CRYPTO_FPU=m

除了笔者这个实验以外, 读者还可以在网上搜索到很多关于AESNI性能优势的文章, 都显示了有了AESNI的硬件辅助, AES加解密获得了显著的性能提升. 比如Intel AES-NI Performance Testing on Linux*/Java*Stack, 显示了AES128和AES256加解密在有AESNI情况下的明显性能优势, 如图9-7、图9-8所示.

图9-7　AES128/256利用AESNI的加密性能提升

图9-8　AES128/256利用AESNI的解密性能提升
本节首先对AESNI新特性进行介绍, 然后对如何让KVM客户机也使用AESNI的步骤进行了介绍. 最后展示了笔者曾经某次对于AESNI在KVM客户机中的实验数据. 相信读者对AESNI有了较好的认识, 在实际应用环境中如果物理硬件平台支持ASENI, 且客户机中用到AES相关的加解密算法, 可以考虑将AESNI特性暴露给客户机使用, 以便提高加解密性能.
9.3.4　完全暴露宿主机CPU特性
在5.2.4节中介绍过, QEMU提供qemu64作为默认的CPU模型. 对于部分CPU特性, 也可以通过“+”号来添加一个或多个CPU特性到一个基础的CPU模型之上, 如前面介绍的AESNI, 可以选用“-cpu qemu64, +aes”参数来让客户机支持AESNI.
当需要客户机尽可能地使用宿主机和物理CPU支持的特性时, QEMU也提供了“-cpu host”参数来尽可能多地暴露宿主机CPU特性给客户机, 从而在客户机中可以看到和使用CPU的各种特性(如果QEMU/KVM同时支持该特性). 在笔者的E5-2699 v4(Broadwell)平台上, 对“-cpu host”参数的效果演示如下.
1)在KVM宿主机中查看CPU信息. 命令行如下:

[root@kvm-host ~]# cat /proc/cpuinfo <!-- 省略其余逻辑CPU信息的输出 -->processor   : 87vendor_id   : GenuineIntelcpu family   : 6model      : 79model name   : Intel(R) Xeon(R) CPU E5-2699 v4 @ 2.20GHzstepping   : 1microcode   : 0xb00001dcpu MHz      : 2029.414cache size   : 56320 KBphysical id   : 1siblings   : 44core id      : 28cpu cores   : 22apicid      : 121initial apicid   : 121fpu      : yesfpu_exception   : yescpuid level   : 20wp      : yesflags      : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch ida arat epb pln pts dtherm intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_localbogomips   : 4403.45clflush size   : 64cache_alignment   : 64address sizes   : 46 bits physical, 48 bits virtualpower management:

2)用“-cpu host”参数启动一个RHEL 7.3客户机系统. 命令行如下:

[root@kvm-host ~]# qemu-system-x86_64 -enable-kvm -cpu host -smp 4 -m 8G -drive file=./rhel7.img,format=raw,if=virtio -device virtio-net-pci,netdev=nic0 -netdev bridge,id=nic0,br=virbr0 -snapshot

3)在客户机中查看CPU信息. 命令行如下:

[root@kvm-guest ~]# cat /proc/cpuinfo processor   : 0vendor_id   : GenuineIntelcpu family   : 6model      : 79model name   : Intel(R) Xeon(R) CPU E5-2699 v4 @ 2.20GHzstepping   : 1microcode   : 0x1cpu MHz      : 2194.916cache size   : 4096 KBphysical id   : 0siblings   : 1core id      : 0cpu cores   : 1apicid      : 0initial apicid   : 0fpu      : yesfpu_exception   : yescpuid level   : 13wp      : yesflags      : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology eagerfpu pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch arat fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveoptbogomips   : 4389.83clflush size   : 64cache_alignment   : 64address sizes   : 40 bits physical, 48 bits virtualpower management:

由上面客户机中CPU信息中可知, 客户机看到的CPU模型与宿主机中一致, 都是“E5-2699 v4@2.20GHz”; CPUID等级信息(cpuid level)也与宿主机一致, 在CPU特性标识(flags)中, 也有了“aes”“xsave”“xsaveopt”“avx”“avx2”“rdrand”“smep”“smap”等特性. 这些较高级的特性是默认的qemu64 CPU模型中没有的, 说明“-cpu host”参数成功地将宿主机的特性尽可能多地提供给客户机使用了.
当然, “-cpu host”参数也并没有完全让客户机得到与宿主机同样多的CPU特性, 这是因为QEMU/KVM对于其中的部分特性没有模拟和实现, 如“EPT”“VPID”等CPU特性目前就不能暴露给客户机使用. 另外, 尽管“-cpu host”参数尽可能多地暴露宿主机CPU特性给客户机, 可以让客户机用上更多的CPU功能, 也能提高客户机的部分性能, 但同时, “-cpu host”参数也给客户机的动态迁移增加了障碍. 例如, 在Intel的SandyBridge平台上, 用“-cpu host”参数让客户机使用了AVX新指令进行运算, 此时试图将客户机迁移到没有AVX支持的Intel Westmere平台上去, 就会导致动态迁移的失败. 所以, “-cpu host”参数尽管向客户机提供了更多的功能和更好的性能, 还是需要根据使用场景谨慎使用.
