
<!-- @import "[TOC]" {cmd="toc" depthFrom=1 depthTo=6 orderedList=false} -->

<!-- code_chunk_output -->

- [1. vCPU的概念](#1-vcpu的概念)
  - [1.1. vCPU的三种模式](#11-vcpu的三种模式)
  - [1.2. KVM下系统架构](#12-kvm下系统架构)
- [2. SMP的支持](#2-smp的支持)
  - [2.1. 硬件支持](#21-硬件支持)
  - [2.2. 操作系统支持](#22-操作系统支持)
  - [2.3. QEMU的支持](#23-qemu的支持)
    - [2.3.1. -smp参数](#231-smp参数)
    - [2.3.2. 默认不加-smp](#232-默认不加-smp)
    - [2.3.3. -smp 8](#233-smp-8)
    - [2.3.4. -smp 8,sockets=2,cores=2,threads=2](#234-smp-8sockets2cores2threads2)
    - [2.3.5. -smp 4,maxcpus=8](#235-smp-4maxcpus8)
- [3. CPU过载使用](#3-cpu过载使用)
- [4. CPU模型](#4-cpu模型)
  - [4.1. -cpu ?查看所有模型](#41-cpu-查看所有模型)
  - [4.2. 不加-cpu默认CPU模型](#42-不加-cpu默认cpu模型)
  - [4.3. -cpu model指定模型](#43-cpu-model指定模型)
- [5. 进程的处理器亲和性和vCPU的绑定](#5-进程的处理器亲和性和vcpu的绑定)
  - [5.1. 启动隔离](#51-启动隔离)
    - [5.1.1. kernel启动参数](#511-kernel启动参数)
    - [5.1.2. 通过cpu上进程判断是否隔离](#512-通过cpu上进程判断是否隔离)
  - [5.2. 启动虚拟机并绑定vCPU](#52-启动虚拟机并绑定vcpu)

<!-- /code_chunk_output -->

# 1. vCPU的概念

**QEMU/KVM**为客户机提供一套完整的**硬件系统环境**，在**客户机**看来，其所拥有的CPU即是**vCPU(virtual CPU**). 

在KVM环境中，**每个客户机**都是一个**标准的Linux进程(QEMU进程**)，而**每一个vCPU**在宿主机中是**QEMU进程派生**的一个**普通线程**. 

## 1.1. vCPU的三种模式

在**普通的Linux系统**中，进程一般有**两种执行模式**: **内核模式**和**用户模式**. 

而在**KVM环境**中，增加了第3种模式: **客户模式**. 

vCPU在3种执行模式下的不同分工如下. 

(1)用户模式(User Mode)

主要处理**I/O的模拟和管理**，由**QEMU**的代码实现. 

(2)内核模式(Kernel Mode)

主要处理特别需要高性能和安全相关的指令，如处理客户模式到内核模式的转换，处理客户模式下的I/O指令或其他特权指令引起的退出(VM\-Exit)，处理影子内存管理(shadow MMU). 

(3)客户模式(Guest Mode)

主要执行Guest中的大部分指令，I/O和一些特权指令除外(它们会引起VM\-Exit，被Hypervisor截获并模拟). 

vCPU在KVM中的这3种执行模式下的转换如图5\-1所示. 

![](./images/2019-05-19-20-06-17.png)

## 1.2. KVM下系统架构

在KVM环境中，整个系统的基本分层架构如图5\-2所示. 

![](./images/2019-05-19-20-10-10.png)

在系统的底层CPU硬件中需要有硬件辅助虚拟化技术的支持(Intel VT或AMD\-V等)，宿主机就运行在硬件之上，**KVM的内核部分**是作为**可动态加载内核模块**运行在宿主机中的，其中

- 一个模块是与**硬件平台无关**的实现虚拟化核心基础架构的**kvm模块**，
- 另一个是**硬件平台相关**的kvm\_intel(或kvm\_amd)模块. 

而KVM中的**一个客户机**是作为一个**用户空间进程(qemu**)运行的，它和其他普通的用户空间进程(如gnome、kde、firefox、chrome等)一样由内核来调度，使其运行在物理CPU上，不过它由kvm模块的控制，可以在前面介绍的**3种执行模式**下运行. 

**多个客户机**就是宿主机中的**多个QEMU进程**，而**一个客户机**的**多个vCPU**就是**一个QEMU进程**中的**多个线程**. 

和普通操作系统一样，在**客户机系统**中，同样分别运行着客户机的内核和客户机的用户空间应用程序. 

# 2. SMP的支持

在**SMP系统**中，**多个程序(进程**)可以真正做到**并行执行**，而且单个进程的多个线程也可以得到并行执行，这极大地提高了计算机系统并行处理能力和整体性能. 

## 2.1. 硬件支持

在**硬件**方面，**早期**的计算机系统更多的是在**一个主板**上拥有**多个物理的CPU插槽**，来实现SMP系统. 后来，随着**多核技术**、**超线程(Hyper\-Threading**)技术的出现，SMP系统使用**多处理器**、**多核**、**超线程**等技术中的**一个或多个**. 

## 2.2. 操作系统支持

在**操作系统软件**方面，多数的现代操作系统都提供了**对SMP系统的支持**. 

例如，在Linux中，下面的Bash脚本(cpu\-info.sh)可以根据/proc/cpuinfo文件来检查当前系统中的CPU数量、多核及超线程的使用情况. 

```sh
#!/bin/bash
#filename: cpu-info.sh
#this script only works in a Linux system which has one or more identical physical CPU(s).

echo -n "logical CPU number in total: "
#逻辑CPU个数
cat /proc/cpuinfo | grep "processor" | wc -l

#有些系统没有多核也没有打开超线程，就直接退出脚本
cat /proc/cpuinfo | grep -qi "core id"
if [ $? -ne 0 ]; then
    echo "Warning. No multi-core or hyper-threading is enabled."
    exit 0;
fi

echo -n "physical CPU number in total: "
#物理CPU个数
cat /proc/cpuinfo | grep "physical id" | sort | uniq | wc -l

echo -n "core number in a physical CPU: "
#每个物理CPU上core的个数(未计入超线程)
core_per_phy_cpu=$(cat /proc/cpuinfo | grep "core id" | sort | uniq | wc -l)
echo $core_per_phy_cpu

echo -n "logical CPU number in a physical CPU: "
#每个物理CPU中逻辑CPU(可能是core、threads或both)的个数
logical_cpu_per_phy_cpu=$(cat /proc/cpuinfo | grep "siblings" | sort | uniq | awk- F: '{print $2}')
echo $logical_cpu_per_phy_cpu

#是否打开超线程，以及每个core上的超线程数目
#如果在同一个物理CPU上的两个逻辑CPU具有相同的”core id”，那么超线程是打开的
#此处根据前面计算的core_per_phy_cpu和logical_core_per_phy_cpu的比较来查看超线程
if [ $logical_cpu_per_phy_cpu -gt $core_per_phy_cpu ]; then
    echo "Hyper threading is enabled. Each core has $(expr $logical_cpu_per_phy_cpu / $core_per_phy_cpu ) threads."
elif [ $logical_cpu_per_phy_cpu -eq $core_per_phy_cpu ]; then
    echo "Hyper threading is NOT enabled."
else
    echo "Error. There's something wrong."
fi
```

## 2.3. QEMU的支持

SMP是如此的普及和被广泛使用，而**QEMU**在给**客户机模拟CPU**时，也可以提供**对SMP架构的模拟**，让客户机运行在SMP系统中，充分利用物理硬件的SMP并行处理优势. 

由于**每个vCPU**在**宿主机**中都是**一个线程**，并且**宿主机Linux系统**是支持**多任务处理**的，因此可以通过**两种操作来实现客户机的SMP**，

- 一是将**不同的vCPU的进程交换执行**(分时调度，即使物理硬件非SMP，也可以为客户机模拟出SMP系统环境)，
- 二是将在**物理SMP硬件系统**上**同时执行多个vCPU的进程**. 

### 2.3.1. -smp参数

在qemu命令行中，“\-**smp**”参数即是配置客户机的SMP系统，其具体参数如下: 

```
-smp [cpus=]n[,maxcpus=cpus][,cores=cores][,threads=threads][,sockets=sockets]
```

其中: 

- n用于设置客户机中使用的**逻辑CPU数量**(**默认值是1**). 
- maxcpus用于设置客户机中**最大可能被使用的CPU数量**，包括启动时处于下线(**offline**)状态的**CPU数量**(可用于**热插拔hot\-plug**加入CPU，但**不能超过maxcpus**这个上限). 
- cores用于设置**每个CPU的core**数量(默认值是1). 
- threads用于设置**每个core上的线程数**(默认值是1). 
- sockets用于设置**客户机中看到的总的CPU socket数量**. 

下面通过KVM中的几个qemu命令行示例，来看一下如何将SMP应用于客户机中. 

### 2.3.2. 默认不加-smp

示例1: 

```
qemu-system-x86_64 -m 1G rhel7.img
```

**不加smp参数**，使用其**默认值1！！！**，在**客户机**中查看CPU情况，如下: 

```
[root@kvm-guest ~]# cat /proc/cpuinfo
processor       : 0
vendor_id       : AuthenticAMD
cpu family      : 6
model           : 6
model name      : QEMU Virtual CPU version 2.5+
stepping        : 3
cpu MHz         : 3591.617
cache size      : 512 KB
physical id     : 0
siblings        : 1
core id        : 0
cpu cores        : 1
apciid        : 0
initial apicid  : 0
fpu             : yes
fpu_exception   : yes
cpuid level     : 13
wp              : yes
flags           : fpu de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36
                  clflush mmx fxsr sse sse2 syscall nx lm nopl pni cx16 hypervisor
                  lahf_lm svm
bogomips        : 7183.23
TLB size        : 1024 4K pages
clflush size     : 64
cache_alignment : 64
address sizes   : 40 bits physical, 48 bits virtual
power management:
[root@kvm-guest ~]# ls /sys/devices/system/cpu/
cpu0 isolated modalias offline possible present
cpuidle kernel_max nohz_full online power uevent
```

由上面的输出信息可知，客户机系统识别到**1个QEMU模拟的CPU(cpu0**)，并且在**QEMU monitor**(**默认**按**Alt+Ctrl+2**组合键切换到monitor)中用“info cpus”命令可以看到客户机中的CPU数量及其对应QEMU线程的ID，如下所示: 

```
(qemu) info cpus
* CPU #0: pc=0xffffffff81058e96 (halted) thread_id=10747
```

在**宿主机**中看到相应的**QEMU进程和线程**如下: 

```
[root@kvm-host ~]# ps -eLf | grep qemu
root   10744  9335 10744  0  4 15:30 pts/0  00:00:11 qemu-system-x86_64 rhel7.img -m 1G
root   10744  9335 10745  0  4 15:30 pts/0  00:00:00 qemu-system-x86_64 rhel7.img -m 1G
root   10744  9335 10747 32  4 15:30 pts/0  00:07:51 qemu-system-x86_64 rhel7.img -m 1G
root   10744  9335 10748  0  4 15:30 pts/0  00:00:01 qemu-system-x86_64 rhel7.img -m 1G
root   11394  9766 11394  0  1 15:54 pts/2  00:00:00 grep --color=auto qemu
```

由以上信息可以看出，PID(Process ID)为10744的进程是客户机的进程，它产生了1个线程(线程ID为10747)作为客户机的vCPU运行在宿主机中. 其中，**ps命令**的\-**L参数**指定打印出**线程的ID和线程的个数**，\-**e参数**指定选择**所有的进程**，\-**f参数**指定选择**打印出完全的各列**. 

### 2.3.3. -smp 8

示例2: 

```
qemu-system-x86_64 -smp 8 -m 8G rhel7.img
```

“\-smp 8”表示分配了**8个虚拟的CPU给客户机**，在客户机中用前面提到的“cpu\-info.sh”脚本查看CPU情况，如下: 

```
[root@kvm-guest ~]# cat /proc/cpuinfo
<! --  此处省略/proc/cpuinfo的输出细节 -->
[root@kvm-guest ~]# ./cpu-info.sh        #这个是前面提到的Bash脚本
logical CPU number in total: 8
physical CPU number in total: 8
core number in a physical CPU: 1
logical CPU number in a physical CPU: 1
Hyper threading is NOT enabled.
[root@kvm-guest ~]# ls /sys/devices/system/cpu/
cpu0 cpu1 cpu2 cpu3 cpu4 cpu5 cpu6 cpu7 cpuidle isolated kernel_max modalias nohz_full offline online possible power present uevent
```

由上面的输出信息可知，客户机使用了8个CPU(cpu0\~cpu7)，系统识别的CPU数量也总共是8个，是**8个物理CPU**，而**没有多核、超线程之类的架构！！！**. 

在QEMU monitor中查询CPU状态，如下: 

```
QEMU 2.7.0 monitor - type 'help' for more information
(qemu) VNC server running on ::1:5900
(qemu) info cpus
* CPU #0: pc=0xffffffff81060eb6 (halted) thread_id=186374
  CPU #1: pc=0xffffffff81060eb6 (halted) thread_id=186375
  CPU #2: pc=0xffffffff81060eb6 (halted) thread_id=186376
  CPU #3: pc=0xffffffff81060eb6 (halted) thread_id=186377
  CPU #4: pc=0xffffffff81060eb6 (halted) thread_id=186378
  CPU #5: pc=0xffffffff81060eb6 (halted) thread_id=186379
  CPU #6: pc=0xffffffff81060eb6 (halted) thread_id=186380
  CPU #7: pc=0xffffffff81060eb6 (halted) thread_id=186381
```

在宿主机中看到相应的QEMU进程和线程如下: 

```
[root@kvm-host ~]# ps -eLf | grep qemu
root     186360  96470 186360  0   12 21:43 pts/4    00:00:01 qemu-system-x86_64 ia32e_rhel7u3_kvm.img -smp 8 -m 8G
root     186360  96470 186361  0   12 21:43 pts/4    00:00:00 qemu-system-x86_64 ia32e_rhel7u3_kvm.img -smp 8 -m 8G
root     186360  96470 186374  7   12 21:43 pts/4    00:00:15 qemu-system-x86_64 ia32e_rhel7u3_kvm.img -smp 8 -m 8G
root     186360  96470 186375  1   12 21:43 pts/4    00:00:03 qemu-system-x86_64 ia32e_rhel7u3_kvm.img -smp 8 -m 8G
root     186360  96470 186376  1   12 21:43 pts/4    00:00:03 qemu-system-x86_64 ia32e_rhel7u3_kvm.img -smp 8 -m 8G
root     186360  96470 186377  1   12 21:43 pts/4    00:00:03 qemu-system-x86_64 ia32e_rhel7u3_kvm.img -smp 8 -m 8G
root     186360  96470 186378  1   12 21:43 pts/4    00:00:03 qemu-system-x86_64 ia32e_rhel7u3_kvm.img -smp 8 -m 8G
root     186360  96470 186379  1   12 21:43 pts/4    00:00:03 qemu-system-x86_64 ia32e_rhel7u3_kvm.img -smp 8 -m 8G
root     186360  96470 186380  1   12 21:43 pts/4    00:00:04 qemu-system-x86_64 ia32e_rhel7u3_kvm.img -smp 8 -m 8G
root     186360  96470 186381  1   12 21:43 pts/4    00:00:03 qemu-system-x86_64 ia32e_rhel7u3_kvm.img -smp 8 -m 8G
root     186360  96470 186383  0   12 21:43 pts/4    00:00:01 qemu-system-x86_64 ia32e_rhel7u3_kvm.img -smp 8 -m 8G 
root     186360  96470 186492  0   12 21:46 pts/4    00:00:00 qemu-system-x86_64 ia32e_rhel7u3_kvm.img -smp 8 -m 8G 
root     186519 178215 186519  0    1 21:47 pts/6    00:00:00 grep --color=auto qemu
```

由以上信息可知，PID为186360的进程是QEMU启动客户机的进程，它产生了**8个线程**作为**客户机的8个vCPU**在运行. 

### 2.3.4. -smp 8,sockets=2,cores=2,threads=2

示例3: 

```
qemu-system-x86_64 -m 8G -smp 8,sockets=2,cores=2,threads=2 rhel7.img
```

通过\-smp参数详细定义了客户机中SMP的架构，在客户中得到的CPU信息如下: 

```
[root@kvm-guest ~]# cat /proc/cpuinfo
<! --  此处省略/proc/cpuinfo的输出细节 -->
[root@kvm-guest ~]# sh cpu-info.sh 
logical CPU number in total: 8
physical CPU number in total: 2
core number in a physical CPU: 2
logical CPU number in a physical CPU: 4
Hyper threading is enabled. Each core has 2 threads.
[root@kvm-guest ~]# ls /sys/devices/system/cpu/
cpu0  cpu1  cpu2  cpu3  cpu4  cpu5  cpu6  cpu7  cpuidle  isolated  kernel_max  microcode  modalias  nohz_full  offline  online  possible  power  present  uevent
```

从上面的输出信息可知，客户机中共有8个逻辑CPU(cpu0\~cpu7)，2个CPU socket，每个socket有2个核，每个核有2个线程(超线程处于打开状态). 

### 2.3.5. -smp 4,maxcpus=8

示例4: 

```
qemu-system-x86_64 -m 8G -smp 4,maxcpus=8 rhel7.img -enable-kvm
```

通过\-smp参数详细定义了客户机中**最多有8个CPU**可用，在系统启动之时有**4个处于开启状态**，在客户中得到的CPU信息如下: 

```
[root@kvm-guest ~]# sh cpu-info.sh 
logical CPU number in total: 4
physical CPU number in total: 4
core number in a physical CPU: 1
logical CPU number in a physical CPU: 1
Hyper threading is NOT enabled.

[root@kvm-guest ~]# lscpu
Architecture:          x86_64
CPU op-mode(s):        32-bit, 64-bit
Byte Order:            Little Endian
CPU(s):                4
On-line CPU(s) list:   0-3
Thread(s) per core:    1
Core(s) per socket:    1
Socket(s):             4
NUMA node(s):          1
Vendor ID:             GenuineIntel
CPU family:            6
Model:                 6
Model name:            QEMU Virtual CPU version 2.5+
Stepping:              3
CPU MHz:               2194.980
BogoMIPS:              4389.69
Hypervisor vendor:     KVM
Virtualization type:   full
L1d cache:             32K
L1i cache:             32K
L2 cache:              4096K
NUMA node0 CPU(s):     0-3
```

可以看到，**客户机**启动之后有**4个CPU**，分别位于**4个socket**的**单一core**上. 

下面我们来做**热插拔操作**，切换到QEMU monitor执行“cpu\-add\<id\>”的指令. 

```
(qemu) info cpus
* CPU #0: pc=0xffffffff81060eb6 (halted) thread_id=190609
  CPU #1: pc=0xffffffff81060eb6 (halted) thread_id=190610
  CPU #2: pc=0xffffffff81060eb6 (halted) thread_id=190611
  CPU #3: pc=0xffffffff81060eb6 (halted) thread_id=190612
(qemu) cpu-add 7
(qemu) info cpus
* CPU #0: pc=0xffffffff81060eb6 (halted) thread_id=190609
  CPU #1: pc=0xffffffff81060eb6 (halted) thread_id=190610
  CPU #2: pc=0xffffffff81060eb6 (halted) thread_id=190611
  CPU #3: pc=0xffffffff81060eb6 (halted) thread_id=190612
  CPU #7: pc=0xffffffff81060eb6 (halted) thread_id=190690
```

我们**热插入7号CPU**(0\~7中最后一个)，可以在**QEMU monitor**中看到\#**7号CPU上线！！！**了. 

再在**客户机里面检查**: 

```
[root@kvm-guest ~]# lscpu
Architecture:          x86_64
CPU op-mode(s):        32-bit, 64-bit
Byte Order:            Little Endian
CPU(s):                5
On-line CPU(s) list:   0-4
Thread(s) per core:    1
Core(s) per socket:    1
Socket(s):             5
NUMA node(s):          1
Vendor ID:             GenuineIntel
CPU family:            6
Model:                 6
Model name:            QEMU Virtual CPU version 2.5+
Stepping:              3
CPU MHz:               2194.980
BogoMIPS:              4389.69
Hypervisor vendor:     KVM
Virtualization type:   full
L1d cache:             32K
L1i cache:             32K
L2 cache:              4096K
NUMA node0 CPU(s):     0-4
[root@kvm-guest ~]# sh cpu-info.sh 
logical CPU number in total: 5
physical CPU number in total: 5
core number in a physical CPU: 1
logical CPU number in a physical CPU: 1
Hyper threading is NOT enabled. 
[root@kvm-guest ~]# ls /sys/devices/system/cpu/
cpu0  cpu1  cpu2  cpu3  cpu4  cpuidle  isolated  kernel_max  microcode  modalias  nohz_full  offline  online  possible  power  present  uevent
```

可以看到客户机变成有**5个CPU**了. 但是可以看到，**新增的CPU编号**却是**连续的(cpu4！！！**)，尽管我们**热插入的是\#7号CPU**. 

# 3. CPU过载使用

KVM允许客户机过载使用(over\-commit)物理资源，即允许为**客户机分配的CPU**和**内存数量**多于物理上**实际存在的资源**. 

**物理资源的过载使用**能带来资源充分利用方面的好处. 试想在一台强大的硬件服务器中运行Web服务器、图片存储服务器、后台数据统计服务器等作为虚拟客户机，但是它们不会在同一时刻都负载很高，如Web服务器和图片服务器在**白天工作时间负载较重**，而**后台数据统计服务器**主要在**晚上工作**，所以对物理资源进行合理的过载使用，给这几个客户机分配的系统资源总数多余实际拥有的物理资源，就可能在白天和夜晚都充分利用物理硬件资源，而且由于**几个客户机不会同时对物理资源！！！** 造成很大的压力，它们各自的服务质量(QoS)也能得到保障. 

**CPU的过载使用**是让**一个或多个客户机**使用**vCPU的总数量**超过实际拥有的**物理CPU数量**. 

QEMU会启动**更多的线程**来为客户机提供服务，这些线程也被Linux内核调度运行在物理CPU硬件上. 

关于CPU的过载使用，推荐的做法是对**多个单CPU的客户机**使用**over\-commit**，比如，在拥有**4个逻辑CPU的宿主机**中，同时运行多于4个(如8个、16个)客户机，其中每个客户机都分配一个vCPU. 这时，如果每个宿主机的负载不是很大，宿主机Linux对每个客户机的调度是非常有效的，这样的过载使用并不会带来客户机的性能损失. 

关于CPU的过载使用，**最不推荐**的做法是让**某一个客户机的vCPU数量**超过**物理系统上存在的CPU数量**. 比如，在拥有4个逻辑CPU的宿主机中，同时运行一个或多个客户机，其中每个客户机的vCPU数量多于4个(如16个). 这样的使用方法会带来比较明显的性能下降，其性能反而不如为客户机分配2个(或4个)vCPU的情况. 而且如果客户机负载过重，可能会让整个系统运行不稳定. 不过，在并非100%满负载的情况下，一个(或多个)有4个vCPU的客户机运行在拥有4个逻辑CPU的宿主机中并不会带来明显的性能损失. 

总的来说，KVM允许CPU的过载使用，但是并**不推荐在实际的生产环境**(特别是负载较重的环境)中过载使用CPU. 在生产环境中过载使用CPU，有必要在部署前进行严格的性能和稳定性测试. 

# 4. CPU模型

**每一种虚拟机管理程序**(Virtual Machine Monitor，简称**VMM或Hypervisor**)都会定义自己的策略，让客户机看起来有一个**默认的CPU类型**. 

**有的Hypervisor**会简单地将**宿主机中CPU的类型和特性直接传递给客户机使用**，而**QEMU/KVM**在**默认情况**下会向客户机提供一个名为**qemu64或qemu32**的**基本CPU模型**. 

QEMU/KVM的这种策略会带来一些好处，如可以**对CPU特性**提供一些**高级的过滤功能**，还可以将**物理平台**根据提供的**基本CPU模型**进行**分组**(如将几台IvyBridge和Sandybridge硬件平台分为一组，都提供相互兼容的SandyBridge或qemu64的CPU模型)，从而使客户机在同一组硬件平台上的动态迁移更加平滑和安全. 

## 4.1. -cpu ?查看所有模型

通过如下的命令行可以查看**当前的QEMU**支持的**所有CPU模型**. 

```
[root@localhost ~]# qemu-system-x86_64 -enable-kvm -cpu ?
Available CPUs:
x86 486
x86 Broadwell             Intel Core Processor (Broadwell)
x86 Broadwell-IBRS        Intel Core Processor (Broadwell, IBRS)
x86 Broadwell-noTSX       Intel Core Processor (Broadwell, no TSX)
x86 Broadwell-noTSX-IBRS  Intel Core Processor (Broadwell, no TSX, IBRS)
x86 Cascadelake-Server    Intel Xeon Processor (Cascadelake)
x86 Conroe                Intel Celeron_4x0 (Conroe/Merom Class Core 2)
x86 Dhyana                Hygon Dhyana Processor
x86 EPYC                  AMD EPYC Processor
x86 EPYC-IBPB             AMD EPYC Processor (with IBPB)
x86 Haswell               Intel Core Processor (Haswell)
x86 Haswell-IBRS          Intel Core Processor (Haswell, IBRS)
x86 Haswell-noTSX         Intel Core Processor (Haswell, no TSX)
x86 Haswell-noTSX-IBRS    Intel Core Processor (Haswell, no TSX, IBRS)
x86 Icelake-Client        Intel Core Processor (Icelake)
x86 Icelake-Server        Intel Xeon Processor (Icelake)
x86 IvyBridge             Intel Xeon E3-12xx v2 (Ivy Bridge)
x86 IvyBridge-IBRS        Intel Xeon E3-12xx v2 (Ivy Bridge, IBRS)
x86 KnightsMill           Intel Xeon Phi Processor (Knights Mill)
x86 Nehalem               Intel Core i7 9xx (Nehalem Class Core i7)
x86 Nehalem-IBRS          Intel Core i7 9xx (Nehalem Core i7, IBRS update)
x86 Opteron_G1            AMD Opteron 240 (Gen 1 Class Opteron)
x86 Opteron_G2            AMD Opteron 22xx (Gen 2 Class Opteron)
x86 Opteron_G3            AMD Opteron 23xx (Gen 3 Class Opteron)
x86 Opteron_G4            AMD Opteron 62xx class CPU
x86 Opteron_G5            AMD Opteron 63xx class CPU
x86 Penryn                Intel Core 2 Duo P9xxx (Penryn Class Core 2)
x86 SandyBridge           Intel Xeon E312xx (Sandy Bridge)
x86 SandyBridge-IBRS      Intel Xeon E312xx (Sandy Bridge, IBRS update)
x86 Skylake-Client        Intel Core Processor (Skylake)
x86 Skylake-Client-IBRS   Intel Core Processor (Skylake, IBRS)
x86 Skylake-Server        Intel Xeon Processor (Skylake)
x86 Skylake-Server-IBRS   Intel Xeon Processor (Skylake, IBRS)
x86 Westmere              Westmere E56xx/L56xx/X56xx (Nehalem-C)
x86 Westmere-IBRS         Westmere E56xx/L56xx/X56xx (IBRS update)
x86 athlon                QEMU Virtual CPU version 2.5+
x86 core2duo              Intel(R) Core(TM)2 Duo CPU     T7700  @ 2.40GHz
x86 coreduo               Genuine Intel(R) CPU           T2600  @ 2.16GHz
x86 kvm32                 Common 32-bit KVM processor
x86 kvm64                 Common KVM processor
x86 n270                  Intel(R) Atom(TM) CPU N270   @ 1.60GHz
x86 pentium
x86 pentium2
x86 pentium3
x86 phenom                AMD Phenom(tm) 9550 Quad-Core Processor
x86 qemu32                QEMU Virtual CPU version 2.5+
x86 qemu64                QEMU Virtual CPU version 2.5+
x86 base                  base CPU model type with no features enabled
x86 host                  KVM processor with all supported host features
x86 max                   Enables all features supported by the accelerator in the current host
......
```

这些CPU模型是在源代码**qemu.git/target/i386/cpu.c**中的结构体数组**builtin\_x86\_defs**\[\]中定义的. 

## 4.2. 不加-cpu默认CPU模型

在x86\-64平台上编译和运行的QEMU，在**不加“\-cpu**”参数启动时，采用“**qemu64**”作为**默认的CPU模型**，命令行演示如下: 

```
qemu-system-x86_64 rhel7.img
```

在客户机看到的CPU信息如下:

```
[root@kvm-guest ~]# cat /proc/cpuinfo
processor       : 0
vendor_id       : AuthenticAMD
cpu family      : 6
model           : 6
model name      : QEMU Virtual CPU version 2.5+
stepping        : 3
cpu MHz         : 3591.617
cache size      : 512 KB
physical id     : 0
siblings        : 1
core id         : 0
cpu cores       : 1
apciid          : 0
initial apicid  : 0
fpu             : yes
fpu_exception   : yes
cpuid level     : 13
wp              : yes
flags           : fpu de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36
                   clflush mmx fxsr sse sse2 syscall nx lm nopl pni cx16 hypervisor
                   lahf_lm svm
bogomips         : 7183.23
TLB size        : 1024 4K pages
clflush size     : 64
cache_alignment : 64
address sizes   : 40 bits physical, 48 bits virtual
power management:
```

可知，在**客户机**中看到的**CPU模型**的名称为“**QEMU Virtual CPU version 2.5\+**”，这就是**当前版本QEMU**的“**qemu64**”CPU模型的**名称**. 

## 4.3. -cpu model指定模型

在qemu命令行中，可以用“\-**cpu cpu\_model**”来指定在客户机中的CPU模型. 如下的命令行就在启动客户机时指定了CPU模型为Broadwell. 

```
qemu-system-x86_64 rhel7.img -cpu Broadwell -smp 8 -m 16G -enable-kvm
```

在客户机中查看CPU信息如下: 
```
[root@kvm-guest ~]# cat /proc/cpuinfo 
processor      : 0
vendor_id      : GenuineIntel
cpu family      : 6
model      : 61
model name      : Intel Core Processor (Broadwell)
stepping      : 2
microcode      : 0x1
cpu MHz      : 2194.655
cache size      : 4096 KB
physical id   : 0
siblings      : 1
core id      : 0
cpu cores      : 1
apicid      : 0
initial apicid   : 0
fpu         : yes
fpu_exception   : yes
cpuid level   : 13
wp         : yes
flags      : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca
 cmov pat pse36 clflush mmx fxsr sse sse2 syscall nx rdtscp lm
 constant_tsc rep_good nopl xtopology eagerfpu pni pclmulqdq 
 ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_ 
 deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm 
 abm 3dnowprefetch arat fsgsbase bmi1 hle avx2 smep bmi2 erms
 invpcid rtm rdseed adx smap xsaveopt
bogomips      : 4389.69
clflush size   : 64
cache_alignment   : 64
address sizes   : 40 bits physical, 48 bits virtual
power management:
```

由上面的信息可知，在客户机中看到的CPU是基于Broadwell的Intel Xeon处理器. 

其中的vendor\_id、cpu family、flags、cpuid level等都是在target/i386/cpu.c的buildin\_x86\_defs\[\]中定义好的，具体的关于Broadwell这个CPU模型的定义如下: 

```dict
{
    .name = "Broadwell",
    .level = 0xd,
    .vendor = CPUID_VENDOR_INTEL,
    .family = 6,
    .model = 61,
    .stepping = 2,
    .features[FEAT_1_EDX] =
        CPUID_VME | CPUID_SSE2 | CPUID_SSE | CPUID_FXSR | CPUID_MMX |
        CPUID_CLFLUSH | CPUID_PSE36 | CPUID_PAT | CPUID_CMOV | CPUID_MCA |
        CPUID_PGE | CPUID_MTRR | CPUID_SEP | CPUID_APIC | CPUID_CX8 |
        CPUID_MCE | CPUID_PAE | CPUID_MSR | CPUID_TSC | CPUID_PSE |
        CPUID_DE | CPUID_FP87,
    .features[FEAT_1_ECX] =
        CPUID_EXT_AVX | CPUID_EXT_XSAVE | CPUID_EXT_AES |
        CPUID_EXT_POPCNT | CPUID_EXT_X2APIC | CPUID_EXT_SSE42 |
        CPUID_EXT_SSE41 | CPUID_EXT_CX16 | CPUID_EXT_SSSE3 |
        CPUID_EXT_PCLMULQDQ | CPUID_EXT_SSE3 |
        CPUID_EXT_TSC_DEADLINE_TIMER | CPUID_EXT_FMA | CPUID_EXT_MOVBE |
        CPUID_EXT_PCID | CPUID_EXT_F16C | CPUID_EXT_RDRAND,
    .features[FEAT_8000_0001_EDX] =
        CPUID_EXT2_LM | CPUID_EXT2_RDTSCP | CPUID_EXT2_NX |
        CPUID_EXT2_SYSCALL,
    .features[FEAT_8000_0001_ECX] =
        CPUID_EXT3_ABM | CPUID_EXT3_LAHF_LM | CPUID_EXT3_3DNOWPREFETCH,
    .features[FEAT_7_0_EBX] =
        CPUID_7_0_EBX_FSGSBASE | CPUID_7_0_EBX_BMI1 |
        CPUID_7_0_EBX_HLE | CPUID_7_0_EBX_AVX2 | CPUID_7_0_EBX_SMEP |
        CPUID_7_0_EBX_BMI2 | CPUID_7_0_EBX_ERMS | CPUID_7_0_EBX_INVPCID |
        CPUID_7_0_EBX_RTM | CPUID_7_0_EBX_RDSEED | CPUID_7_0_EBX_ADX |
        CPUID_7_0_EBX_SMAP,
    .features[FEAT_XSAVE] =
        CPUID_XSAVE_XSAVEOPT,
    .features[FEAT_6_EAX] =
        CPUID_6_EAX_ARAT,
    .xlevel = 0x80000008,
    .model_id = "Intel Core Processor (Broadwell)",
},
```

# 5. 进程的处理器亲和性和vCPU的绑定

通常在SMP系统中，Linux内核的进程调度器根据自有的调度策略将系统中的**一个进程**调度到**某个CPU**上执行. 在**默认情况**下，**一个普通进程或线程**的处理器亲和性体现在**所有可用的CPU**上，进程或线程有可能在这些CPU之中的任何一个(包括超线程)上执行. 

进程的处理器亲和性(Processor Affinity)即**CPU的绑定设置**，是指**将进程绑定到特定的一个或多个CPU**上去执行，而不允许将进程调度到其他的CPU上. Linux内核对进程的调度算法也是遵守进程的处理器亲和性设置的. 

设置**进程的处理器亲和性**带来的**好处**是可以减少进程在多个CPU之间交换运行带来的**缓存命中失效(cache missing**)，从该进程运行的角度来看，可能带来一定程度上的性能提升. 换个角度来看，对进程亲和性的设置也可能带来一定的问题，如破坏了原有SMP系统中**各个CPU的负载均衡**(load balance)，这可能会导致整个系统的进程调度变得低效. 对进程的处理器亲和性进行设置可能导致系统的整体性能的下降而非提升. 

**每个vCPU**都是宿主机中一个**普通的QEMU线程**，可以使用**taskset工具**对其设置处理器亲和性，使其绑定到**某一个或几个固定的CPU**上去调度. 

下面举一个云计算应用中需要绑定vCPU的示例. 

作为IaaS(Infrastructure as a Service)类型的云计算提供商的A公司(如Amazon、Google、Azure、阿里云等)，为客户提供一个有**两个逻辑CPU计算能力**的一个客户机. 要求**CPU资源独立被占用**，不受宿主机中其他客户机的负载水平的影响. 为了满足这个需求，可以分如下**两个步骤**来实现. 

## 5.1. 启动隔离

启动宿主机时**隔离出两个逻辑CPU**专门供一个客户机使用. 

### 5.1.1. kernel启动参数

在Linux内核**启动的命令**行中加上“**isolcpus**=”参数，可以实现CPU的隔离，使得在系统启动后普通进程默认都不会被调度到被隔离的CPU上执行. 例如，隔离cpu4和cpu5的内核启动命令行如下: 

```
[root@kvm-host ~]# cat /proc/cmdline 
BOOT_IMAGE=/vmlinuz-3.10.0-514.el7.x86_64 root=/dev/mapper/rhel-root ro crashkernel=auto rd.lvm.lv=rhel/root rd.lvm.lv=rhel/swap rhgb quiet LANG=en_US.UTF-8 intel_iommu=on isolcpus=4,5
```

### 5.1.2. 通过cpu上进程判断是否隔离

在系统启动后，在宿主机中检查是否隔离成功的命令行如下: 

```
[root@kvm-host ~]# ps -eLo psr | grep -e "^[[:blank:]]*0$" | wc -l
23
[root@kvm-host ~]# ps -eLo psr | grep -e "^[[:blank:]]*1$" | wc -l
10
[root@kvm-host ~]# ps -eLo psr | grep -e "^[[:blank:]]*2$" | wc -l
28
[root@kvm-host ~]# ps -eLo psr | grep -e "^[[:blank:]]*3$" | wc -l
23
[root@kvm-host ~]# ps -eLo psr | grep -e "^[[:blank:]]*4$" | wc -l
5
[root@kvm-host ~]# ps -eLo psr | grep -e "^[[:blank:]]*5$" | wc -l
6
[root@kvm-host ~]# ps -eLo ruser,pid,ppid,lwp,psr,args | awk '{if($5==5) print $0}'
root         32      2     32   5 [watchdog/5]
root         33      2     33   5 [migration/5]
root         34      2     34   5 [ksoftirqd/5]
root         35      2     35   5 [kworker/5:0]
root         36      2     36   5 [kworker/5:0H]
root       5439      2   5439   5 [kworker/5:1]
[root@kvm-host ~]# ps -eLo ruser,pid,ppid,lwp,psr,args | awk '{if($5==4) print $0}'
root         27      2     27   4 [watchdog/4]
root         28      2     28   4 [migration/4]
root         29      2     29   4 [ksoftirqd/4]
root         30      2     30   4 [kworker/4:0]
root         31      2     31   4 [kworker/4:0H]
[root@kvm-host ~]# ps -eLo ruser,pid,ppid,lwp,psr,args | awk '{if($5==1) print $0}'
root         12      2     12   1 [watchdog/1]
root         13      2     13   1 [migration/1]
root         14      2     14   1 [ksoftirqd/1]
root         16      2     16   1 [kworker/1:0H]
root        491      2    491   1 [kworker/1:1]
root       1220      2   1220   1 [kworker/1:1H]
gdm        4050   4009   4050   1 /usr/libexec/gnome-settings-daemon
root       6374   6008   6374   1 /usr/libexec/tracker-miner-apps
root       7521      2   7521   1 [kworker/1:2]
root       8332   6623   8332   1 ps -eLo ruser,pid,ppid,lwp,psr,args
```

由上面的命令行输出信息可知，**cpu0\~cpu3**上分别有23个、10个、18个、23个线程在运行，而cpu4和cpu5上分别只有5个、6个线程在运行. 而且，根据输出信息中cpu4和cpu5上运行的线程信息(也包括进程在内)，分别有migration线程(用于进程在不同CPU间迁移)、两个kworker线程(用于处理workqueues)、ksoftirqd线程(用于调度CPU软中断的进程)、watchdog线程，这些进程都是内核对各个CPU的守护进程. 没有其他的普通进程在cup4和cpu5上运行(对比cpu0上，可以看到它有一些普通进程)，说明对它们的隔离是生效的. 

这里简单解释一下上面的一些命令行工具及其参数的意义. ps命令显示当前系统的进程信息的状态，它的“\-e”参数用于显示所有的进程，“\-L”参数用于将线程(light\-weight process，LWP)也显示出来，“\-o”参数表示以用户自定义的格式输出(其中“psr”这列表示当前分配给进程运行的处理器编号，“lwp”列表示线程的ID，“ruser”表示运行进程的用户，“pid”表示进程的ID，“ppid”表示父进程的ID，“args”表示运行的命令及其参数). 结合ps和awk工具的使用，是为了分别将在处理器cpu4和cpu5上运行的进程打印出来. 

## 5.2. 启动虚拟机并绑定vCPU

启动一个拥有两个vCPU的客户机，并将其vCPU绑定到宿主机中两个CPU上. 

此操作过程的命令行如下: 

```
#启动一个客户机
[root@kvm-host ~]# qemu-system-x86_64 -enable-kvm -smp 2 -m 4G rhel7.img -daemonize

#查看代表vCPU的QEMU线程
[root@kvm-host ~]# ps -eLo ruser,pid,ppid,lwp,psr,args | grep qemu | grep -v grep
root       8645      1   8645   6 qemu-system-x86_64 -enable-kvm -smp 2 -m 4G rhel7.img -daemonize
root       8645      1   8646  25 qemu-system-x86_64 -enable-kvm -smp 2 -m 4G rhel7.img -daemonize
root       8645      1   8647   2 qemu-system-x86_64 -enable-kvm -smp 2 -m 4G rhel7.img -daemonize
root       8645      1   8648   0 qemu-system-x86_64 -enable-kvm -smp 2 -m 4G rhel7.img -daemonize
root       8645      1   8649   3 qemu-system-x86_64 -enable-kvm -smp 2 -m 4G rhel7.img -daemonize
root       8645      1   8651  17 qemu-system-x86_64 -enable-kvm -smp 2 -m 4G rhel7.img -daemonize

#绑定代表整个客户机的QEMU进程，使其运行在cpu4上
[root@kvm-host ~]# taskset -pc 4 8645
pid 8645's current affinity list: 0-3,6-87
pid 8645's new affinity list: 4
#绑定第1个vCPU的线程，使其运行在cpu4上
[root@kvm-host ~]# taskset -pc 4 8649
pid 8649's current affinity list: 0-3,6-87
pid 8649's new affinity list: 4
#绑定第2个vCPU的线程，使其运行在cpu5上
[root@kvm-host ~]# taskset -pc 5 8651
pid 8651's current affinity list: 0-3,6-87
pid 8651's new affinity list: 5

#查看QEMU线程的绑定是否生效，下面的第5列为处理器亲和性
[root@kvm-host ~]# ps -eLo ruser,pid,ppid,lwp,psr,args | grep qemu | grep -v grep
root       8645      1   8645   4 qemu-system-x86_64 -enable-kvm -smp 2 -m 4G rhel7.img -daemonize
root       8645      1   8646  25 qemu-system-x86_64 -enable-kvm -smp 2 -m 4G rhel7.img -daemonize
root       8645      1   8647   2 qemu-system-x86_64 -enable-kvm -smp 2 -m 4G rhel7.img -daemonize
root       8645      1   8649   4 qemu-system-x86_64 -enable-kvm -smp 2 -m 4G rhel7.img -daemonize
root       8645      1   8651   5 qemu-system-x86_64 -enable-kvm -smp 2 -m 4G rhel7.img -daemonize
#执行vCPU的绑定后，查看在cpu4上运行的线程
[root@kvm-host ~]# ps -eLo ruser,pid,ppid,lwp,psr,args | awk '{if($5==4) print $0}'
root         27      2     27   4 [watchdog/4]
root         28      2     28   4 [migration/4]
root         29      2     29   4 [ksoftirqd/4]
root         30      2     30   4 [kworker/4:0]
root         31      2     31   4 [kworker/4:0H]
root       8645      1   8645   4 qemu-system-x86_64 -enable-kvm -smp 2 -m 4G rhel7.img -daemonize
root       8645      1   8649   4 qemu-system-x86_64 -enable-kvm -smp 2 -m 4G rhel7.img -daemonize
root       9478      2   9478   4 [kworker/4:1H]
root       9490      2   9490   4 [kworker/4:1]
#执行vCPU的绑定后，查看在cpu5上运行的线程
[root@kvm-host ~]# ps -eLo ruser,pid,ppid,lwp,psr,args | awk '{if($5==5) print $0}'
root         32      2     32   5 [watchdog/5]
root         33      2     33   5 [migration/5]
root         34      2     34   5 [ksoftirqd/5]
root         35      2     35   5 [kworker/5:0]
root         36      2     36   5 [kworker/5:0H]
root       5439      2   5439   5 [kworker/5:1]
root       8645      1   8651   5 qemu-system-x86_64 -enable-kvm -smp 2 -m 4G rhel7.img -daemonize
```

注: 也可以通过修改xml实现

由上面的命令行及其输出信息可知，在CPU进行绑定之前，代表这个客户机的QEMU进程和代表各个vCPU的QEMU线程分别被调度到cpu6、cpu3和cpu17上. 使用taskset命令将QEMU进程和第1个vCPU的线程绑定到cpu4，将第2个vCPU线程绑定到cpu5上. 在绑定之后即可查看到绑定结果生效，代表两个vCPU的QEMU线程分别运行在cpu4和cpu5上(即使再过一段时间，它们也不会被调度到其他CPU上去). 

对于taskset命令，此处使用的语法是: 

```
taskset -pc cpulist pid
```

根据上面的输出，在运行taskset命令之前，QEMU线程的处理器亲和性是除cpu4和cpu5以外的其他所有cpu; 而在运行“taskset \-pc 4 8645”命令后，提示该进程就只能被调度到cpu4上去运行，即通过taskset工具实现了将vCPU进程绑定到特定的CPU上. 

在上面命令行中，根据ps命令可以看到QEMU的线程和进程的关系，但如何查看vCPU与QEMU线程之间的关系呢？可以切换(使用“Ctrl\+Alt\+2”组合键)到QEMU monitor中进行查看，运行“info cpus”命令即可(还记得3.6节中运行过的“info kvm”命令吧)，其输出结果如下: 

```
(qemu) info cpus
* CPU #0: pc=0xffffffff81060eb6 (halted) thread_id=8649
  CPU #1: pc=0xffffffff81060eb6 (halted) thread_id=8651
```

由上面的输出信息可知，客户机中的cpu0对应的线程ID为8649，cpu1对应的线程ID为8651. 另外，“CPU#0”前面有一个星号(*)，表示cpu0是BSP(Boot Strap Processor，系统最初启动时在SMP生效前使用的CPU). 

总的来说，在KVM环境中，一般并**不推荐手动设置QEMU进程的处理器亲和性**来绑定vCPU，但是，在非常了解系统硬件架构的基础上，根据实际应用的需求，可以将其绑定到特定的CPU上，从而提高客户机中的CPU执行效率或实现CPU资源独享的隔离性. 