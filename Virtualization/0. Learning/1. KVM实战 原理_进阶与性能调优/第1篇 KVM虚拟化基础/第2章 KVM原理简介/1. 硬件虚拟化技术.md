
<!-- @import "[TOC]" {cmd="toc" depthFrom=1 depthTo=6 orderedList=false} -->

<!-- code_chunk_output -->

- [1. 概述](#1-概述)
- [2. CPU虚拟化](#2-cpu虚拟化)
- [3. 内存虚拟化](#3-内存虚拟化)
- [4. I/O虚拟化](#4-io虚拟化)
- [5. Intel虚拟化技术发展](#5-intel虚拟化技术发展)

<!-- /code_chunk_output -->

# 1. 概述

建议参照<系统虚拟化>

# 2. CPU虚拟化

CPU是计算机系统最核心的模块, 我们的程序执行到最后都是翻译为机器语言在CPU上执行的. 在没有CPU硬件虚拟化技术之前, 通常使用指令的二进制翻译(binary translation)来实现虚拟客户机中CPU指令的执行, 很早期的VMware就使用这样的方案, 其指令执行的翻译比较复杂, 效率比较低. 所以Intel最早发布的虚拟化技术就是CPU虚拟化方面的, 这才为本书的主角——KVM的出现创造了必要的硬件条件. 

Intel在处理器级别提供了对虚拟化技术的支持, 被称为VMX(virtual-machine extensions). 有两种VMX操作模式: VMX根操作(root operation)与VMX非根操作(non-root operation). 作为虚拟机监控器中的KVM就是运行在根操作模式下, 而虚拟机客户机的整个软件栈(包括操作系统和应用程序)则运行在非根操作模式下. 进入VMX非根操作模式被称为”VM Entry"; 从非根操作模式退出, 被称为”VM Exit". 

VMX的根操作模式与非VMX模式下最初的处理器执行模式基本一样, 只是它现在支持了新的VMX相关的指令集以及一些对相关控制寄存器的操作. VMX的非根操作模式是一个相对受限的执行环境, 为了适应虚拟化而专门做了一定的修改; 在客户机中执行的一些特殊的敏感指令或者一些异常会触发”VM Exit"退到虚拟机监控器中, 从而运行在VMX根模式. 正是这样的限制, 让虚拟机监控器保持了对处理器资源的控制. 

一个虚拟机监控器软件的最基础的运行生命周期及其与客户机的交互如图2-1所示. 

![](./images/2019-05-30-19-45-59.png)

软件通过执行VMXON指令进入VMX操作模式下; 在VMX模式下通过VMLAUNCH和VMRESUME指令进入客户机执行模式, 即VMX非根模式; 当在非根模式下触发VM Exit时, 处理器执行控制权再次回到宿主机的虚拟机监控器上; 最后虚拟机监控可以执行VMXOFF指令退出VMX执行模式. 

逻辑处理器在根模式和非根模式之间的切换通过一个叫作VMCS(virtual-machine control data structure)的数据结构来控制; 而VMCS的访问是通过VMCS指针来操作的. VMCS指针是一个指向VMCS结构的64位的地址, 使用VMPTRST和VMPTRLD指令对VMCS指针进行读写, 使用MREAD、VMWRITE和VMCLEAR等指令对VMCS实现配置. 

对于一个逻辑处理器, 它可以维护多个VMCS数据结构, 但是在任何时刻只有一个VMCS在当前真正生效. 多个VMCS之间也是可以相互切换的, VMPTRLD指令就让某个VMCS在当前生效, 而其他VMCS就自然成为不是当前生效的. 一个虚拟机监控器会为一个虚拟客户机上的每一个逻辑处理器维护一个VMCS数据结构. 

根据Intel的官方文档, 我们这里列举部分在非根模式下会导致”VM Exit"的敏感指令和一些异常供读者朋友参考, 这对于理解KVM的执行机制是必要的, 因为KVM也必须按照CPU的硬件规范来实现虚拟化软件逻辑. 

1)一定会导致VM Exit的指令: CPUID、GETSEC、INVD、XSETBV等, 以及VMX模式引入的INVEPT、INVVPID、VMCALL、VMCLEAR、VMLAUNCH、VMPTRLD、VMPTRST、VMRESUME、VMXOFF、VMXON等. 

2)在一定的设置条件下会导致VM Exit的指令: CLTS、HLT、IN、OUT、INVLPG、INVPCID、LGDT、LMSW、MONITOR、MOV from CR3、MOV to CR3、MWAIT、MWAIT、RDMSR、RWMSR、VMREAD、VMWRITE、RDRAND、RDTSC、XSAVES、XRSTORS等. 如在处理器的虚拟机执行控制寄存器中的”HLT exiting"比特位被置为1时, HLT的执行就会导致VM Exit. 

3)可能会导致VM Exit的事件: 一些异常、三次故障(Triple fault)、外部中断、不可屏蔽中断(NMI)、INIT信号、系统管理中断(SMI)等. 如在虚拟机执行控制寄存器中的”NMI exiting"比特位被置为1时, 不可屏蔽中断就会导致VM Exit. 

最后提一下, 由于发生一次VM Exit的代价是比较高的(可能会消耗成百上千个CPU执行周期, 而平时很多指令是几个CPU执行周期就能完成), 所以对于VM Exit的分析是虚拟化中性能分析和调优的一个关键点. 

# 3. 内存虚拟化

内存虚拟化的目的是给虚拟客户机操作系统提供一个从0地址开始的连续物理内存空间, 同时在多个客户机之间实现隔离和调度. 在虚拟化环境中, 内存地址的访问会主要涉及以下4个基础概念, 图2-2形象地展示了虚拟化环境中内存地址. 

![](./images/2019-05-30-19-46-51.png)

1)客户机虚拟地址, GVA(Guest Virtual Address)

2)客户机物理地址, GPA(Guest Physical Address)

3)宿主机虚拟地址, HVA(Host Virtual Address)

4)宿主机物理地址, HPA(Host Physical Address)

内存虚拟化就是要将客户机虚拟地址(GVA)转化为最终能够访问的宿主机上的物理地址(HPA). 对于客户机操作系统而言, 它不感知内存虚拟化的存在, 在程序访问客户机中虚拟地址时, 通过CR3寄存器可以将其转化为物理地址, 但是在虚拟化环境中这个物理地址只是客户机的物理地址, 还不是真实内存硬件上的物理地址. 所以, 虚拟机监控器就需要维护从客户机虚拟地址到宿主机物理地址之间的一个映射关系, 在没有硬件提供的内存虚拟化之前, 这个维护映射关系的页表叫作影子页表(Shadow Page Table). 内存的访问和更新通常是非常频繁的, 要维护影子页表中对应关系会非常复杂, 开销也较大. 同时需要为每一个客户机都维护一份影子页表, 当客户机数量较多时, 其影子页表占用的内存较大也会是一个问题. 

Intel CPU在硬件设计上就引入了EPT(Extended Page Tables, 扩展页表), 从而将客户机虚拟地址到宿主机物理地址的转换通过硬件来实现. 当然, 这个转换是通过两个步骤来实现的, 如图2-3所示. 首先, 通过客户机CR3寄存器将客户机虚拟地址转化为客户机物理地址, 然后通过查询EPT来实现客户机物理地址到宿主机物理地址的转化. EPT的控制权在虚拟机监控器中, 只有当CPU工作在非根模式时才参与内存地址的转换. 使用EPT后, 客户机在读写CR3和执行INVLPG指令时不会导致VM Exit, 而且客户页表结构自身导致的页故障也不会导致VM Exit. 所以通过引入硬件上EPT的支持, 简化了内存虚拟化的实现复杂度, 同时也提高了内存地址转换的效率. 

![](./images/2019-05-30-19-48-11.png)

除了EPT, Intel在内存虚拟化效率方面还引入了VPID(Virtual-processor identifier)特性, 在硬件级对TLB资源管理进行了优化. 在没有VPID之前, 不同客户机的逻辑CPU在切换执行时需要刷新TLB, 而TLB的刷新会让内存访问的效率下降. VPID技术通过在硬件上为TLB增加一个标志, 可以识别不同的虚拟处理器的地址空间, 所以系统可以区分虚拟机监控器和不同虚拟机上不同处理器的TLB, 在逻辑CPU切换执行时就不会刷新TLB, 而只需要使用对应的TLB即可. VPID的示意图如图2-4所示. 当CPU运行在非根模式下, 且虚拟机执行控制寄存器的”enable VPID"比特位被置为1时, 当前的VPID的值是VMCS中的VPID执行控制域的值, 其值是非0的. VPID的值在3种情况下为0, 第1种是在非虚拟化环境中执行时, 第2种是在根模式下执行时, 第3种情况是在非根模式下执行但”enable VPID"控制位被置0时. 

![](./images/2019-05-30-19-48-18.png)

# 4. I/O虚拟化

在虚拟化的架构下, 虚拟机监控器必须支持来自客户机的I/O请求. 

通常情况下有以下**4种I/O虚拟化方式**. 

1)设备模拟: 在虚拟机监控器中模拟一个传统的I/O设备的特性, 比如在QEMU中模拟一个Intel的千兆网卡或者一个IDE硬盘驱动器, 在客户机中就暴露为对应的硬件设备. 客户机中的I/O请求都由虚拟机监控器捕获并模拟执行后返回给客户机. 

2)前后端驱动接口: 在虚拟机监控器与客户机之间定义一种全新的适合于虚拟化环境的交互接口, 比如常见的virtio协议就是在客户机中暴露为virtio-net、virtio-blk等网络和磁盘设备, 在QEMU中实现相应的virtio后端驱动. 

3)设备直接分配: 将一个物理设备, 如一个网卡或硬盘驱动器直接分配给客户机使用, 这种情况下I/O请求的链路中很少需要或基本不需要虚拟机监控器的参与, 所以性能很好. 

4)设备共享分配: 其实是设备直接分配方式的一个扩展. 在这种模式下, 一个(具有特定特性的)物理设备可以支持多个虚拟机功能接口, 可以将虚拟功能接口独立地分配给不同的客户机使用. 如SR-IOV就是这种方式的一个标准协议. 

表2-1展示了这4种I/O虚拟化方式的优缺点, 给读者一个概括性的认识. 在这4种方式中, 前两种都是纯软件的实现, 后两种都需要特定硬件特性的支持. 

![](./images/2019-05-30-19-49-08.png)

设备直接分配在Intel平台上就是VT-d(Virtualization Technology For Directed I/O)特性, 一般在系统BIOS中可以看到相关的参数设置. Intel VT-d为虚拟机监控器提供了几个重要的能力: I/O设备分配、DMA重定向、中断重定向、中断投递等. 图2-5描述了在VT-d硬件特性的帮助下实现的设备直接分配的架构, 并与最传统的、通过软件模拟设备的I/O设备虚拟化进行了对比. 

![](./images/2019-05-30-19-49-24.png)

尽管VT-d特性支持的设备直接分配方式性能可以接近物理设备在非虚拟化环境中的性能极限, 但是它有一个缺点: 单个设备只能分配给一个客户机, 而在虚拟化环境下一个宿主机上往往运行着多个客户机, 很难保证每个客户机都能得到一个直接分配的设备. 为了克服这个缺点, 设备共享分配硬件技术就应运而生, 其中SR-IOV(Single Root I/O Virtualization and Sharing)就是这样的一个标准. 实现了SR-IOV规范的设备, 有一个功能完整的PCI-e设备成为物理功能(Physical Function, PF). 在使能了SR-IOV之后, PF就会派生出若干个虚拟功能(Virtual Function, VF). VF看起来依然是一个PCI-e设备, 它拥有最小化的资源配置, 有用独立的资源, 可以作为独立的设备直接分配给客户机使用. Intel的很多高级网卡如82599系列网卡就支持SR-IOV特性, 一个85299网卡PF就即可配置出多达63个VF, 基本可满足单个宿主机上的客户机分配使用. 当然, SR-IOV这种特性可以看作VT-d的一个特殊例子, 所以SR-IOV除了设备本身要支持该特性, 同时也需要硬件平台打开VT-d特性支持. 图2-6展示了一个Intel以太网卡支持SR-IOV的硬件基础架构. 

![](./images/2019-05-30-19-49-37.png)

# 5. Intel虚拟化技术发展

虚拟化技术从最初的纯软件的虚拟化技术, 逐步发展到硬件虚拟化技术的支持, 时至今日硬件虚拟化技术已比较成熟. 前面3小节已经分别就各种硬件虚拟化技术进行了介绍, 这里以Intel平台为例, 再对其做一个小结. 

Intel硬件虚拟化技术大致分为如下3个类别(这个顺序也基本上是相应技术出现的时间先后顺序). 

1)VT-x技术: 是指Intel处理器中进行的一些虚拟化技术支持, 包括CPU中引入的最基础的VMX技术, 使得KVM等硬件虚拟化基础的出现成为可能. 同时也包括内存虚拟化的硬件支持EPT、VPID等技术. 

2)VT-d技术: 是指Intel的芯片组的虚拟化技术支持, 通过Intel IOMMU可以实现对设备直接分配的支持. 

3)VT-c技术: 是指Intel的I/O设备相关的虚拟化技术支持, 主要包含两个技术: 一个是借助虚拟机设备队列(VMDq)最大限度提高I/O吞吐率, VMDq由Intel网卡中的专用硬件来完成; 另一个是借助虚拟机直接互连(VMDc)大幅提升虚拟化性能, VMDc主要就是基于SR-IOV标准将单个Intel网卡产生多个VF设备, 用来直接分配给客户机. 

图2-7展示了Intel的硬件虚拟化技术的发展线路图, 从中我们可以看到从2005年开始支持VT-x硬件虚拟化, 到现在较多的SR-IOV等VT-d的虚拟化技术, 硬件虚拟化技术家族有了越来越多的成员, 技术特性也逐步完善. 如何在具体业务的生产环境中充分利用硬件虚拟化技术带来的技术红利, 构建高性能、可扩展、易维护的虚拟化环境, 可能是大家学习虚拟化的一个主要目标. 通过本书, 希望大家也能够了解一些实践经验和受到一些启发. 

![](./images/2019-05-30-19-50-24.png)