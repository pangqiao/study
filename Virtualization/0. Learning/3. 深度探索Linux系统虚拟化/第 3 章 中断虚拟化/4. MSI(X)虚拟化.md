
虽然 APIC 相比 PIC 更进了一步, 但是我们看到, 外设发出中断请求后, 需要经过 I/O APIC 才能到达 LAPIC(CPU). 如果中断请求可以从设备直接发送给 LAPIC, 而不是绕道 I/O APIC, 可以大大减少中断处理的延迟. 事实上, 在 1999 年 PCI 2.2 就引入了MSI. MSI 全称是 Message Signaled Interrupts, 从名字就可以看出, 第3代中断技术不再基于管脚, 而是基于消息. 在 PCI 2.2 时, MSI是设备的一个可选特性, 到了2004年, PCIE规范发布, MSI就成为了PCIE设备强制要求的特性. 在PCI 3.3时, 又对MSI进行了一定的增强, 称为 MSI-X. 相比MSI, MSI-X 的每个设备可以支持更多的中断, 并且每个中断可以独立配置.

除了减少中断延迟外, 因为不再存在管脚的概念了, 所以之前因为管脚有限而共享管脚的问题自然就消失了. 之前当某个管脚有信号时, 操作系统需要逐个调用共享这个管脚的中断服务程序去试探是否可以处理这个中断, 直到某个中断服务程序可以正确处理. 同样的道理, 不再受管脚的数量约束, MSI 能够支持的中断数也大大增加了. 支持 MSI 的设备绕过 I/O APIC, 直接与 LAPIC 通过系统总线相连, 如下图所示.

![2024-03-04-22-29-01.png](./images/2024-03-04-22-29-01.png)

从PCI 2.1开始, 如果设备需要扩展某种特性, 可以向配置空间中的 Capabilities List 中增加一个 Capability, MSI利用的就是这个特性, 将 I/O APIC 中的功能扩展到设备自身. MSI 的 Capability Structure 如图3-13所示.

![2024-03-04-22-29-39.png](./images/2024-03-04-22-29-39.png)

为了支持多个中断, MSI-X 的 Capability Structure 在 MSI 的基础上增加了table, 其中字段 Table Offset 和 BIR 定义了 table 所在的位置, 其中 BIR 为BAR Indicator Register, 即指定使用哪个 BAR 寄存器, 然后从指定的这个 BAR 寄存器中取出映射在 CPU 地址空间中的基址, 加上 Table Offset 就定位了 table 的位置, 如下图所示.

![2024-03-04-22-30-08.png](./images/2024-03-04-22-30-08.png)

当外设准备发送中断消息时, 其将从 Capability Structure 中提取相关信息. 消息地址取自字段 message address, 其中 bits 31-20 是一个固定的值 0FEEH. PCI 总线根据消息地址, 得知这是一个中断消息, 会将其发送给 Host-to-PCI 桥, Host-to-PCI 桥将其发送到目的 CPU(LAPIC). 消息体取自 message data, 主要部分是中断向量.

# MSI(X)Capability 数据结构

在初始化设备时, 如果设备支持 MSI, 并且设备配置为启用 MSI 支持, 则内核中将组织 MSI Capability 数据结构, 然后通过 MMIO 的方式写到 PCI 设备的配置空间中. 内核中的 PCI 公共层为驱动提供了接口 `pci_enable_msix` 配置 MSI Capability 数据结构:

```cpp
commit 82af8ce84ed65d2fb6d8c017d3f2bbbf161061fb
virtio_pci: optional MSI-X support
linux.git/drivers/pci/msi.c
int pci_enable_msix(struct pci_dev* dev, struct msix_entry *entries, int nvec)
{
    ...
    status = pci_msi_check_device(dev, nvec, PCI_CAP_ID_MSIX);
    ...
}

static int msix_capability_init(struct pci_dev *dev,
                struct msix_entry *entries, int nvec)
{
    ...
     pci_read_config_dword(dev, msix_table_offset_reg(pos), &table_offset);
    bir = (u8)(table_offset & PCI_MSIX_FLAGS_BIRMASK);
    table_offset &= ~PCI_MSIX_FLAGS_BIRMASK;
    phys_addr = pci_resource_start (dev, bir) + table_offset;
    base = ioremap_nocache(phys_addr, nr_entries * PCI_MSIX_ENTRY_SIZE);
    ...
    /* MSI-X Table Initialization */
    for (i = 0; i < nvec; i++) {
        entry = alloc_msi_entry(dev);
        ...
        entry->msi_attrib.pos = pos;
        entry->mask_base = base;
        ...
        list_add_tail(&entry->list, &dev->msi_list);
    }

    ret = arch_setup_msi_irqs(dev, nvec, PCI_CAP_ID_MSIX);
    ...
}
```

代码中的变量bir和table_offset是不是特别熟悉, 没错, 就是PCI配置空间header中的这两个字段: Table Offset和BIR. 根据这两个变量, 内核计算出table的位置, 然后将配置空间中table所在位置, 通过iomap的方式, 映射到CPU的地址空间, 后面就可以像访问内存一样直接访问这个table了. 然后, 在内存中组织MSI Capability数据结构, 最后将组织好的MSI Capability数据结构, 包括这个结构映射到地址空间中的位置, 传给体系结构相关的函数 arch_setup_msi_irqs 中构建消息, 包括中断向量、目标CPU、触发模式等, 写入设备的PCI配置空间.

当Guest内核侧设置设备的MSI的Capability数据结构, 即写PCI设备的配置空间时, 将导致VM exit, 进入虚拟设备一侧. 虚拟设备将Guest内核设置的MSI的Capability信息记录到设备的MSI的Capability结构体中. 以虚拟设备virtio net为例, 函数callback_mmio接收Guest内核侧发来的配置信息, 记录到设备的MSI的Capability数据结构中:

```cpp
commit 3395f880e871c3fdd31f774f93415b1ed38a768b
kvm tools: Add MSI-X support to virtio-net
kvmtool.git/virtio/net.c
void virtio_net__init(const struct virtio_net_parameters *params)
{
    ...
    ndev.msix_io_block = pci_get_io_space_block();
    kvm__register_mmio(params->kvm, ndev.msix_io_block, 0x100, callback_mmio, NULL);
    ...
}

static void callback_mmio(u64 addr, u8 *data, u32 len, u8 is_write, void *ptr)
{
    void *table = pci_header.msix.table;
    if (is_write)
        memcpy(table + addr - ndev.msix_io_block, data, len);
    else
        memcpy(data, table + addr - ndev.msix_io_block, len);
}
```

# 建立IRQ routing表项

之前我们看到KVM模块为了从架构上统一处理虚拟PIC、虚拟APIC以及虚拟MSI, 设计了一个IRQ routing表. 对于每个中断, 在表格irq_routing中都需要建立一个表项, 当中断发生后, 会根据管脚信息, 或者说gsi, 索引到具体的表项, 提取表项中的中断向量, 调用表项中的函数指针set指向的函数发起中断.

所以, 建立好虚拟设备中的MSI-X Capability Structure还不够, 还需要将中断的信息补充到负责IRQ routing的表中. 当然, 虚拟设备并不能在建立好MSI-X Capability后, 就自作主张地发起配置KVM模块中的IRQ routing表, 而是需要由Guest内核的驱动发起这个过程, 以Virtio标准为例, 其定义了queue_msix_vector用于Guest内核驱动通知虚拟设备配置IRQ routing信息. 以virtio驱动为例, 其在配置virtioqueue时, 如果确认虚拟设备支持MSI并且也启用了MSI, 则通知虚拟设备建立IRQ routing表项:

```cpp
commit 82af8ce84ed65d2fb6d8c017d3f2bbbf161061fb
virtio_pci: optional MSI-X support
linux.git/drivers/virtio/virtio_pci.c
static struct virtqueue *vp_find_vq(...)
{
    ...
    if (callback && vp_dev->msix_enabled) {
        iowrite16(vector, vp_dev->ioaddr + VIRTIO_MSI_QUEUE_VECTOR);
        ...
    }
    ...
}
```

虚拟设备收到Guest内核驱动的通知后, 则从MSI Capability结构体中提取中断相关信息, 向内核发起请求, 为这个队列建立相应的IRQ routing表项

```cpp
commit 3395f880e871c3fdd31f774f93415b1ed38a768b
kvm tools: Add MSI-X support to virtio-net
kvmtool.git/virtio/net.c
static bool virtio_net_pci_io_out(...)
{
    ...
    case VIRTIO_MSI_QUEUE_VECTOR: {
        ...
        gsi = irq__add_msix_route(kvm,
                      pci_header.msix.table[vec].low,
                      pci_header.msix.table[vec].high,
                      pci_header.msix.table[vec].data);
        ...
    }
    ...
}

kvmtool.git/irq.c
int irq__add_msix_route(struct kvm *kvm, u32 low, u32 high, u32 data)
{
    int r;

    irq_routing->entries[irq_routing->nr++] =
        (struct kvm_irq_routing_entry) {
            .gsi = gsi,
            .type = KVM_IRQ_ROUTING_MSI,
            .u.msi.address_lo = low,
            .u.msi.address_hi = high,
            .u.msi.data = data,
        };

    r = ioctl(kvm->vm_fd, KVM_SET_GSI_ROUTING, irq_routing);
    ...
}
```

# MSI 设备中断过程

对于虚拟设备而言, 如果启用了MSI, 中断过程就不需要虚拟I/O APIC参与了. 虚拟设备直接从自身的MSI(-X)Capability Structure中提取目的CPU等信息, 向目的CPU关联的虚拟LAPIC发送中断请求, 后续LAPIC的操作与之前讨论的APIC虚拟化完全相同, 整个过程如图所示.

![2024-03-05-21-25-47.png](./images/2024-03-05-21-25-47.png)

在KVM设计了IRQ routing后, 当KVM收到虚拟设备发来的中断时, 不再区分是PIC、APIC还是MSI, 而是调用一个统一的接口 kvm_set_irq, 该函数遍历IRQ routing表中的每一个表项, 调用匹配的每个entry的set指向的函数发起中断, 如此就实现了代码统一. set 指向的函数, 负责注入中断, 比如对于使用PCI的外设, set指向 kvm_set_pic_irq; 对于使用APIC的外设, set指向 kvm_set_ioapic_irq; 对于支持并启用了MSI的外设, 则set指向为MSI实现的发送中断的接口kvm_set_msi.

```cpp
commit 79950e1073150909619b7c0f9a39a2fea83a42d8
KVM: Use irq routing API for MSI
linux.git/virt/kvm/irq_comm.c
int setup_routing_entry(...)
{
    ...
    switch (ue->type) {
    case KVM_IRQ_ROUTING_IRQCHIP:
        ...
        switch (ue->u.irqchip.irqchip) {
        case KVM_IRQCHIP_PIC_MASTER:
            e->set = kvm_set_pic_irq;
        ...
        case KVM_IRQCHIP_IOAPIC:
                e->set = kvm_set_ioapic_irq;
        ...
    case KVM_IRQ_ROUTING_MSI:
        e->set = kvm_set_msi;
        ...
    ...
}
```

函数kvm_set_msi的实现, 与I/O APIC的set函数非常相似. I/O APIC从中断重定向表提取中断信息, 而MSI-X是从MSI-X Capability提取信息, 找到目标CPU, 可见, MSI-X就是将I/O APIC的功能下沉到外设中. 最后, 都是调用目标CPU关联的虚拟LAPIC的接口 kvm_apic_set_irq 向 Guest 注入中断:

```cpp
commit 79950e1073150909619b7c0f9a39a2fea83a42d8
KVM: Use irq routing API for MSI
linux.git/virt/kvm/irq_comm.c
static void kvm_set_msi(...)
{
    ...
    int dest_id = (e->msi.address_lo & MSI_ADDR_DEST_ID_MASK)
            >> MSI_ADDR_DEST_ID_SHIFT;
    int vector = (e->msi.data & MSI_DATA_VECTOR_MASK)
            >> MSI_DATA_VECTOR_SHIFT;
    ...
    /* IOAPIC delivery mode value is the same as MSI here */
    switch (delivery_mode) {
    case IOAPIC_LOWEST_PRIORITY:
        vcpu = kvm_get_lowest_prio_vcpu(ioapic->kvm, vector,
                deliver_bitmask);
        if (vcpu != NULL)
            kvm_apic_set_irq(vcpu, vector, trig_mode);
    ...
}
```
