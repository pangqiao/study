
<!-- @import "[TOC]" {cmd="toc" depthFrom=1 depthTo=6 orderedList=false} -->

<!-- code_chunk_output -->

- [1 磁盘I/O性能测试工具](#1-磁盘io性能测试工具)
  - [1.1 DD](#11-dd)
  - [1.2 fio](#12-fio)
  - [1.3 Bonnie\+\+](#13-bonnie)
  - [1.4 hdparm](#14-hdparm)
- [2 测试环境配置](#2-测试环境配置)
- [3 性能测试方法](#3-性能测试方法)
  - [3.1 DD](#31-dd)
  - [3.2 fio](#32-fio)
  - [3.3 Bonnie\+\+](#33-bonnie)
- [4　性能测试数据](#4-性能测试数据)
  - [4.1 DD](#41-dd)
  - [4.2 fio](#42-fio)
  - [4.3 Bonnie\+\+](#43-bonnie)

<!-- /code_chunk_output -->

# 1 磁盘I/O性能测试工具

在一个计算机系统中，CPU获取**自身缓存数据**的速度非常快，读写内存的速度也比较快，内部局域网速度也比较快（特别是使用万兆以太网），但是磁盘I/O的速度是相对比较慢的。

很多的日常软件的运行都会读写磁盘，而且大型的数据库应用（如Oracle、MySQL等）都是磁盘I/O密集型的应用，所以在**KVM虚拟化**中磁盘I/O的性能也是比较关键的。

测试磁盘I/O性能的工具有很多，如DD、Bonnie\+\+、fio、iometer、hdparm等。下面简单介绍其中几个工具。

## 1.1 DD

DD（命令为dd）是Linux系统上一个非常流行的**文件复制工具**，在复制文件的同时可以根据其具体选项进行转换和格式化等操作。

通过DD工具复制同一个文件（相同数据量）所需要的时间长短即可粗略评估磁盘I/O的性能。

一般的Linux系统中都自带这个工具，用man dd命令即可查看DD工具的使用手册。

## 1.2 fio

fio是一个被广泛使用的进行**磁盘性能及压力测试的工具**。它功能强大而灵活，可以用它定义（模拟）出**各种工作负载（workload**），模拟**真实使用场景**，以更准确地衡量磁盘的性能。

除了测试磁盘**读写的带宽**以外，它还统计**IOPS**并且以**不同的延迟时间**分布表示；

除了**总的延迟时间**，它还分别统计**I/O递交的时间延迟**和**I/O完成的时间延迟**。

fio的主页是：http://git.kernel.dk/?p=fio.git;a=summary 。它可以运行在Linux、UNIX、Windows等多个操作系统平台上，多数Linux发行版也包含它的安装包。

## 1.3 Bonnie\+\+

Bonnie\+\+是以Bonnie的代码为基础编写而成的软件，它使用一系列对硬盘驱动器和文件系统的简单测试来衡量其性能。

Bonnie\+\+可以模拟像数据库那样去访问一个单一的大文件，也可以模拟像Squid那样创建、读取和删除许许多多的小文件。

它可以实现有序地读写一个文件，也可以随机地查找一个文件中的某个部分，而且支持按字符方式和按块方式读写。

## 1.4 hdparm

hdparm是一个用于**获取和设置SATA和IDE设备参数**的工具，在RHEL 7.3中可以用yum install hdparm命令来安装hdparm工具。

hdparm也可以**粗略地测试磁盘的I/O性能**，通过如下的命令即可粗略评估sdb这个磁盘的**读性能**。

```
hdparm -tT /dev/sdb
```

本节选择了DD、FIO、Bonnie\+\+这3种工具用于KVM虚拟化的磁盘I/O性能测试。

# 2 测试环境配置

对KVM的磁盘I/O虚拟化性能测试的环境配置，与前面CPU、内存、网络性能测试的环境配置基本相同，下面仅说明一下不同之处和需要强调的地方。

在本次对磁盘I/O的性能测试中，**客户机第2块硬盘**专门用于磁盘性能测试，这块硬盘是**宿主机的一个LVM分区**。

我们不选择前面通常使用的raw image文件，是为了**避免宿主机文件系统这一层的消耗**，从而使得客户机与宿主机直接的磁盘性能比较更加公平。

如图10\-10所示，因为笔者的宿主机环境是LVM分区，所以本节采用中间的一种方式。当然，最右边采用直接的物理硬盘分区的方式层次更简洁，可以预见其绝对性能应该更好。但如同前面几节的测试目标一样，我们这里注重测量虚拟化层的性能损耗，也就是客户机环境中基准测试结果，与同样资源、软件堆栈层次的原生系统中同样基准测试的结果进行对比。

![](./images/2019-05-11-23-01-25.png)

另外，磁盘性能测试与磁盘分区（包括物理分区和LVM分区）时候的参数设定、在硬盘分区上创建文件系统时候的参数设定，以及关联与具体磁盘的IO Scheduler都有密切关系。但这些不是我们这里测试的关注点，我们只要注意原始环境的参数设置和客户机里的参数设置一致即可。具体来说，我们这里的IO Scheduler、分区设置（LVM）、文件系统（XFS）的参数都选用最简单的默认值，这也覆盖了大多数用户的使用场景。

```
[root@kvm-host current]# lvcreate --size 32G --name perf-test-lvm rhel
[root@kvm-host current]# lvdisplay /dev/rhel/perf-test-lvm
    --- Logical volume ---
    LV Path                /dev/rhel/perf-test-lvm
    LV Name                perf-test-lvm
    VG Name                rhel
    LV UUID                x8hVqL-U0Z8-i8fl-rFz1-tqrR-sAxW-vHV0d2
    LV Write Access        read/write
    LV Creation host, time kvm-host, 2017-04-29 21:43:12 +0800
    LV Status              available
    # open                 1
    LV Size                32.00 GiB
    Current LE             8192
    Segments               1
    Allocation             inherit
    Read ahead sectors     auto
    - currently set to     8192
    Block device           253:2
    
[root@kvm-guest current]# mkfs -t xfs -f /dev/sda
```

在实验中使用的希捷2 TB大小的SATA硬盘，型号如下：

```
ST2000DL003-9VT166
```

本次测试评估了QEMU/KVM中的纯软件模拟的IDE磁盘和使用virtio\-blk驱动的磁盘（见其他内容），启动客户机的QEMU命令行分别如下（注意，客户机有两块硬盘，一块是系统盘，一直是virtio\-blk接口，第二块才是我们的测试硬盘，/dev/rhel/perf\-test\-lvm）：

```
#1. 使用IDE磁盘
qemu-system-x86_64 -enable-kvm -cpu host -smp cpus=4,cores=4,sockets=1 -m 16G -drive file=./rhel7.img,format=raw,if=none,media=disk,id=virtio_drive0 -device virtio-blk-pci,drive=virtio_drive0,bootindex=0 -drive file=/dev/rhel/perf-test-lvm,media=disk,if=ide,format=raw,id=ide_drive,cache=none -device virtio-net-pci,netdev=nic0 -netdev bridge,id=nic0,br=virbr0 -name perf_test -display sdl

#2. 使用virtio-blk磁盘
qemu-system-x86_64 -enable-kvm -cpu host -smp cpus=4,cores=4,sockets=1 -m 16G -drive file=./rhel7.img,format=raw,if=none,media=disk,id=virtio_drive0 -device virtio-blk-pci,drive=virtio_drive0,bootindex=0 -drive file=/dev/rhel/perf-test-lvm,media=disk,if=none,format=raw,id=virtio_drive1,cache=none -device virtio-blk-pci,drive=virtio_drive1 -device virtio-net-pci,netdev=nic0 -netdev bridge,id=nic0,br=virbr0 -name perf_test -display sdl
```

从上面的命令行可以看出，测试磁盘镜像文件是raw格式的，并且配置有“cache=none”来绕过页面缓存。配置为“cache=none”绕过了页面缓存，但是没有绕过磁盘自身的磁盘缓存；如果要在宿主机中彻底绕过这两种缓存，可以在启动客户机时配置为“cache=directsync”。不过由于“cache=directsync”配置会让客户机中磁盘I/O效率比较低，所以这种配置用得比较少，常用的配置一般为“cache=writethrough”“cache=none”等。关于“cache=xx”选项的配置，可以参考5.4.1节。

由于启动客户机时使用的磁盘配置选项“cache=xx”的设置对磁盘I/O测试结果的影响非常大，所以本次结果仅能代表“cache=none”这样配置下的一次基准测试。

# 3 性能测试方法

对非虚拟化的原生系统和KVM客户机都执行相同的磁盘I/O基准测试，然后对比其测试结果。

## 3.1 DD

用DD工具对读取磁盘文件进行测试，测试4种不同的块大小。使用的命令如下：

```
dd if=file.dat of=/dev/null iflag=direct bs=1K count=100K
dd if=file.dat of=/dev/null iflag=direct bs=8K count=100K
dd if=file.dat of=/dev/null iflag=direct bs=1M count=10K
dd if=file.dat of=/dev/null iflag=direct bs=8M count=2K
```

在上面命令中，if=xx表示输入文件（即被读取的文件），of=xx表示输出文件（即写入的文件）。

这里为了测试读磁盘的速度，所以读取一个磁盘上的文件，然后将其写到/dev/null这个空设备中。iflag=xx表示打开输入文件时的标志，此处设置为**direct是为了绕过页面缓存**，以得到更真实的读取磁盘的性能。bs=xx表示一次读写传输的数据量大小，count=xx表示执行多少次数据的读写。

用DD工具向磁盘上写入文件的测试，也测试4种不同的块大小。使用的命令如下：

```
dd if=/dev/zero of=dd1.dat conv=fsync oflag=direct bs=1K count=100K
dd if=/dev/zero of=dd1.dat conv=fsync oflag=direct bs=8K count=100K
dd if=/dev/zero of=dd1.dat conv=fsync oflag=direct bs=1M count=10K
dd if=/dev/zero of=dd1.dat conv=fsync oflag=direct bs=8M count=2K
```

在上面的命令中，为了测试磁盘写入的性能，使用了/dev/zero这个提供空字符的特殊设备作为输入文件。conv=fsync表示每次写入都要同步到物理磁盘设备后才返回，oflag=direct表示使用直写的方式绕过页面缓存。conv=fsync和oflag=direct这两个配置都是为了写入数据时尽可能地绕过缓存，从而尽可能真实地反映磁盘的实际I/O性能。

关于dd命令的详细参数，可以用man dd命令查看其帮助文档。

## 3.2 fio

fio是广泛使用的对硬盘进行基准和压力测试的工具。它支持多达30种IO引擎（ioengine），如sync、psync、posixaio、libaio等。

下载fio源代码（ https://github.com/axboe/fio/release ，本书使用写作时最新版本为3.0版），解压、configure、make、make install之后，就可以使用fio命令了。

在后面的磁盘I/O性能中，具体使用fio命令如下：

```
# 原生环境
# 顺序读
fio -filename=/dev/rhel/perf-test-lvm -rw=read -direct=1 -ioengine=sync -size=2g -numjobs=1 -bs=8k -name=robert_read
# 顺序写
fio -filename=/dev/rhel/perf-test-lvm -rw=write -direct=1 -ioengine=sync -size=2g -numjobs=1 -bs=8k -name=robert_write
# 随机读
fio -filename=/dev/rhel/perf-test-lvm -rw=randread -direct=1 -ioengine=sync -size=128m -numjobs=1 -bs=8k -name=robert_randread
# 随机写
fio -filename=/dev/rhel/perf-test-lvm -rw=randwrite -direct=1 -ioengine=sync -size=128m -numjobs=1 -bs=8k -name=robert_randwrite

# ide guest
fio -filename=/dev/sda -rw=read -direct=1 -ioengine=sync -size=2g -numjobs=1 -bs=8k -name=robert_read
fio -filename=/dev/sda -rw=write -direct=1 -ioengine=sync -size=2g -numjobs=1 -bs=8k -name=robert_write
fio -filename=/dev/sda -rw=randread -direct=1 -ioengine=sync -size=128m -num-jobs=1 -bs=8k -name=robert_randread
fio -filename=/dev/sda -rw=randwrite -direct=1 -ioengine=sync -size=128m -num-jobs=1 -bs=8k -name=robert_randwrite

# virtio guest
fio -filename=/dev/vdb -rw=read -direct=1 -ioengine=sync -size=2g -numjobs=1 -bs=8k -name=robert_read
fio -filename=/dev/vdb -rw=write -direct=1 -ioengine=sync -size=2g -numjobs=1 -bs=8k -name=robert_write
fio -filename=/dev/vdb -rw=randread -direct=1 -ioengine=sync -size=128m -numjobs=1 -bs=8k -name=robert_randread
fio -filename=/dev/vdb -rw=randwrite -direct=1 -ioengine=sync -size=128m -numjobs=1 -bs=8k -name=robert_randwrite
```

在上面的命令中，以原生系统情况为例，我们对/dev/rhel/perf-test-lvm分区进行fio测试（\-filename），进行的操作包括顺序读（\-rw=read）、顺序写（\-rw=write）、随机读（\-rw=randread）、随机写（\-rw=randwrite）。size参数指定了I/O操作的大小，因为不同的I/O操作其速度不同。为了在有限时间内完成测试，我们指定了不同的大小（2g、128m）。

I/O引擎采用的是最传统的sync，以便与dd的数据进行类比。direct参数指示绕过页面缓存，与dd命令中的oflag=direct类似。bs参数的意思是块大小（block size），我们采用8k也是为了与dd的速率进行类比（不指定的话，默认是4k，这也是绝大多数文件系统默认的块大小）。numjobs参数决定了会同时启动多少个进程并行进行I/O操作，这里采用1也是为了与上面dd测试保持一致。name参数只是给这次fio测试取个名字。

在IDE guest以及virtio guest中，命令与原生系统一样，只是将测试对象改成了/dev/sda和/dev/vdb，它们是/dev/rhel/perf\-test\-lvm分区以不同形式传入客户机后，在客户机里看到的虚拟磁盘的路径（名字）。

## 3.3 Bonnie\+\+

从 http://www.coker.com.au/bonnie++/ 网页下载bonnie\+\+\-1.03e.tgz文件，然后解压，对其进行配置、编译、安装的命令行操作如下：

```
[root@kvm-guest ~]# cd bonnie++-1.03e
[root@kvm-guest bonnie++-1.03e]# ./configure
[root@kvm-guest bonnie++-1.03e]# make
[root@kvm-guest bonnie++-1.03e]# make install
```

也可以从Fedora EPEL（Extra Packages for Enterprise Linux） https://fedoraproject.org/wiki/EPEL ，通过yum直接在RHEL7中安装Bonnie++。

本次测试使用Bonnie\+\+的命令如下：

```
bonnie++ -d /mnt/raw_disk -D -b -m kvm-guest -x 3 -u root
```

其中，\-d指定测试对象是哪个目录（我们测试用的分区mount到哪里），\-D表示在批量I/O测试时使用直接I/O的方式（O\_DIRECT），\-b的含义等同于fio的‘\-direct=1’和dd工具的‘conv=fsync’，指绕过写缓存，每次写操作都同步到磁盘，\-m kvm\-guest表示Bonnie\+\+得到的主机名为kvm\-guest，\-x 3表示循环执行3遍测试，\-u root表示以root用户运行测试。
在执行完测试后，默认会在当前终端上输出测试结果。可以将其CSV格式的测试结果通过Bonnie\+\+提供的bon\_csv2html转化为更容易读的HTML文档。命令行操作如下：

```
[root@kvm-guest bonnie++-1.03e]# echo "native,4G,102817,88,58631,25,56712,4,108330,91,151383,7,299.0,1,16,+++++,+++,+++++,+++,++++"| perl bon_csv2html > native-bonnie-1.html
```

Bonnie\+\+是一个强大的测试硬盘和文件系统的工具，关于Bonnie++命令的用法，可以用man bonnie++命令获取帮助手册，关于Bonnie\+\+工具的原理及测试方法的简介，可以参考其源代码中的readme.html文档。

# 4　性能测试数据

分别用DD、fio、Bonnie\+\+这3个工具在原生系统和KVM客户机中进行测试，然后对比其测试结果数据。为了尽量减小误差，每个测试项目都收集了3次测试数据，下面提供的测试数据都是根据3次测试计算出的平均值。

## 4.1 DD

使用DD工具测试磁盘读写性能，将得到的测试数据进行对比，如图10\-11和图10\-12所示。读写速率越大，说明磁盘I/O性能越好。

![](./images/2019-05-11-23-19-52.png)

![](./images/2019-05-11-23-39-19.png)

根据图10-11和图10-12中所示的数据可知，virtio方式的磁盘比纯模拟的IDE磁盘在小数据块时（bs=1K，bs=8K）读写性能要好，在大数据块时（bs=1M，bs=8M）virtio的读写速度与纯模拟的IDE情况差不多，甚至在8M数据块情况下还逊于后者。在数据块较大时（如bs=1M，bs=8M），KVM客户机中virtio和IDE两种方式的磁盘的读写性能都与原生系统相差不大。

## 4.2 fio

fio的基准测试数据对比如图10-13所示。顺序读、写的速度与上面DD的数据一致：Qemu模拟的IDE硬盘，性能只有原生的大约一半左右，而virtio的情况则几乎和原生系统一样，甚至顺序写的性能还略好于原生系统一点点。随机读、写是DD没有涵盖的测试项，从fio的结果我们可以看到，随机读写的速率要比顺序读写的速率低得多，在很低速的情况下，原生系统、模拟IDE、virtio的性能都差不多，这也与上面DD在bs=1k的低速情况下的性能表现相吻合。

## 4.3 Bonnie\+\+

图10-14所示为Bonnie++中文件读写操作的测试结果对比，其中的Seq-O-Per-Char表示顺序按字节写、Seq-O-Blk表示顺序按块写，Seq-O-Rewr就表示先读后写并lseek，Seq-I-Per-Char表示顺序按字节读，Seq-I-Blk表示顺序按块读。图10-15为文件创建、删除操作的测试结果对比，其中Rand、Seek表示随机改变文件读写指针偏移量（使用lseek()和random()函数），Seq-Create、Seq-Del等顾名思义，不赘述了。

本次Bonnie++测试没有指定一次读写的数据块的大小，默认值是8 KB，所以，图10-15所示的读写速率测试结果与前面DD、fio工具测试每次读写8 KB数据时的测试结果大体一致，相差不大。但仔细观察可以看到，不同于DD和fio，Bonnie++中的不少测试项原生系统的数据反而不如虚拟环境的模拟IDE和Virtio-Blk，具体的原因要分析其源代码，这里不深究了。

![](./images/2019-05-11-23-40-49.png)

![](./images/2019-05-11-23-41-04.png)

![](./images/2019-05-11-23-41-18.png)

从DD、fio、Bonnie\+\+的测试结果可以看出，当一次读写的数据块较小时，KVM客户机中的磁盘读写速率比非虚拟化原生系统慢得多，而一次读写数据块较大时，磁盘I/O性能则差距不大。在一般情况下，用virtio方式的磁盘I/O性能比纯模拟的IDE磁盘要好一些。
