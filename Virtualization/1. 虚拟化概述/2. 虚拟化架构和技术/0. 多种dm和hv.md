
bhyve, THE BSD HYPERVISOR, https://bhyve.org/

crosvm, https://github.com/google/crosvm.git

cloud-hypervisor, https://github.com/cloud-hypervisor/cloud-hypervisor.git

kvmtool: https://github.com/kvmtool/kvmtool.git

kvm-unit-tests: https://gitlab.com/kvm-unit-tests/kvm-unit-tests.git

linux kvm tests: tools/testing/selftests/kvm/





# 1. 硬件虚拟化技术背景

详细见`系统虚拟化`



## 1.1. 指令集

传统的处理器通过选择不同的运行(Ring 特权)模式, 来选择指令集的范围, 内存的寻址方式, 中断发生方式等操作. 在原有的 Ring 特权等级的基础上, 处理器的硬件虚拟化技术带来了一个新的运行模式: Guest 模式[1], 来实现指令集的虚拟化. 当切换到 Guest 模式时, 处理器提供了先前完整的特权等级, 让 Guest 操作系统可以不加修改的运行在物理的处理器上. Guest 与 Host 模式的处理器上下文完全由硬件进行保存与切换. 此时, 虚拟机监视器(Virtual Machine Monitor)通过一个位于内存的数据结构(Intel 称为 VMCS, AMD 称为 VMCB)来控制 Guest 系统同 Host 系统的交互, 以完成整个平台的虚拟化.

## 1.2. MMU

传统的操作系统通过硬件 MMU 完成虚拟地址到物理地址的映射. 在虚拟化环境中, Guest 的虚拟地址需要更多一层的转换, 才能放到地址总线上:

```
    Guest 虚拟地址 -> Guest 物理地址 -> Host 物理地址
                 ^               ^
                 |               |
                MMU1            MMU2
```

其中 MMU1 可以由软件模拟(Shadow paging 中的 vTLB)或者硬件实现(Intel EPT、AMD NPT). MMU2 由硬件提供.

## 1.3. IO

系统的 IO 虚拟化技术, 通常是 VMM 捕捉 Guest 的 IO 请求, 通过软件模拟的传统设备将其请求传递给物理设备. 一些新的支持虚拟化技术的设备, 通过硬件技术(如 Intel VT-d), 可以将其直接分配给 Guest 操作系统, 避免软件开销.
