
# 中断模式

**非中断模式**直接把**虚拟 pci 设备**当做一个**共享内存**进行操作, **中断模式**则会操作**虚拟 pci 的寄存器**进行通信, 数据的传输都会触发一次**虚拟 pci 中断**并触发中断回调, 使接收方显式感知到数据的到来, 而不是一直阻塞在 read.

在虚拟机之间或者宿主机与虚拟机之间通过**共享内存进行通信**的情形下, **共享内存的两端**必须依赖**轮询方式**来实现**通知机制**. 这种方式是 ivshmem 提供的 `ivshmem-plain` 设备的使用方式.

**中断模式** 就是 `ivshmem-doorbell` 设备的**使用方式**, 它提供了**基于中断**的通知机制.

从代码分析和实际验证:

* **guest** 与 **guest** 之间可以实现中断与非中断 2 种模式下的通信

* **host** 与 **guest** 之间只支持**非中断模式**的通信.

### INTx vs. MSI

传统的中断都有**专门的中断 pin**, 当中断信号产生时, 中断 PIN 电平产生变化(一般是拉低). INTx 就是传统的外部中断触发机制, 它使用**专门的通道**来产生控制信息. 然而 PCIe 并没有多根独立的中断 PIN, 于是使用特殊的信号来模拟中断 PIN 的置位和复位.

**MSI** 的全称是 `Message Signaled Interrupt`. MSI 出现在 PCI 2.2 和 PCIe 的规范中, 是一种内部中断信号机制. MSI 允许设备向一段指定的 MMIO 地址空间写一小段数据, 然后 chipset 以此产生相应的中断给 CPU.

从电气机械的角度, MSI 减少了对 interrupt pin 个数的需求, 增加了中断号的数量, **传统的 PCI 中断**只允许**每个 device** 拥有 **4 个中断**, 并且由于这些中断都是共享的, 大部分 device 都只有一个中断, **MSI** 允许**每个 device** 有1, 2, 4, 8, 16甚至 **32 个中断**.

使用 MSI 也有一点点性能上的优势. 使用传统的 PIN 中断, 当中断到来时, 程序去读内存获取数据时有可能会产生冲突. 其原因 device 的数据主要通过DMA来传输, 而在PIN中断到达时, DMA传输还未能完成, 此时cpu不能获取到数据, 只能空转. 而MSI不会存在这个问题, 因为MSI都是发生在DMA传输完成之后的.

### ivshmem-doorbell

`ivshmem-doorbell` 提供了**两种中断方式**:

* 一种是**传统的**基于 **INTx** 的中断, 它主要使用 **BAR0** 的 `Interrupt Mask` 和 `Interrupt Status` 两个寄存器;

* 另一种是基于 `MSI-X` 的中断, 它主要使用 **BAR0** 的 **IVPosition** 和 **Doorbell** 两个寄存器.

由于具有了如上特性, ivshmem 在执行 **pin 中断**时(**revision 0** 时候), 则写 1 到 `Interrupt Status` **bit0** 触发对目标 guest 的中断.

如果使用 **Msi**, 则 ivshmem 设备支持**多 msi 向量**. 低 16 bit 写入值为 0 到 Guest 所支持的最大向量. 低 16 位写入的值就是目标 guest 上将会触发的msi向量. Msi 向量在 vm 启动时配置. Mis 不设置 status 位, 因此除了中断自身, 所有信息都通过共享内存区域通信. 由于设备支持多 msi 向量, 这样可以使用不同的向量来表示不同的事件. 这些向量的含义由用户来定.

使用共享的设备端叫做 peer. **IVPosition 寄存器**存储**该 peer 的数字标识符**(0-65535), 称做 `peer_id`. 该寄存器为**只读寄存器**.

而 Doorbell 寄存器为**只写寄存器**. ivshmem-doorbell 设备支持**多个中断向量**, **写入 Doorbell 寄存器**则触发**共享该内存**的**某个 peer 的某个中断**. Doorbell 为 32 位

* 低 16 位为 `peer_id`;

* 高 16 位为**中断向量号**(这里是从 0 开始的**顺序号**, 而**非** PCI 驱动在 Guest 虚拟机内部所申请的**向量号**).

# server

使用 `ivshmem-doorbell` 机制需要运行 `ivshmem-server`. 它是在 host 上运行的一个应用程序, 在 qemu 启动前会启动, 根据参数**创建共享内存**, 并通过**监听本地 UNIX DOMAIN SOCKET** 等待共享内存的 **peer** 来**连接**.

添加了 `ivshmem-doorbell` **设备**的 QEMU 进程会连接该 **socket**, 待 socket 连接建立后，server 会通过 socket 指派给每个 vm 一个 peer id 号(用来标识 vm), 并且将 id 号同 eventfd 文件描述符一起发给 qemu 进程(一个efd表示一个中断向量，msi下有多个efd). guests 之间通过 eventfd 来通知中断. 每个 guest 都在与自己 id 所绑定的 eventfd 上侦听, 并且使用其它 eventfd 来向其它 guest 发送中断.

server 代码参考 `contrib/ivshmem-server/*`

# server 初始化

ivshmem_server 启动方式如下:

```
cd contrib/ivshmem-server
./ivshmem-server -l 4M -M fg-doorbell -n 8 -F -v
```

* `-l`: 共享内存大小

* `-M`: share memory 文件名, `-m` 表明是目录

* `-n`: 向量数目

* `-F`: 不是 daemonize

* `-v`: verbose

Server 端通过 select 侦听一个 qemu 上 socket 的连接。

```cpp
int
ivshmem_server_start(IvshmemServer *server)
{
    struct sockaddr_un s_un;
    int shm_fd, sock_fd;

    // -M shm文件
    if (server->use_shm_open) {
        IVSHMEM_SERVER_DEBUG(server, "Using POSIX shared memory: %s\n",
                             server->shm_path);
        shm_fd = shm_open(server->shm_path, O_CREAT | O_RDWR, S_IRWXU);
    // -m 目录名
    } else {
        gchar *filename = g_strdup_printf("%s/ivshmem.XXXXXX", server->shm_path);
        IVSHMEM_SERVER_DEBUG(server, "Using file-backed shared memory: %s\n",
                             server->shm_path);
        // 产生临时文件
        shm_fd = mkstemp(filename);
        unlink(filename);
        g_free(filename);
    }
    if (ivshmem_server_ftruncate(shm_fd, server->shm_size) < 0) {
    }
    ...
    // 创建一个 socket
    sock_fd = socket(AF_UNIX, SOCK_STREAM, 0);
    // local, 用于同一台机器上的进程间通信
    s_un.sun_family = AF_UNIX;
    // s_un.sun_path 默认是 /tmp/ivshmem_socket
    snprintf(s_un.sun_path, sizeof(s_un.sun_path), "%s",
            server->unix_sock_path);
    // 与本地文件进行绑定 
    if (bind(sock_fd, (struct sockaddr *)&s_un, sizeof(s_un)) < 0)
    // 监听
    if (listen(sock_fd, IVSHMEM_SERVER_LISTEN_BACKLOG) < 0)

    server->sock_fd = sock_fd;
    server->shm_fd = shm_fd;
}

static int
ivshmem_server_poll_events(IvshmemServer *server)
{
    // fd 集
    fd_set fds;
    int ret = 0, maxfd;

    while (!ivshmem_server_quit) {
        // 清空 fds
        FD_ZERO(&fds);
        maxfd = 0;
        // 获取所有需要监听的 fds
        ivshmem_server_get_fds(server, &fds, &maxfd);
        // int select(int maxfd,fd_set *rdset,fd_set *wrset,fd_set *exset,struct timeval *timeout);
        // maxfd 是需要监视的最大的文件描述符值 + 1
        // rdset 是需要检测的可读文件描述符的集合
        // wrset 是需要检测的可写文件描述符的集合
        // exset 是需要检测的异常文件描述符的集合
        // timeval, 如果在这个时间内需要监视的描述符没有事件发生则函数返回，返回值为0
        // 返回触发的套接字个数
        ret = select(maxfd, &fds, NULL, NULL, NULL);

        if (ret == 0) {
            continue;
        }
        // 遍历fds, 看是哪些个触发(可能多个同时触发)
        if (ivshmem_server_handle_fds(server, &fds, maxfd) < 0) {
            fprintf(stderr, "ivshmem_server_handle_fds() failed\n");
            break;
        }
    }

    return ret;
}

void
ivshmem_server_get_fds(const IvshmemServer *server, fd_set *fds, int *maxfd)
{
    IvshmemServerPeer *peer;
    // 增加了 socket fd
    FD_SET(server->sock_fd, fds);
    if (server->sock_fd >= *maxfd) {
        *maxfd = server->sock_fd + 1;
    }
    // 所有 peer 的 fd, 刚启动时候没有
    QTAILQ_FOREACH(peer, &server->peer_list, next) {
        // 增加了
        FD_SET(peer->sock_fd, fds);
        if (peer->sock_fd >= *maxfd) {
            *maxfd = peer->sock_fd + 1;
        }
    }
}
```

`select()`：查看指定 fd_set 中 socket 状态，如果 fd_set 中有套接字**准备就绪**(触发(读、写或执行))，则会返回，返回值为**触发的套接字个数**.

`accept()`：经过 创建套接字 `socket()`, 绑定 `bind()` 以及 `listen()` 之后，将监听 socket 和客户端 socket 建立一个全新连接，并返回 client 的 socket 信息，以及新的 fd；

> 判断是否有客户端发起链接请求，一般用 `select()`，然后 `accept()`。

# QEMU 侧 socket 设备

使用 qemu 参数 `–chardev socket`, 文件使用 ivshmem server 的

```
// 一个 socket 设备
-chardev socket,path=/tmp/ivshmem_socket,id=fg-doorbell
```

qemu 通过查找 chardev 注册类型 `register_types` 会调用 `qmp_chardev_open_socket->qmp_chardev_open_socket_client->qio_channel_socket_connect_sync->socket_connect`，实现与 server 通信, 从而**建立 socket 连接**.

# server 处理连接请求

**server** 收到 socket 请求后, 调用 `ivshmem_server_handle_new_conn` 会指派给每个 vm 一个 id 号，并且将 id 号和**一系列 eventfd 文件描述符**(一个中断向量对应一个)一起发给 qemu 进程，qemu 间通过高效率的 eventfd 方式通信。

```cpp
typedef struct IvshmemServerPeer {
    // 链表
    QTAILQ_ENTRY(IvshmemServerPeer) next;
    // peer 的 socket fd
    int sock_fd;
    // peer id
    int64_t id;
    // 向量数组
    EventNotifier vectors[IVSHMEM_SERVER_MAX_VECTORS];
    // 向量数
    unsigned vectors_count;
} IvshmemServerPeer;

// contrib/ivshmem-server/ivshmem-server.c
static int
ivshmem_server_handle_new_conn(IvshmemServer *server)
{
    ...
    // 初始化连接, 返回新的fd, 用来和对端通信
    newfd = qemu_accept(server->sock_fd,
                        (struct sockaddr *)&unaddr, &unaddr_len);

    qemu_socket_set_nonblock(newfd);

    /* allocate new structure for this peer */
    peer = g_malloc0(sizeof(*peer));
    peer->sock_fd = newfd;

    for (i = 0; i < G_MAXUINT16; i++) {
        // 遍历整个server->peer_list, 看server->cur_id == peer->id
        if (ivshmem_server_search_peer(server, server->cur_id) == NULL) {
            break;
        }
        server->cur_id++;
    }
    // 新id, 从0开始
    peer->id = server->cur_id++;

    // 一个向量对应一个 eventfd
    peer->vectors_count = server->n_vectors;
    for (i = 0; i < peer->vectors_count; i++) {
        if (event_notifier_init(&peer->vectors[i], FALSE) < 0) {
            ...
        }
    }

    // 给新连接的 client(peer->sock_fd) 发送消息
    // 1. 0(协议版本号)
    // 2. peer id
    // 3. -1 并且 message 中带 server->shm_fd(server的共享内存体fd)
    if (ivshmem_server_send_initial_info(server, peer) < 0) {
        ...
    }

    // 遍历 peer_list, 给其他所有peer都发送(新peer_id, 新client的所有向量对应的eventfd)
    QTAILQ_FOREACH(other_peer, &server->peer_list, next) {
        for (i = 0; i < peer->vectors_count; i++) {
            ivshmem_server_send_one_msg(other_peer->sock_fd, peer->id,
                                        peer->vectors[i].wfd);
        }
    }

    // 给新client发送其他所有peer的(peer_id, client的所有向量对应的eventfd)
    QTAILQ_FOREACH(other_peer, &server->peer_list, next) {
        for (i = 0; i < peer->vectors_count; i++) {
            ivshmem_server_send_one_msg(peer->sock_fd, other_peer->id,
                                        other_peer->vectors[i].wfd);
        }
    }

    // 将新client的peer_id和所有向量的eventfd发给新client自己
    for (i = 0; i < peer->vectors_count; i++) {
        ivshmem_server_send_one_msg(peer->sock_fd, peer->id,
                                    event_notifier_get_fd(&peer->vectors[i]));
    }
    // 添加到server的peer链表
    QTAILQ_INSERT_TAIL(&server->peer_list, peer, next);
    return 0;
}
```

# QEMU 侧 doorbell 设备

Qemu 侧同时定义 doorbell Pci 设备, 字符设备对应上面 socket 设备:

```
// doorbell 设备
-device ivshmem-doorbell,chardev=fg-doorbell,vectors=8
```

ivshmem 初始化时通过 `ivshmem_read` 接收 server 端发来的 posn 和 efd 信息，再通过 `create_eventfd_chr_device` 创建 eventfd 字符设备。

```cpp
// hw/misc/ivshmem.c
ivshmem_doorbell_realize() -> ivshmem_common_realize()

static void ivshmem_common_realize(PCIDevice *dev, Error **errp)
{
    ......
    // plain 设备
    // mem-path=/dev/shm/shm1
    if (s->hostmem != NULL) {
        ......
        // bar2 来自于传入的参数
    // doorbell 设备
    } else {
        ......
        // 第一
        // BAR2 来自于 server shm_fd
        ivshmem_recv_setup(s, &err);
        // 第二
        qemu_chr_fe_set_handlers(&s->server_chr, ivshmem_can_receive,
                                 ivshmem_read, NULL, NULL, s, NULL, true);
        // 第三
        if (ivshmem_setup_interrupts(s, errp) < 0) {
    }
    ......
    vmstate_register_ram(s->ivshmem_bar2, DEVICE(s));
    // bar2 注册
    pci_register_bar(PCI_DEVICE(s), 2,
                     PCI_BASE_ADDRESS_SPACE_MEMORY |
                     PCI_BASE_ADDRESS_MEM_PREFETCH |
                     PCI_BASE_ADDRESS_MEM_TYPE_64,
                     s->ivshmem_bar2);
}
```

第一. 从 server 接收**初始化信息**.

```cpp
static void ivshmem_recv_setup(IVShmemState *s, Error **errp)
{
    int64_t msg;
    ...
    msg = ivshmem_recv_msg(s, &fd, &err);
    // 获取第一个信息: 版本号
    // 版本号应该是 0
    if (msg != IVSHMEM_PROTOCOL_VERSION) {
    }
    // 获取第二个信息: peer id
    msg = ivshmem_recv_msg(s, &fd, &err);
    if (fd != -1 || msg < 0 || msg > IVSHMEM_MAX_PEERS) {
    }
    // 也就是设备的 vm_id
    s->vm_id = msg;

    // 理论上应该获取第三个信息: -1和(带有server的shm_fd)
    // server的shm_fd在fd变量中
    do {
        msg = ivshmem_recv_msg(s, &fd, &err);
        process_msg(s, msg, fd, &err);
    // -1 信息就直接退出
    } while (msg != -1);
}
```

`process_msg` 针对 (-1) 调用 `process_msg_shmem` 处理 sever 的 shm_fd

```cpp
static void process_msg_shmem(IVShmemState *s, int fd, Error **errp)
{
    struct stat buf;
    size_t size;
    // bar2 不应该已经初始化了 
    if (s->ivshmem_bar2) {
        return;
    }
    // 获取server shm_fd的信息
    if (fstat(fd, &buf) < 0) {
    }
    // 文件大小,就是shm大小
    size = buf.st_size;

    // mmap这个shm_fd到BAR2
    // 也就是说doorbell设备的BAR2是server shm_fd
    if (!memory_region_init_ram_from_fd(&s->server_bar2, OBJECT(s), "ivshmem.bar2", size, RAM_SHARED, fd, 0, errp)) {
        return;
    }

    s->ivshmem_bar2 = &s->server_bar2;
}
```

第二. 





在 qemu 一端通过 `–chardev socket` 建立 **socket 连接**，并通过 `-device ivshmem` 建立**共享内存设备**.

```
// doorbell 设备, 是个 socket 设备
-chardev socket,path=/tmp/ivshmem_socket,id=fg-doorbell \
// doorbell 设备
-device ivshmem-doorbell,chardev=fg-doorbell,vectors=8

// plain 设备, 是个 memory 设备
-object memory-backend-file,size=8M,share=true,mem-path=/dev/shm/shm3,id=shm3 \
-device ivshmem-plain,memdev=shm3,bus=pci.0,addr=0x1f,master=on
```












`ivshmem-doorbell` 设备支持**多个中断向量**, **ivshmem-server** 会为 ivshmem 虚拟 PCI 设备支持的**每个中断向量**创建一个 **eventfd**, 并将**共享内存**以及为**所有客户端中断向量**所创建的 **eventfd** 都通过 **SCM_RIGHTS** 机制传递给**所有客户端进程**. 这样所有的 peer 便都具备了独立的**两两之间的通知通道**. 之后在虚拟机内通过触发 ivshmem 虚拟 PCI 设备的 DOORBELL 寄存器的写入, 虚拟机的 QEMU 进程便会通过 **DOORBELL 寄存器**中的 `peer_id` 和**中断向量号**来找到**相应的 eventfd**, 从而通知到对端的 QEMU 进程来产生相应的 PCI 中断.

要使用中断机制, **用户态程序**是无能为力的, 需要编写相应的 PCI 驱动来实现. 本文通过一个简单的 PCI 驱动示例来说明 `ivshmem-doorbell` 的 MSI-X 中断机制的使用.

`ivpci.c` 代码如下:

```cpp
#include <linux/init.h>
#include <linux/module.h>
#include <linux/kernel.h>
#include <linux/version.h>
#include <linux/err.h>
#include <linux/fs.h>
#include <linux/mm.h>
#include <linux/pci.h>
#include <linux/pci_regs.h>
#include <linux/cdev.h>
#include <linux/device.h>
#include <linux/uaccess.h>
#include <linux/interrupt.h>
#include <linux/ioctl.h>
#include <linux/sched.h>
#include <linux/wait.h>


#define DRV_NAME        "ivpci"
#define DRV_VERSION     "0.1"
#define PFX             "[IVPCI] "
#define DRV_FILE_FMT    DRV_NAME"%d"

#define IVPOSITION_OFF  0x08    /* VM ID */
#define DOORBELL_OFF    0x0c    /* Doorbell */

#define IOCTL_MAGIC         ('f')
#define IOCTL_RING          _IOW(IOCTL_MAGIC, 1, u32)
#define IOCTL_WAIT          _IO(IOCTL_MAGIC, 2)
#define IOCTL_IVPOSITION    _IOR(IOCTL_MAGIC, 3, u32)


static int g_max_devices = 2;
MODULE_PARM_DESC(g_max_devices, "number of devices can be supported");
module_param(g_max_devices, int, 0400);


struct ivpci_private {
    struct pci_dev      *dev;
    struct cdev         cdev;
    int                 minor;

    u8                  revision;
    u32                 ivposition;

    u8 __iomem          *base_addr;
    u8 __iomem          *regs_addr;

    unsigned int        bar0_addr;
    unsigned int        bar0_len;
    unsigned int        bar1_addr;
    unsigned int        bar1_len;
    unsigned int        bar2_addr;
    unsigned int        bar2_len;

    char                (*msix_names)[256];
    struct msix_entry   *msix_entries;
    int                 nvectors;
};



static int event_toggle;
DECLARE_WAIT_QUEUE_HEAD(wait_queue);


/* store major device number shared by all ivshmem devices */
static dev_t g_ivpci_devno;
static int  g_ivpci_major;

static struct class *g_ivpci_class;

/* number of devices owned by this driver */
static int g_ivpci_count;

static struct ivpci_private *g_ivpci_devs;


static struct pci_device_id ivpci_id_table[] = {
    { 0x1af4, 0x1110, PCI_ANY_ID, PCI_ANY_ID, 0, 0, 0},
    { 0 },
};
MODULE_DEVICE_TABLE(pci, ivpci_id_table);


static struct ivpci_private *ivpci_get_private(void)
{
    int i;

    for (i = 0; i < g_max_devices; i++) {
        if (g_ivpci_devs[i].dev == NULL) {
            return &g_ivpci_devs[i];
        }
    }

    return NULL;
}

static struct ivpci_private *ivpci_find_private(int minor)
{
    int i;
    for (i = 0; i < g_max_devices; i++) {
        if (g_ivpci_devs[i].dev != NULL && g_ivpci_devs[i].minor == minor) {
            return &g_ivpci_devs[i];
        }
    }

    return NULL;
}


static irqreturn_t ivpci_interrupt(int irq, void *dev_id)
{
    struct ivpci_private *ivpci_dev = dev_id;

    if (unlikely(ivpci_dev == NULL)) {
        return IRQ_NONE;
    }

    dev_info(&ivpci_dev->dev->dev, PFX "interrupt: %d\n", irq);

    event_toggle = 1;
    wake_up_interruptible(&wait_queue);

    return IRQ_HANDLED;
}


static int ivpci_request_msix_vectors(struct ivpci_private *ivpci_dev, int n)
{
    int ret, i;

    ret = -EINVAL;

    dev_info(&ivpci_dev->dev->dev, PFX "request msi-x vectors: %d\n", n);

    ivpci_dev->nvectors = n;

    ivpci_dev->msix_entries = kmalloc(n * sizeof(struct msix_entry),
            GFP_KERNEL);
    if (ivpci_dev->msix_entries == NULL) {
        ret = -ENOMEM;
        goto error;
    }

    ivpci_dev->msix_names = kmalloc(n * sizeof(*ivpci_dev->msix_names),
            GFP_KERNEL);
    if (ivpci_dev->msix_names == NULL) {
        ret = -ENOMEM;
        goto free_entries;
    }

    for (i = 0; i < n; i++) {
        ivpci_dev->msix_entries[i].entry = i;
    }

    ret = pci_enable_msix_exact(ivpci_dev->dev, ivpci_dev->msix_entries, n);
    if (ret) {
        dev_err(&ivpci_dev->dev->dev, PFX "unable to enable msix: %d\n", ret);
        goto free_names;
    }

    for (i = 0; i < ivpci_dev->nvectors; i++) {
        snprintf(ivpci_dev->msix_names[i], sizeof(*ivpci_dev->msix_names),
                "%s%d-%d", DRV_NAME, ivpci_dev->minor, i);

        ret = request_irq(ivpci_dev->msix_entries[i].vector,
                ivpci_interrupt, 0, ivpci_dev->msix_names[i], ivpci_dev);

        if (ret) {
            dev_err(&ivpci_dev->dev->dev, PFX "unable to allocate irq for " \
                    "msix entry %d with vector %d\n", i,
                    ivpci_dev->msix_entries[i].vector);
            goto release_irqs;
        }

        dev_info(&ivpci_dev->dev->dev,
                PFX "irq for msix entry: %d, vector: %d\n",
                i, ivpci_dev->msix_entries[i].vector);
    }

    return 0;

release_irqs:
    for ( ; i > 0; i--) {
        free_irq(ivpci_dev->msix_entries[i - 1].vector, ivpci_dev);
    }
    pci_disable_msix(ivpci_dev->dev);

free_names:
    kfree(ivpci_dev->msix_names);

free_entries:
    kfree(ivpci_dev->msix_entries);

error:
    return ret;
}

static void ivpci_free_msix_vectors(struct ivpci_private *ivpci_dev)
{
    int i;

    for (i = ivpci_dev->nvectors; i > 0; i--) {
        free_irq(ivpci_dev->msix_entries[i - 1].vector, ivpci_dev);
    }
    pci_disable_msix(ivpci_dev->dev);

    kfree(ivpci_dev->msix_names);
    kfree(ivpci_dev->msix_entries);
}


static int ivpci_open(struct inode *inode, struct file *filp)
{
    int minor = iminor(inode);
    struct ivpci_private *ivpci_dev;

    ivpci_dev = ivpci_find_private(minor);
    filp->private_data = (void *) ivpci_dev;
    BUG_ON(filp->private_data == NULL);

    dev_info(&ivpci_dev->dev->dev, PFX "open ivpci\n");

    return 0;
}


static int ivpci_mmap(struct file *filp, struct vm_area_struct *vma)
{
    int ret;
    unsigned long len;
    unsigned long off;
    unsigned long start;
    struct ivpci_private *ivpci_dev;

    ivpci_dev = (struct ivpci_private *)filp->private_data;
    BUG_ON(ivpci_dev == NULL);
    BUG_ON(ivpci_dev->base_addr == NULL);

    dev_info(&ivpci_dev->dev->dev, PFX "mmap ivpci bar2\n");

    /* `vma->vm_start` and `vma->vm_end` had aligned to page size */
    WARN_ON(offset_in_page(vma->vm_start));
    WARN_ON(offset_in_page(vma->vm_end));

    off = vma->vm_pgoff << PAGE_SHIFT;
    start = ivpci_dev->bar2_addr;

    /* Align up to page size. */
    len = PAGE_ALIGN((start & ~PAGE_MASK) + ivpci_dev->bar2_len);
    start &= PAGE_MASK;

    dev_info(&ivpci_dev->dev->dev, PFX "mmap vma pgoff: %lu, 0x%0lx - 0x%0lx," \
            " aligned length: %lu\n", vma->vm_pgoff, vma->vm_start,
            vma->vm_end, len);

    if (vma->vm_end - vma->vm_start + off > len) {
        dev_err(&ivpci_dev->dev->dev,
                PFX "mmap overflow the end, %lu - %lu + %lu > %lu",
                vma->vm_end, vma->vm_start, off, len);
        ret = -EINVAL;
        goto error;
    }

    off += start;
    vma->vm_pgoff = off >> PAGE_SHIFT;
    vma->vm_flags |= VM_IO|VM_SHARED|VM_DONTEXPAND|VM_DONTDUMP;

    if (io_remap_pfn_range(vma, vma->vm_start, off >> PAGE_SHIFT,
                vma->vm_end - vma->vm_start, vma->vm_page_prot)) {
        dev_err(&ivpci_dev->dev->dev, PFX "mmap bar2 failed\n");
        ret = -ENXIO;
        goto error;
    }

    ret = 0;

error:
    return ret;
}


static ssize_t ivpci_read(struct file *filp, char *buffer, size_t len,
        loff_t *poffset)
{
    int bytes_read = 0;
    unsigned long offset;
    struct ivpci_private *ivpci_dev;

    ivpci_dev = (struct ivpci_private *)filp->private_data;

    BUG_ON(ivpci_dev == NULL);
    BUG_ON(ivpci_dev->base_addr == NULL);


    offset = *poffset;
    dev_info(&ivpci_dev->dev->dev, PFX "read ivpci bar2, offset: %lu\n",
            offset);

    /* beyoud the end */
    if (len > ivpci_dev->bar2_len - offset) {
        len = ivpci_dev->bar2_len - offset;
    }

    if (len == 0) {
        return 0;
    }

    bytes_read = copy_to_user(buffer, (char *)ivpci_dev->base_addr + offset,
            len);
    if (bytes_read > 0) {
        dev_err(&ivpci_dev->dev->dev,
                PFX "read ivpci bar2, copy_to_user() failed: %d\n", bytes_read);
        return -EFAULT;
    }

    *poffset += len;
    return len;
}


static long ivpci_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)
{
    int ret;
    struct ivpci_private *ivpci_dev;
    u16 ivposition;
    u16 vector;
    u32 value;

    ivpci_dev = (struct ivpci_private *)filp->private_data;

    BUG_ON(ivpci_dev == NULL);
    BUG_ON(ivpci_dev->base_addr == NULL);

    switch (cmd) {
    case IOCTL_RING:
        if (copy_from_user(&value, (u32 *) arg, sizeof(value)) ) {
            dev_err(&ivpci_dev->dev->dev, PFX "copy from user failed");
            return -1;
        }
        vector = value & 0xffff;
        ivposition = value & 0xffff0000 >> 16;
        dev_info(&ivpci_dev->dev->dev,
                PFX "ring doorbell: value: %u(0x%x), vector: %u, peer id: %u\n",
                value, value, vector, ivposition);
        writel(value & 0xffffffff, ivpci_dev->regs_addr + DOORBELL_OFF);
        break;

    case IOCTL_WAIT:
        dev_info(&ivpci_dev->dev->dev, PFX "wait for interrupt\n");
        ret = wait_event_interruptible(wait_queue, (event_toggle == 1));
        if (ret == 0) {
            dev_info(&ivpci_dev->dev->dev, PFX "wakeup\n");
            event_toggle = 0;
        } else if (ret == -ERESTARTSYS) {
            dev_err(&ivpci_dev->dev->dev, PFX "interrupted by signal\n");
            return ret;
        } else {
            dev_err(&ivpci_dev->dev->dev, PFX "unknown failed: %d\n", ret);
            return ret;
        }
        break;

    case IOCTL_IVPOSITION:
        dev_info(&ivpci_dev->dev->dev, PFX "get ivposition: %u\n",
                ivpci_dev->ivposition);
        if (copy_to_user((u32 *) arg, &ivpci_dev->ivposition, sizeof(u32))) {
            dev_err(&ivpci_dev->dev->dev, PFX "copy to user failed");
            return -1;
        }
        break;

    default:
        dev_err(&ivpci_dev->dev->dev, PFX "bad ioctl command: %d\n", cmd);
        return -1;
    }

    return 0;
}


static long ivpci_write(struct file *filp, const char *buffer, size_t len,
        loff_t *poffset)
{
    int bytes_written = 0;
    unsigned long offset;
    struct ivpci_private *ivpci_dev;

    ivpci_dev = (struct ivpci_private *)filp->private_data;

    BUG_ON(ivpci_dev == NULL);
    BUG_ON(ivpci_dev->base_addr == NULL);

    dev_info(&ivpci_dev->dev->dev, PFX "write ivpci bar2\n");

    offset = *poffset;

    if (len > ivpci_dev->bar2_len - offset) {
        len = ivpci_dev->bar2_len - offset;
    }

    if (len == 0) {
        return 0;
    }

    bytes_written = copy_from_user(ivpci_dev->base_addr + offset, buffer, len);
    if (bytes_written > 0) {
        return -EFAULT;
    }

    *poffset += len;
    return len;
}


static loff_t ivpci_lseek(struct file *filp, loff_t offset, int origin)
{
    loff_t retval = -1;
    struct ivpci_private *ivpci_dev;

    ivpci_dev = (struct ivpci_private *)filp->private_data;

    BUG_ON(ivpci_dev == NULL);
    BUG_ON(ivpci_dev->base_addr == NULL);

    dev_info(&ivpci_dev->dev->dev,
            PFX "lseek ivpci bar2, offset: %llu, origin: %d\n", offset, origin);

    switch (origin) {
    case 1:
        offset += filp->f_pos;
        /* fall through */
    case 0:
        retval = offset;
        if (offset > ivpci_dev->bar2_len) {
            offset = ivpci_dev->bar2_len;
        }
        filp->f_pos = offset;
    }
    return retval;
}


static int ivpci_release(struct inode *inode, struct file *filp)
{
    struct ivpci_private *ivpci_dev;
    ivpci_dev = (struct ivpci_private *)filp->private_data;

    BUG_ON(ivpci_dev == NULL);
    BUG_ON(ivpci_dev->base_addr == NULL);

    dev_info(&ivpci_dev->dev->dev, PFX "release ivpci\n");

    return 0;
}


static struct file_operations ivpci_ops = {
    .owner          = THIS_MODULE,
    .open           = ivpci_open,
    .mmap           = ivpci_mmap,
    .unlocked_ioctl = ivpci_ioctl,
    .read           = ivpci_read,
    .write          = ivpci_write,
    .llseek         = ivpci_lseek,
    .release        = ivpci_release,
};


static int ivpci_probe(struct pci_dev *pdev, const struct pci_device_id *id)
{
    int ret;
    struct ivpci_private *ivpci_dev;
    dev_t devno;

    dev_info(&pdev->dev, PFX "probing for device: %s\n", pci_name(pdev));

    if (g_ivpci_count >= g_max_devices) {
        dev_err(&pdev->dev, PFX "reach the maxinum number of devices, " \
                "please adapt the `g_max_devices` value, reload the driver\n");
        ret = -1;
        goto out;
    }

    ret = pci_enable_device(pdev);
    if (ret < 0) {
        dev_err(&pdev->dev, PFX "unable to enable device: %d\n", ret);
        goto out;
    }

    /* Reserved PCI I/O and memory resources for this device */
    ret = pci_request_regions(pdev, DRV_NAME);
    if (ret < 0) {
        dev_err(&pdev->dev, PFX "unable to reserve resources: %d\n", ret);
        goto disable_device;
    }

    ivpci_dev = ivpci_get_private();
    BUG_ON(ivpci_dev == NULL);

    pci_read_config_byte(pdev, PCI_REVISION_ID, &ivpci_dev->revision);

    dev_info(&pdev->dev, PFX "device %d:%d, revision: %d\n", g_ivpci_major,
            ivpci_dev->minor, ivpci_dev->revision);

    /* Pysical address of BAR0, BAR1, BAR2 */
    ivpci_dev->bar0_addr = pci_resource_start(pdev, 0);
    ivpci_dev->bar0_len = pci_resource_len(pdev, 0);
    ivpci_dev->bar1_addr = pci_resource_start(pdev, 1);
    ivpci_dev->bar1_len = pci_resource_len(pdev, 1);
    ivpci_dev->bar2_addr = pci_resource_start(pdev, 2);
    ivpci_dev->bar2_len = pci_resource_len(pdev, 2);

    dev_info(&pdev->dev, PFX "BAR0: 0x%0x@0x%0x\n", ivpci_dev->bar0_addr,
            ivpci_dev->bar0_len);
    dev_info(&pdev->dev, PFX "BAR1: 0x%0x@0x%0x\n", ivpci_dev->bar1_addr,
            ivpci_dev->bar1_len);
    dev_info(&pdev->dev, PFX "BAR2: 0x%0x@0x%0x\n", ivpci_dev->bar2_addr,
            ivpci_dev->bar2_len);

    //ivpci_dev->regs_addr = ioremap(ivpci_dev->bar0_addr, ivpci_dev->bar0_len);
    ivpci_dev->regs_addr = pci_iomap(pdev, 0, 0x100);
    if (!ivpci_dev->regs_addr) {
        dev_err(&pdev->dev, PFX "unable to ioremap bar0, size: %d\n",
                ivpci_dev->bar0_len);
        goto release_regions;
    }

    //ivpci_dev->base_addr = ioremap(ivpci_dev->bar2_addr, ivpci_dev->bar2_len);
    ivpci_dev->base_addr = pci_iomap(pdev, 2, 0);
    if (!ivpci_dev->base_addr) {
        dev_err(&pdev->dev, PFX "unable to ioremap bar2, size: %d\n",
                ivpci_dev->bar2_len);
        goto iounmap_bar0;
    }
    dev_info(&pdev->dev, PFX "BAR2 map: %p\n", ivpci_dev->base_addr);

    /*
     * Create character device file.
     */
    cdev_init(&ivpci_dev->cdev, &ivpci_ops);
    ivpci_dev->cdev.owner = THIS_MODULE;

    devno = MKDEV(g_ivpci_major, ivpci_dev->minor);
    ret = cdev_add(&ivpci_dev->cdev, devno, 1);
    if (ret < 0) {
        dev_err(&pdev->dev, PFX "unable to add chrdev %d:%d to system: %d\n",
                g_ivpci_major, ivpci_dev->minor, ret);
        goto iounmap_bar2;
    }

    if (device_create(g_ivpci_class, NULL, devno, NULL, DRV_FILE_FMT,
                ivpci_dev->minor) == NULL)
    {
        dev_err(&pdev->dev, PFX "unable to create device file: %d:%d\n",
                g_ivpci_major, ivpci_dev->minor);
        goto delete_chrdev;
    }

    ivpci_dev->dev = pdev;
    pci_set_drvdata(pdev, ivpci_dev);

    if (ivpci_dev->revision == 1) {
        /* Only process the MSI-X interrupt. */
        ivpci_dev->ivposition = ioread32(ivpci_dev->regs_addr + IVPOSITION_OFF);

        dev_info(&pdev->dev, PFX "device ivposition: %u, MSI-X: %s\n",
                ivpci_dev->ivposition,
                (ivpci_dev->ivposition == 0) ? "no": "yes");

        if (ivpci_dev->ivposition != 0) {
            ret = ivpci_request_msix_vectors(ivpci_dev, 4);
            if (ret != 0) {
                goto destroy_device;
            }
        }
    }

    g_ivpci_count++;
    dev_info(&pdev->dev, PFX "device probed: %s\n", pci_name(pdev));
    return 0;

destroy_device:
    devno = MKDEV(g_ivpci_major, ivpci_dev->minor);
    device_destroy(g_ivpci_class, devno);
    ivpci_dev->dev = NULL;

delete_chrdev:
    cdev_del(&ivpci_dev->cdev);

iounmap_bar2:
    iounmap(ivpci_dev->base_addr);

iounmap_bar0:
    iounmap(ivpci_dev->regs_addr);

release_regions:
    pci_release_regions(pdev);

disable_device:
    pci_disable_device(pdev);

out:
    pci_set_drvdata(pdev, NULL);
    return ret;
}


static void ivpci_remove(struct pci_dev *pdev)
{
    int devno;
    struct ivpci_private *ivpci_dev;

    dev_info(&pdev->dev, PFX "removing ivshmem device: %s\n", pci_name(pdev));

    ivpci_dev = pci_get_drvdata(pdev);
    BUG_ON(ivpci_dev == NULL);

    ivpci_free_msix_vectors(ivpci_dev);

    ivpci_dev->dev = NULL;

    devno = MKDEV(g_ivpci_major, ivpci_dev->minor);
    device_destroy(g_ivpci_class, devno);

    cdev_del(&ivpci_dev->cdev);

    iounmap(ivpci_dev->base_addr);
    iounmap(ivpci_dev->regs_addr);

    pci_release_regions(pdev);
    pci_disable_device(pdev);
    pci_set_drvdata(pdev, NULL);
}


static struct pci_driver ivpci_driver = {
    .name       = DRV_NAME,
    .id_table   = ivpci_id_table,
    .probe      = ivpci_probe,
    .remove     = ivpci_remove,
};


static int __init ivpci_init(void)
{
    int ret, i, minor;

    pr_info(PFX "*********************************************************\n");
    pr_info(PFX "module loading\n");

    ret = alloc_chrdev_region(&g_ivpci_devno, 0, g_max_devices, DRV_NAME);
    if (ret < 0) {
        pr_err(PFX "unable to allocate major number: %d\n", ret);
        goto out;
    }

    g_ivpci_devs = kzalloc(sizeof(struct ivpci_private) * g_max_devices,
            GFP_KERNEL);
    if (g_ivpci_devs == NULL) {
        goto unregister_chrdev;
    }

    minor = MINOR(g_ivpci_devno);
    for (i = 0; i < g_max_devices; i++) {
        g_ivpci_devs[i].minor = minor++;
    }

    g_ivpci_class = class_create(THIS_MODULE, DRV_NAME);
    if (g_ivpci_class == NULL) {
        pr_err(PFX "unable to create the struct class\n");
        goto free_devs;
    }

    g_ivpci_major = MAJOR(g_ivpci_devno);
    pr_info(PFX "major: %d, minor: %d\n", g_ivpci_major, MINOR(g_ivpci_devno));

    ret = pci_register_driver(&ivpci_driver);
    if (ret < 0) {
        pr_err(PFX "unable to register driver: %d\n", ret);
        goto destroy_class;
    }

    pr_info(PFX "module loaded\n");
    return 0;

destroy_class:
    class_destroy(g_ivpci_class);

free_devs:
    kfree(g_ivpci_devs);

unregister_chrdev:
    unregister_chrdev_region(g_ivpci_devno, g_max_devices);

out:
    return -1;
}

static void __exit ivpci_exit(void)
{
    pci_unregister_driver(&ivpci_driver);

    class_destroy(g_ivpci_class);

    kfree(g_ivpci_devs);

    unregister_chrdev_region(g_ivpci_devno, g_max_devices);

    pr_info(PFX "module unloaded\n");
    pr_info(PFX "*********************************************************\n");
}


/************************************************
 * Just for eliminating the compiling warnings.
 ************************************************/
#define my_module_init(initfn)					\
	static inline initcall_t __inittest(void)		\
	{ return initfn; }					\
	int init_module(void) __cold __attribute__((alias(#initfn)));

#define my_module_exit(exitfn)					\
	static inline exitcall_t __exittest(void)		\
	{ return exitfn; }					\
	void cleanup_module(void) __cold __attribute__((alias(#exitfn)));

my_module_init(ivpci_init);
my_module_exit(ivpci_exit);

MODULE_AUTHOR("flygoast@126.com");
MODULE_LICENSE("GPL");
MODULE_DESCRIPTION("Demo PCI driver for ivshmem device");
MODULE_VERSION(DRV_VERSION);
```

简单解释一下代码逻辑. **驱动被加载**时, 会调用 `ivpci_init`. 我们希望**所有的 ivshmem 设备**使用**相同的设备 major 号**, 因而在这里调用 `alloc_chrdev_region` 申请足够的 major 和 minor 号, 接着创建出足够的 ivshmem 设备所需的数据结构 `struct ivpci_private`, 并将 minor 号存储在这些结构中. 接着, 创建**设备文件 class**, 然后**注册** ivshmem 设备的**驱动结构**.

当 ivshmem 设备被检测到, probe 函数 `ivpci_probe` 会被调用. 我们调用 `pci_enable_device` **启用该设备**, 并将 PCI 设备的 **3 个 BAR 空间**映射到内存空间, 方便后续访问. 然后**创建对应的设备文件**, 给**用户态程序**提供访问接口.

QEMU-2.6.0 之后, ivshmem 虚拟 PCI 设备配置空间的 revision 字段为 1, 这样才支持 MSI-X 中断机制, 所以我们只处理 revision 为 1 的 ivshmem 设备.

接下来, 函数 `ivpci_request_msix_vectors` 调用 `pci_enable_msix` 分配中断向量, 然后依次调用 `request_irq` 注册我们的中断处理程序: `ivpci_interrupt`.

示例程序里简单使用 **waitqueue** 来实现**等待和通知**. 当**用户态程序**以 `IOCTL_WAIT` 命令调用 ioctl 时, 就让该进程在 waitqueue 上进行**等待**. 当**中断触发**时, 我们的中断处理程序会调用 `wake_up_interruptible` 唤醒 waitqueue 上所等待的进程.

**设备文件**的文件操作的 mmap 实现可以把设备 **BAR2** 空间映射到用户空间, 与上一篇介绍的用户程序直接 mmap sysfs 的 resource2 文件作用是一致的.

示例的**用户态代码**, `ioctl.c` 如下:

```cpp
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <unistd.h>
#include <errno.h>
#include <fcntl.h>
#include <sys/ioctl.h>
#include <linux/types.h>
#include <assert.h>


#define IOCTL_MAGIC         ('f')
#define IOCTL_RING          _IOW(IOCTL_MAGIC, 1, __u32)
#define IOCTL_WAIT          _IO(IOCTL_MAGIC, 2)
#define IOCTL_IVPOSITION    _IOR(IOCTL_MAGIC, 3, __u32)

static void usage(void)
{
    printf("Usage: \n"  \
           "  ioctl <devfile> ivposition\n"    \
           "  ioctl <devfile> wait\n"          \
           "  ioctl <devfile> ring <peer_id> <vector_id>\n");
}


int main(int argc, char **argv)
{
    int fd, arg, ret, vector, peer;

    if (argc < 3) {
        usage();
        return -1;
    }

    fd = open(argv[1], O_RDWR);
    assert(fd != -1);

    ret = ioctl(fd, IOCTL_IVPOSITION, &peer);
    printf("IVPOSITION: %d\n", peer);

    if (strcmp(argv[2], "ivposition") == 0) {
        return 0;

    } if (strcmp(argv[2], "wait") == 0) {
        if (argc != 3) {
            usage();
            return -1;
        }
        printf("wait:\n");
        ret = ioctl(fd, IOCTL_WAIT, &arg);

    } else if (strcmp(argv[2], "ring") == 0) {
        if (argc != 5) {
            usage();
            return -1;
        }
        printf("ring:\n");
        peer = atoi(argv[3]);
        vector = atoi(argv[4]);

        arg = ((peer & 0xffff) << 16) | (vector & 0xffff);
        printf("arg: 0x%x\n", arg);

        ret = ioctl(fd, IOCTL_RING, &arg);

    } else {
        printf("Invalid command: %s\n", argv[2]);
        usage();
        return -1;
    }

    printf("ioctl finished, returned value: %d\n", ret);

    close(fd);

    return 0;
}
```

Makefile 如下:

```makefile
CONFIG_MODULE_SIG=n

obj-m += ivpci.o
all:
	make -C /lib/modules/$(shell uname -r)/build M=$(PWD) modules
	gcc -g -O0 ioctl.c -o ioctl
clean:
	make -C /lib/modules/$(shell uname -r)/build M=$(PWD) clean
	rm -rf ioctl
```

接下来, 在宿主机上运行 `ivshmem-server`:

```
sudo -u qemu /tmp/ivshmem-server -l 4M -M fg-doorbell -n 8 -F -v

cd contrib/ivshmem-server
./ivshmem-server -l 4M -M fg-doorbell -n 8 -F -v
*** Example code, do not use in production ***
Using POSIX shared memory: fg-doorbell
create & bind socket /tmp/ivshmem_socket
```

由于 `ivshmem-server` 分配的第一个 `peer_id` 为 0, 而当 ivshmem 设备的 **ivposition** 为 **0** 时, **无法判断**是不支持 `MSI-X` 中断还是分配的 `peer_id` 为 0. 所以我们可以先调用 ivshmem-client 程序来先连接 `ivshmem-server` 来占据 0 号 `peer_id`:

```
# cd build/contrib/ivshmem-client
# ./ivshmem-client
dump: dump peers (including us)
int <peer> <vector>: notify one vector on a peer
int <peer> all: notify all vectors of a peer
int all: notify all vectors of all peers (excepting us)
cmd> listen on server socket 3
dump
our_id = 0
  vector 0 is enabled (fd=5)
  vector 1 is enabled (fd=6)
  vector 2 is enabled (fd=7)
  vector 3 is enabled (fd=8)
  vector 4 is enabled (fd=9)
  vector 5 is enabled (fd=10)
  vector 6 is enabled (fd=11)
  vector 7 is enabled (fd=12)
cmd>
```

`ivshmem-server` 会多 log 是:

```
accept()=5
new peer id = 0
peer->sock_fd=5
```

接下来启动两台虚拟机, 分别给两台虚拟机各动态添加一个 `ivshmem-plain` 和一个 `ivshmem-doorbell` 设备:

```
# nc -U /tmp/mon_test

object_add memory-backend-file,size=8M,share=true,mem-path=/dev/shm/shm3,id=shm3
device_add ivshmem-plain,memdev=shm3,bus=pci.0,addr=0x1f,master=on

chardev-add socket,path=/tmp/ivshmem_socket,id=fg-doorbell
device_add ivshmem-doorbell,chardev=fg-doorbell,vectors=8
```

`ivshmem-server` 会多 log 是(分别添加 `ivshmem-doorbell` 设备时候增加的 log):

```
accept()=14
new peer id = 1
peer->sock_fd=5
peer->sock_fd=14

accept()=23
new peer id = 2
peer->sock_fd=5
peer->sock_fd=14
peer->sock_fd=23
```

在两台虚拟机中都装载驱动模块:

```
insmod ./ivpci.ko
```

dmesg 查看加载信息:

```
[  154.223277] pci 0000:00:1f.0: [1af4:1110] type 00 class 0x050000
[  154.223663] pci 0000:00:1f.0: reg 0x10: [mem 0x00000000-0x000000ff]
[  154.224146] pci 0000:00:1f.0: reg 0x18: [mem 0x00000000-0x007fffff 64bit pref]
[  154.225810] pci 0000:00:1f.0: BAR 2: assigned [mem 0x380000800000-0x380000ffffff 64bit pref]
[  154.226072] pci 0000:00:1f.0: BAR 0: assigned [mem 0x81141100-0x811411ff]
[  163.622405] pci 0000:00:07.0: [1af4:1110] type 00 class 0x050000
[  163.622749] pci 0000:00:07.0: reg 0x10: [mem 0x00000000-0x000000ff]
[  163.622911] pci 0000:00:07.0: reg 0x14: [mem 0x00000000-0x00000fff]
[  163.623162] pci 0000:00:07.0: reg 0x18: [mem 0x00000000-0x003fffff 64bit pref]
[  163.625931] pci 0000:00:07.0: BAR 2: assigned [mem 0x380000400000-0x3800007fffff 64bit pref]
[  163.627021] pci 0000:00:07.0: BAR 1: assigned [mem 0x81144000-0x81144fff]
[  163.627212] pci 0000:00:07.0: BAR 0: assigned [mem 0x81141200-0x811412ff]
[  172.057362] ivpci: loading out-of-tree module taints kernel.
[  172.057431] ivpci: module verification failed: signature and/or required key missing - tainting kernel
[  172.057882] [IVPCI] *********************************************************
[  172.057884] [IVPCI] module loading
[  172.057922] [IVPCI] major: 241, minor: 0
[  172.057962] ivpci 0000:00:1f.0: [IVPCI] probing for device: 0000:00:1f.0
[  172.058124] ivpci 0000:00:1f.0: [IVPCI] device 241:0, revision: 1
[  172.058128] ivpci 0000:00:1f.0: [IVPCI] BAR0: 0x81141100@0x100
[  172.058132] ivpci 0000:00:1f.0: [IVPCI] BAR1: 0x0@0x0
[  172.058135] ivpci 0000:00:1f.0: [IVPCI] BAR2: 0x800000@0x800000
[  172.058169] ivpci 0000:00:1f.0: [IVPCI] BAR2 map: 00000000c306fc7d
[  172.058547] ivpci 0000:00:1f.0: [IVPCI] device ivposition: 0, MSI-X: no
[  172.058551] ivpci 0000:00:1f.0: [IVPCI] device probed: 0000:00:1f.0
[  172.058600] ivpci 0000:00:07.0: [IVPCI] probing for device: 0000:00:07.0
[  172.058700] ivpci 0000:00:07.0: [IVPCI] device 241:1, revision: 1
[  172.058703] ivpci 0000:00:07.0: [IVPCI] BAR0: 0x81141200@0x100
[  172.058707] ivpci 0000:00:07.0: [IVPCI] BAR1: 0x81144000@0x1000
[  172.058709] ivpci 0000:00:07.0: [IVPCI] BAR2: 0x400000@0x400000
[  172.058730] ivpci 0000:00:07.0: [IVPCI] BAR2 map: 00000000fb21d139
[  172.058788] ivpci 0000:00:07.0: [IVPCI] device ivposition: 2, MSI-X: yes
[  172.058791] ivpci 0000:00:07.0: [IVPCI] request msi-x vectors: 4
[  172.061168] ivpci 0000:00:07.0: [IVPCI] irq for msix entry: 0, vector: 30
[  172.061339] ivpci 0000:00:07.0: [IVPCI] irq for msix entry: 1, vector: 31
[  172.061498] ivpci 0000:00:07.0: [IVPCI] irq for msix entry: 2, vector: 32
[  172.061654] ivpci 0000:00:07.0: [IVPCI] irq for msix entry: 3, vector: 33
[  172.061656] ivpci 0000:00:07.0: [IVPCI] device probed: 0000:00:07.0
[  172.063914] [IVPCI] module loaded
```

可以看到两个 ivshmem 设备都被检测到, 驱动只对 `0000:00:07.0` 设备进行了中断处理.

通过 lspci 查看该设备, 可以看到 `MSI-X` 的 Capabilities 结构显示支持 `MSI-X` 机制

```
# lspci
00:07.0 RAM memory: Red Hat, Inc. Inter-VM shared memory (rev 01)
00:1f.0 RAM memory: Red Hat, Inc. Inter-VM shared memory (rev 01)

# lspci -vvv -s 00:07.0
00:07.0 RAM memory: Red Hat, Inc. Inter-VM shared memory (rev 01)
        Subsystem: Red Hat, Inc. QEMU Virtual Machine
        Physical Slot: 7
        Control: I/O+ Mem+ BusMaster- SpecCycle- MemWINV- VGASnoop- ParErr- Stepping- SERR- FastB2B- DisINTx+
        Status: Cap+ 66MHz- UDF- FastB2B- ParErr- DEVSEL=fast >TAbort- <TAbort- <MAbort- >SERR- <PERR- INTx-
        Region 0: Memory at 81141200 (32-bit, non-prefetchable) [size=256]
        Region 1: Memory at 81144000 (32-bit, non-prefetchable) [size=4K]
        Region 2: Memory at 380000400000 (64-bit, prefetchable) [size=4M]
        Capabilities: [40] MSI-X: Enable+ Count=8 Masked-
                Vector table: BAR=1 offset=00000000
                PBA: BAR=1 offset=00000800
        Kernel driver in use: ivpci

# lspci -vvv -s 00:1f.0
00:1f.0 RAM memory: Red Hat, Inc. Inter-VM shared memory (rev 01)
        Subsystem: Red Hat, Inc. QEMU Virtual Machine
        Physical Slot: 31
        Control: I/O+ Mem+ BusMaster- SpecCycle- MemWINV- VGASnoop- ParErr- Stepping- SERR- FastB2B- DisINTx-
        Status: Cap- 66MHz- UDF- FastB2B- ParErr- DEVSEL=fast >TAbort- <TAbort- <MAbort- >SERR- <PERR- INTx-
        Region 0: Memory at 81141100 (32-bit, non-prefetchable) [size=256]
        Region 2: Memory at 380000800000 (64-bit, prefetchable) [size=8M]
        Kernel driver in use: ivpci
```

查看系统的中断信息, 可以看到29-32的中断由我们的驱动来处理:

```
# cat /proc/interrupts
           CPU0       CPU1
 30:          0          0   PCI-MSI 114688-edge      ivpci1-0
 31:          0          0   PCI-MSI 114689-edge      ivpci1-1
 32:          0          0   PCI-MSI 114690-edge      ivpci1-2
 33:          0          0   PCI-MSI 114691-edge      ivpci1-3
```

在其中一台虚拟机上执行用户态程序, 进程被阻塞, `peer_id` 为1:

```
# ./ioctl /dev/ivpci1 wait
IVPOSITION: 1
wait:

```

可以看到此时进程进入睡眠状态:

```

```

在另一台机器上执行用户态程序触发对端的 1 号中断:

```
# ./ioctl /dev/ivpci1 ring 1 1
IVPOSITION: 2
ring:
arg: 0x10001
ioctl finished, returned value: 0
```

此时, peer_id 为 1 的虚拟机上的用户态进程将从睡眠状态返回:

```
# ./ioctl /dev/ivpci1 wait
IVPOSITION: 1
wait:
ioctl finished, returned value: 0
```

使用中断机制, 可以避免各 peer通过轮询进行消息通知, 性能更优. 但 ivshmem-server 的实现以及 ivshmem 机制的服务端与客户端之间的协议设计本身的生产可用性还是有所不足. 比如上边提到的第一个 peer_id 为 0. 而 peer_id 的分配机制本身所携带的信息过少, ivshmem-server 没有足够的信息来区分哪个设备的 peer_id 各是多少. 在实际生产应用中, 需要独立实现注册中心来收集各设备 peer_id. 这样还需要注册中心与虚拟机内的程序、宿主机上的辅助程序都要具备网络连通性. 这本身会对云平台的网络结构带来比较大的依赖. 在生产环境中使用中断机制建议对 ivshmem-server 进行较大完善.


